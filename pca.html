<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt-br" xml:lang="pt-br"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Análise de Componentes Principais – PSI5892</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Procurar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PSI5892</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Procurar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Alternar de navegação" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-teoria" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Teoria</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-teoria">    
        <li>
    <a class="dropdown-item" href="./introducao.html">
 <span class="dropdown-text">Introdução</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./regressao_linear.html">
 <span class="dropdown-text">Regressão linear</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./lms.html">
 <span class="dropdown-text">O algoritmo LMS</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-exercícios-para-aula" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Exercícios para aula</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-exercícios-para-aula">    
        <li>
    <a class="dropdown-item" href="./ex_aula_1.html">
 <span class="dropdown-text">Exercício 1 - Regressão Linear</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./ex_aula_2.html">
 <span class="dropdown-text">Exercício 2 - O algoritmo LMS</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-material-de-apoio" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Material de apoio</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-material-de-apoio">    
        <li>
    <a class="dropdown-item" href="./python_videos.html">
 <span class="dropdown-text">Tópicos de programação com Python</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./exemplo_pytorch.html">
 <span class="dropdown-text">Exemplo MLP com PyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./exemplo_pytorch_cnn.html">
 <span class="dropdown-text">Exemplo CNN com PyTorch</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://edisciplinas.usp.br/course/view.php?id=116758"> 
<span class="menu-text">Moodle</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Análise de Componentes Principais</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div hidden="">
<p><span class="math display">\[
\newcommand{\AM}{{\mathbf A}}
\newcommand{\IM}{{\mathbf I}}
\newcommand{\vM}{{\mathbf v}}
\newcommand{\xM}{{\mathbf x}}
\newcommand{\yM}{{\mathbf y}}
\newcommand{\E}{{\rm E}}
\]</span></p>
</div>
<div class="justify">
<p>A redução do número de variáveis de entrada de um modelo preditivo é chamada de redução de dimensionalidade. Menos variáveis de entrada podem resultar em um modelo mais simples que, por sua vez, pode ter melhor desempenho ao fazer previsões de dados novos.</p>
<p>A técnica mais popular para redução de dimensionalidade em Aprendizado de Máquina é a <strong>Análise de Componentes Principais</strong> (<em>Principal Component Analysis</em> - PCA). Essa técnica de Álgebra Linear reduz a dimensionalidade de um conjunto de dados no qual há um grande número de dados correlacionados, mantendo o máximo possível da variação presente no conjunto. Essa redução é alcançada pela transformação para um novo conjunto de variáveis não correlacionadas e que são ordenadas de modo que as primeiras retenham a maior parte da variação das originais.</p>
<p>A seguir vamos detalhar essa técnica.</p>
<section id="maximizando-a-variância" class="level2">
<h2 class="anchored" data-anchor-id="maximizando-a-variância">Maximizando a variância</h2>
<p>Considere um conjunto de dados <span class="math inline">\(\{\mathbf{x}_n\}\)</span>, em que <span class="math inline">\(n=1, 2, \ldots, N\)</span> e <span class="math inline">\(\mathbf{x}_n \in \mathbb{R}^{D}\)</span>, ou seja, os vetores coluna <span class="math inline">\(\mathbf{x}_n\)</span> têm dimensão <span class="math inline">\(D\times 1\)</span> e elementos reais. O objetivo do PCA é projetar os dados em um espaço de dimensão <span class="math inline">\(M&lt;D\)</span> e ao mesmo tempo maximizar a variância dos dados projetados. Neste momento, vamos assumir que <span class="math inline">\(M\)</span> é conhecido. Há técnicas para determinar um valor apropriado para <span class="math inline">\(M\)</span>, como veremos posteriormente.</p>
<p>Considere a projeção em um espaço de uma dimensão <span class="math inline">\(M=1\)</span>. Podemos definir a direção deste espaço, usando um vetor coluna de dimensão <span class="math inline">\(D\times 1\)</span>, denotado por <span class="math inline">\(\mathbf{u}_1\)</span>. Por conveniência e sem perda de generalidade, vamos assumir que <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=\|\mathbf{u}_1\|^2=1\)</span>, já que estamos interessados na direção do vetor <span class="math inline">\(\mathbf{u}_1\)</span> e não em sua magnitude. Cada vetor <span class="math inline">\(\mathbf{x}_n\)</span> do conjunto de dados é então projetado no escalar <span class="math inline">\(p_{1n}=\mathbf{u}_1^{\rm T}\mathbf{x}_n\)</span>. A média dos dados projetados vale</p>
<p><span class="math display">\[
\overline{p}_1=\frac{1}{N}\sum_{n=1}^{N}p_{1n}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n=\mathbf{u}_1^{\rm T}\left[\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n\right]=\mathbf{u}_1^{\rm T}\overline{\mathbf{x}}
\]</span></p>
<p>em que <span class="math inline">\(\overline{\mathbf{x}}\)</span> é o valor médio do conjunto de dados. A variância dos dados projetados é dada por</p>
<p><span class="math display">\[
\sigma_{p_1}^2=\frac{1}{N}\sum_{n=1}^{N}(p_{1n}-\overline{p})^2=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n-\mathbf{u}_1^{\rm T}\overline{\mathbf{x}})^2
\]</span> <span class="math display">\[
=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n\mathbf{x}_n^{\rm T}\mathbf{u}_1 + \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1) - \frac{2}{N} \sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1.
\]</span></p>
<p>Note que o segundo somatório do lado direito da última igualdade dessa expressão vale <span class="math display">\[
\frac{2}{N} \sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1=
2 \mathbf{u}_1^{\rm T}\left[\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n\right]\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1= 2 \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1.
\]</span></p>
<p>Assim, podemos escrever a variância <span class="math inline">\(\sigma_{p_1}^2\)</span> como <span class="math display">\[
\sigma_{p_1}^2=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n\mathbf{x}_n^{\rm T}\mathbf{u}_1 - \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1)=
\mathbf{u}_1^{\rm T} \left[\frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n\mathbf{x}_n^{\rm T}-\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}) \right]\mathbf{u}_1.
\]</span></p>
<p>Definindo a matriz de covariância dos dados <span class="math display">\[
\mathbf{S}=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n-\overline{\mathbf{x}})(\mathbf{x}_n-\overline{\mathbf{x}})^{\rm T},
\]</span></p>
<p>e identificando que ela é termo entre colchetes da expressão anterior, chega-se finalmente a</p>
<p><span class="math display">\[
\sigma_{p_1}^2=\mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1.
\]</span></p>
<p>Vamos agora maximizar a variância <span class="math inline">\(\sigma_{p_1}^2\)</span> com relação à <span class="math inline">\(\mathbf{u}_1\)</span>. Note que se trata de um critério com restrição, pois <span class="math inline">\(\|\mathbf{u}_1\|\rightarrow \infty\)</span> maximiza <span class="math inline">\(\sigma_p^2\)</span>, mas deve ser evitado. Com esse propósito, vamos considerar a seguinte restrição <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=\|\mathbf{u}_1\|^2=1\)</span>, o que leva ao critério <span class="math display">\[
\max_{\mathbf{u}_1} \mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1\;\;\text{sujeito a}\;\;\mathbf{u}_1^{\rm T}\mathbf{u}_1=1.
\]</span></p>
<p>Para maximizar a variância levando em conta a restrição, pode-se considerar um multiplicador de Lagrange, denotado por <span class="math inline">\(\lambda_1\)</span>, o que leva à maximização do seguinte critério sem restrição <span class="math display">\[
J(\mathbf{u}_1)=\mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1+\lambda_1(1-\mathbf{u}_1^{\rm T}\mathbf{u}_1).
\]</span></p>
<p>Calculando a derivando de <span class="math inline">\(J(\mathbf{u}_1)\)</span> em relação à <span class="math inline">\(\mathbf{u}_1\)</span>, obtém-se <span class="math display">\[
\frac{d J(\mathbf{u}_1)}{d\mathbf{u}_1}=\mathbf{S}\mathbf{u}_1-\lambda_1\mathbf{u_1}.
\]</span></p>
<p>Igualando essa derivada a zero, chega-se a</p>
<p><span class="math display">\[\begin{equation*}
\fbox{$\displaystyle
\mathbf{S}\mathbf{u}_1=\lambda_1\mathbf{u}_1.
$}
\end{equation*}\]</span></p>
<p>Essa igualdade ocorre apenas quando <span class="math inline">\(\mathbf{u}_1\)</span> for o autovetor de <span class="math inline">\(\mathbf{S}\)</span> associado ao autovalor <span class="math inline">\(\lambda_1\)</span>. Para recordar isso, no Apêndice~A há uma recordação da teoria de Álgebra Linear relacionada a autovalores e autovetores.</p>
<p>Multiplicando ambos os lados da relação anterior à esquerda por <span class="math inline">\(\mathbf{u}_1^{\rm T}\)</span> e usando o fato de que <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=1\)</span>, obtemos <span class="math display">\[
\sigma_{p_1}^2=\mathbf{u}_1^{\rm T}\mathbf{S}\mathbf{u}_1=\lambda_1.
\]</span></p>
<p>Então a variância <span class="math inline">\(\sigma_p^2\)</span> será máxima quando <span class="math inline">\(\mathbf{u}_1\)</span> for o autovetor de <span class="math inline">\(\mathbf{S}\)</span> relacionado ao maior autovalor <span class="math inline">\(\lambda_1\)</span>. Esse autovetor é conhecido como o <strong>primeiro componente principal</strong>.</p>
<p>Podemos adicionar componentes principais, escolhendo cada nova direção como aquela que maximiza a variância projetada entre todas as direções ortogonais possíveis às já consideradas. Dessa forma, é possível demonstrar por indução que ao considerar um espaço de dimensão <span class="math inline">\(M\)</span>, a projeção linear ótima para a qual a variância dos dados projetados é maximizada é definida pelos <span class="math inline">\(M\)</span> autovetores <span class="math inline">\(\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_M\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span>, correspondentes aos seus <span class="math inline">\(M\)</span> maiores autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_M\)</span>.</p>
<p>Resumindo, a análise de componentes principais envolve o cálculo da média <span class="math inline">\(\overline{\mathbf{x}}\)</span> e da matriz de covariância <span class="math inline">\(\mathbf{S}\)</span> do conjunto de dados. Em seguida, deve-se encontrar os <span class="math inline">\(M\)</span> autovetores de <span class="math inline">\(\mathbf{S}\)</span> relacionados aos seus <span class="math inline">\(M\)</span> maiores autovalores. Se levarmos em conta todos os <span class="math inline">\(D\)</span> autovetores de <span class="math inline">\(\mathbf{S}\)</span>, não haverá redução de dimensionalidade. Trata-se de uma transformação linear ortogonal que transforma os dados para um novo sistema de coordenadas de modo que a maior variância por qualquer projeção dos dados fica ao longo da primeira coordenada (primeiro componente principal), a segunda maior variância fica ao longo da segunda coordenada (segundo componente principal) e assim por diante. O primeiro componente principal é o mais importante porque explica a maior parcela da variância dos dados, o segundo componente é o segundo mais importante e assim sucessivamente. O objetivo é descrever a maxima variabilidade do conjunto de dados original com um conjunto menor de variáveis.</p>
</section>
<section id="gerando-um-conjunto-de-dados-não-correlacionados" class="level2">
<h2 class="anchored" data-anchor-id="gerando-um-conjunto-de-dados-não-correlacionados">Gerando um conjunto de dados não correlacionados</h2>
<p>Vimos que os <span class="math inline">\(M\)</span> autovetores <span class="math inline">\(\mathbf{u}_k=[u_{k1}\;u_{k2}\;\cdots\;u_{kD}]^{\rm T}\)</span>, associados aos <span class="math inline">\(M\)</span> maiores autovalores <span class="math inline">\(\lambda_k\)</span>, <span class="math inline">\(k=1, 2, \cdots, M\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span> são os componentes principais. Os dados projetados em cada um desses componentes são dados por <span class="math display">\[
p_{kn}=\mathbf{u}_k^{\rm T}\mathbf{x}_n=u_{k1}x_{1n}+u_{k2}x_{2n}+\cdots u_{kD}x_{Dn}
\]</span></p>
<p>para <span class="math inline">\(k=1, 2, \cdots, M\)</span> e <span class="math inline">\(n=1, 2 \cdots N\)</span>. Assim, os dados projetados são combinações lineares de todas as variáveis presentes nos vetores do conjunto de dados e os elementos dos autovetores, chamados de <em>loadings</em> na literatura, são os pesos dessas combinações.</p>
<p>Organizando o conjunto original de dados na matriz <span class="math display">\[
\mathbf{X}=[\mathbf{x}_1\;\mathbf{x}_2\;\cdots\;\mathbf{x}_N]
\]</span></p>
<p>e os componentes principais na matriz <span class="math display">\[
\mathbf{U}=\left[\begin{array}{c}
                    \mathbf{u}_1^{\rm T} \\
                    \mathbf{u}_2^{\rm T} \\
                    \vdots \\
                    \mathbf{u}_M^{\rm T}
                  \end{array}
\right],
\]</span></p>
<p>obtemos a matriz dos dados transformados</p>
<p><span class="math display">\[
\mathbf{P}=[\mathbf{p}_1\;\mathbf{p}_2\;\cdots\;\mathbf{p}_N]
\]</span></p>
<p>por meio da transformação linear <span class="math display">\[
\mathbf{P}=\mathbf{U}\mathbf{X},
\]</span></p>
<p>ou seja, <span class="math display">\[
\left[\begin{array}{cccc}
            p_{11} &amp;p_{12} &amp; \cdots &amp; p_{1N} \\
            p_{21} &amp;p_{22} &amp; \cdots &amp; p_{2N} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            p_{M1} &amp;p_{M2} &amp; \cdots &amp; p_{MN}
          \end{array}\right]_{M\times N}=\left[\begin{array}{cccc}
            u_{11} &amp;u_{12} &amp; \cdots &amp; u_{1D} \\
            u_{21} &amp;u_{22} &amp; \cdots &amp; u_{2D} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            u_{M1} &amp;u_{M2} &amp; \cdots &amp; u_{MD}
          \end{array}
        \right]_{M\times D}\left[\begin{array}{cccc}
            x_{11} &amp;x_{12} &amp; \cdots &amp; x_{1N} \\
            x_{21} &amp;x_{22} &amp; \cdots &amp; x_{2N} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            x_{D1} &amp;x_{D2} &amp; \cdots &amp; x_{DN}
          \end{array}
        \right]_{D\times N}.
\]</span></p>
<p>Devido à restrição do critério considerado para maximizar a variância dos dados projetados, os componentes principais, autovetores da matriz de covariância dos dados, possuem norma unitária. Além disso, de Álgebra Linear sabe-se que esses vetores são ortogonais entre si. Da teoria de Probabilidades, sabe-se que duas variáveis aleatórias são não correlacionadas quando sua covariância é nula.</p>
<p>Considere os dados projetados por dois componentes principais distintos, ou seja, <span class="math inline">\(\mathbf{u}_k\)</span> e <span class="math inline">\(\mathbf{u}_{\ell}\)</span> com <span class="math inline">\(k\neq \ell\)</span> e <span class="math inline">\(\{k, \ell\} \in \{1, 2, \ldots, M\}\)</span>. A covariância entre esses dados é calculada como <span class="math display">\[
{\rm cov}(k,\ell)={\rm E}\{(\mathbf{u}_k^{\rm T}\mathbf{x}_n-\mathbf{u}_k^{\rm T}\overline{\mathbf{x}})(\mathbf{x}_n^{\rm T}\mathbf{u}_\ell-\overline{\mathbf{x}}^{\rm T}\mathbf{u}_\ell)\}
\]</span></p>
<p>em que <span class="math inline">\(\E\{\cdot\}\)</span> representa a esperança matemática. Os vetores <span class="math inline">\(\mathbf{u}_k\)</span> e <span class="math inline">\(\mathbf{u}_{\ell}\)</span> não são variáveis aleatórias e podem sair da esperança. Assim, <span class="math display">\[
{\rm cov}(k,\ell)=\mathbf{u}_k^{\rm T}\,\E\{\mathbf{x}_n\mathbf{x}_n^{\rm T}-\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}-\overline{\mathbf{x}}\mathbf{x}_n^{\rm T}+\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\}\,\mathbf{u}_\ell.
\]</span></p>
<p>A esperança que aparece nessa expressão é a matriz de covariância dos dados, o que leva a <span class="math display">\[
{\rm cov}(k,\ell)=\mathbf{u}_k^{\rm T}\,\mathbf{S}\,\mathbf{u}_\ell.
\]</span></p>
<p>Lembrando que <span class="math inline">\(\mathbf{S}\,\mathbf{u}_\ell=\lambda_\ell\)</span> e que <span class="math inline">\(\mathbf{u}_k^{\rm T}\mathbf{u}_\ell=0\)</span> (os autovetores são ortogonais), chega-se</p>
<p><span class="math display">\[
{\rm cov}(k,\ell)=\lambda_\ell\mathbf{u}_k^{\rm T}\mathbf{u}_\ell=0,
\]</span></p>
<p>o que mostra que os dados transformados por componentes principais distintos são <strong>não correlacionados</strong>.</p>
</section>
<section id="quantos-componentes-principais-usar" class="level2">
<h2 class="anchored" data-anchor-id="quantos-componentes-principais-usar">Quantos componentes principais usar?</h2>
<p>Vimos que os autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_D\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span> são as variâncias dos dados transformados. Assim, a soma de todos os autovalores é a variância total explicada, ou seja, <span class="math display">\[
\sigma^2_{\text{total}}=\sum_{k=1}^{D}\lambda_k.
\]</span></p>
<p>Consequentemente, a proporção da variância explicada (em %) por cada componente principal <span class="math inline">\(\ell\)</span>, <span class="math inline">\(\ell=1, 2, \cdots M\)</span> é dada por <span class="math display">\[
\%{\rm var}_\ell=100\displaystyle\frac{\lambda_\ell}{\displaystyle\sum_{k=1}^{D}\lambda_k}=100\displaystyle\frac{\lambda_\ell}{\displaystyle\sigma^2_{\text{total}}}.
\]</span></p>
<p>Como o PCA é um método usado para redução de dimensionalidade, deve-se considerar apenas os componentes que explicam a maior parte da variação dos dados. Não existe um ponto de corte absoluto para descartamos os componentes principais menos significativos. Em geral, considera-se o número de componentes principais para que a soma da proporção da variância explicada seja em torno de 80%.</p>
</section>
<section id="normalizando-o-conjunto-de-dados" class="level2">
<h2 class="anchored" data-anchor-id="normalizando-o-conjunto-de-dados">Normalizando o conjunto de dados</h2>
<p>Em geral, os elementos dos vetores de dados <span class="math inline">\(\mathbf{x}_n\)</span> podem representar variáveis com diferentes ordens de grandeza. Diante disso, é importante normalizar os dados antes de calcular o PCA. Suponha que cada vetor <span class="math inline">\(\mathbf{x}_n\)</span> do banco de dados seja dado por</p>
<p><span class="math display">\[
\mathbf{x}_n=[x_{1n}\;x_{2n}\;\cdots\; x_{Dn}]^{\rm T},
\]</span></p>
<p>em que cada variável <span class="math inline">\(x_{nk}\)</span>, <span class="math inline">\(k=1, 2, \ldots, D\)</span> representa uma grandeza. A normalização mais comum dos dados leva em conta a média</p>
<p><span class="math display">\[
\overline{x}_k=\frac{1}{N}\sum_{n=1}^{N}x_{kn},
\]</span></p>
<p>e o desvio padrão de cada variável <span class="math display">\[
\sigma_{x_k}=\sqrt{\frac{1}{N}\sum_{n=1}^{N}(x_{kn}-\overline{x}_k)^2},
\]</span></p>
<p>para <span class="math inline">\(k=1, 2, \ldots, D\)</span>. Assim, as variáveis são normalizadas como</p>
<p><span class="math display">\[
\widetilde{x}_{kn}=\frac{x_{kn}-\overline{x}_k}{\sigma_{x_k}}
\]</span></p>
<p>e os vetores de dados normalizados sobre os quais o PCA deve ser calculado é dado por</p>
<p><span class="math display">\[
\widetilde{\mathbf{x}}_n=[\widetilde{x}_{1n}\;\widetilde{x}_{2n}\;\cdots\; \widetilde{x}_{Dn}]^{\rm T}
\]</span></p>
<p>para <span class="math inline">\(n=1, 2, \ldots, N\)</span>.</p>
<p>Nas seções anteriores, a formulação do PCA foi feita sobre o conjunto de dados não normalizados <span class="math inline">\(\{\mathbf{x}_n\}\)</span>. No entanto, a normalização é aconselhável como forma de evitar enviesar a influência de certas variáveis quando as variáveis originais têm dispersões ou escalas significativamente diferentes. Quando as medidas originais já possuem dispersões semelhantes, a padronização tem pouco efeito.</p>
</section>
<section id="exemplos" class="level2">
<h2 class="anchored" data-anchor-id="exemplos">Exemplos</h2>
<p>Considere que os pontos azuis indicados na <a href="#fig-pca-umadim" class="quarto-xref">Figura&nbsp;1</a> pertencem a um conjunto de dados normalizados <span class="math inline">\(\{\widetilde{\mathbf{x}}_n\}\)</span> com <span class="math inline">\(N=10\)</span> e <span class="math inline">\(D=2\)</span>. Vamos considerar o subespaço gerado pelo primeiro componente principal, diminuindo a dimensão para <span class="math inline">\(M=1\)</span>. A matriz de covariância dos dados é dada por <span class="math display">\[
\mathbf{S}=\left[\begin{array}{cc}
                     0,900 &amp; 0,725 \\
                     0,725 &amp; 0,900
                   \end{array}
\right]
\]</span></p>
<p>cujos autovalores são <span class="math inline">\(\lambda_1=1,625\)</span> e <span class="math inline">\(\lambda_2=0,1750\)</span> e os autovetores de norma unitária associados são <span class="math inline">\(\mathbf{u}_1=[1/\sqrt{2}\;\;\;\; 1/\sqrt{2}]^{\rm T}\)</span> e <span class="math inline">\(\mathbf{u}_2=[-1/\sqrt{2}\;\;\;\; 1/\sqrt{2}]^{\rm T}\)</span>, respectivamente. Considerando o componente principal <span class="math inline">\(\mathbf{u}_1\)</span>, autovetor associado ao maior autovalor, os dados projetados são calculados como</p>
<p><span class="math display">\[
p_n=\frac{1}{\sqrt{2}}\widetilde{x}_{1n}+\frac{1}{\sqrt{2}}\widetilde{x}_{2n}.
\]</span></p>
<p>Esse componente explica <span class="math inline">\(\%{\rm var}_1=100(1,625)/(1,625+0.1750)=90,28\%\)</span> da variância total. O PCA busca um espaço de dimensão <span class="math inline">\(M=1\)</span>, denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.</p>
<div id="fig-pca-umadim" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-umadim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/fig_pca_umadim.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-umadim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;1: O PCA busca um espaço de dimensão menor conhecido como subespaço principal, denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.
</figcaption>
</figure>
</div>
<p>O PCA pode ser usado para compressão de imagens. Para ilustrar isso, na <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;2</a>(a), consideramos imagem média (<span class="math inline">\(\overline{\mathbf{x}}\)</span>) e os quatro primeiros componentes principais (<span class="math inline">\(\mathbf{u}_1,\cdots,\mathbf{u}_4\)</span>) com os seus autovalores correspondentes baseados em imagens do dígito três da base de dados MNIST. Na <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;2</a>(b), são mostradas a imagem original e as imagens reconstruídas considerando 1, 10, 50 e 250 componentes principais. A medida que se aumenta o valor de <span class="math inline">\(M\)</span>, a reconstrução se torna mais precisa e é perfeita para <span class="math inline">\(M=D=784\)</span>.</p>
<div id="fig-mnist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/fig_mnist.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;2: (a) Imagem média e os quatro primeiros componentes principais baseados em imagens do dígito três da base de dados MNIST; (b) Imagem original e reconstrução da imagem considerando 1, 10, 50 e 250 componentes principais. Fonte: <span class="citation" data-cites="bishop_pattern_2011">(<a href="#ref-bishop_pattern_2011" role="doc-biblioref">Bishop 2011</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="leitura-adicional" class="level2">
<h2 class="anchored" data-anchor-id="leitura-adicional">Leitura adicional</h2>
<p>O livro <span class="citation" data-cites="jolliffe_principal_2002">(<a href="#ref-jolliffe_principal_2002" role="doc-biblioref">Jolliffe 2002</a>)</span> é uma referência recomendada para quem quiser se aprofundar no assunto.</p>
</section>


</div>




<div id="quarto-appendix" class="default"><section id="autovalores-e-autovetores" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">Autovalores e Autovetores</h2><div class="quarto-appendix-contents">

<p>Seja <span class="math inline">\(\AM\)</span> uma matriz <span class="math inline">\(M\times M\)</span> com elementos constantes. Essa matriz, quando aplicada a um vetor <span class="math inline">\(\xM\)</span> com dimensão <span class="math inline">\(M\times 1\)</span>, resulta em um vetor <span class="math inline">\(\yM\)</span> com dimensão <span class="math inline">\(M\times 1\)</span>, ou seja,</p>
<p><span class="math display">\[\begin{equation}\label{eq:TL}
\yM=\AM\, \xM.
\end{equation}\]</span></p>
<p>Nota-se que a matriz <span class="math inline">\(\AM\)</span> representa uma transformação linear que transforma um vetor <span class="math inline">\(\xM\)</span> no vetor <span class="math inline">\(\yM\)</span>. O vetor <span class="math inline">\(\yM\)</span> pode ser interpretado como resultado da projeção do vetor <span class="math inline">\(\xM\)</span> nas colunas da matriz <span class="math inline">\(\AM\)</span>. De modo geral, essa transformação muda o módulo e a direção do vetor <span class="math inline">\(\xM\)</span>.</p>
<p>Um caso particular de grande interesse prático é aquele em que <span class="math inline">\(\xM\)</span> é um vetor não nulo e <span class="math inline">\(\AM\,\xM\)</span> é um múltiplo escalar de <span class="math inline">\(\xM\)</span>. Para destacar esse vetor <span class="math inline">\(\xM\)</span> dos demais vamos denotá-lo como <span class="math inline">\(\vM\)</span>, assim,</p>
<p><span id="eq-tl-eig"><span class="math display">\[
\begin{equation*}
\fbox{$\displaystyle
\AM\, \vM=\lambda\vM
$}
\end{equation*}
\tag{1}\]</span></span></p>
<p>em que <span class="math inline">\(\lambda\)</span> é uma constante real ou complexa. Nesse caso, a transformação linear aplicada em <span class="math inline">\(\vM\)</span> resulta em um múltiplo escalar dele mesmo. O vetor particular <span class="math inline">\(\xM=\vM\)</span> representa uma direção privilegiada no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>, tal que a ação da transformação <span class="math inline">\(\AM\)</span> sobre o vetor <span class="math inline">\(\vM\)</span> age apenas sobre o módulo desse vetor mantendo a sua direção. O escalar <span class="math inline">\(\lambda\)</span> é chamado de autovalor de <span class="math inline">\(\AM\)</span> e <span class="math inline">\(\vM\)</span> de autovetor associado a <span class="math inline">\(\lambda\)</span>. Cabe observar que para um dado autovalor <span class="math inline">\(\lambda\)</span> podem existir vários vetores não nulos <span class="math inline">\(\vM\)</span> que satisfazem a <a href="#eq-tl-eig" class="quarto-xref">Equação&nbsp;1</a>, como veremos a seguir. Além disso, o autovetor <span class="math inline">\(\vM\)</span> não pode ser nulo, porém, o autovalor <span class="math inline">\(\lambda\)</span> pode ser nulo.</p>
</div></section><section id="a-obtenção-dos-autovalores-e-autovetores" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">A obtenção dos autovalores e autovetores</h2><div class="quarto-appendix-contents">

<p>Por conveniência vamos reescrever a <a href="#eq-tl-eig" class="quarto-xref">Equação&nbsp;1</a> da seguinte forma,</p>
<p><span id="eq-eig-mod"><span class="math display">\[
\begin{equation}
% \AM\, \xM-\lambda\IM\xM=
\left( \AM-\lambda\IM\right)\vM=\mathbf{0},
\end{equation}
\tag{2}\]</span></span></p>
<p>em que <span class="math inline">\(\IM\)</span> denota a matriz identidade de dimensão <span class="math inline">\(M\times M\)</span> e <span class="math inline">\(**0**\)</span> um vetor de zeros de dimensão <span class="math inline">\(M\times 1\)</span>. Nota-se que <span class="math inline">\(\lambda\)</span> é um autovalor da matriz <span class="math inline">\(\AM\)</span> se e somente se a <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a> possui uma solução não trivial.</p>
<p>A partir da <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a>, usando conceitos de solução de sistemas de equações e particularizando para o caso de interesse, seguem as afirmações:</p>
<ul>
<li><p>A <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a> terá uma solução não trivial se e somente se <span class="math inline">\(\left( \AM-\lambda\IM\right)\)</span> for singular, ou seja, <span id="eq-sol1"><span class="math display">\[
\begin{equation*}
\fbox{$\displaystyle
\det\left( \AM-\lambda\IM\right)=0.
$}
\end{equation*}
\tag{3}\]</span></span></p>
<p>Ao aplicar a operação de determinante em <span class="math inline">\(\left( \AM-\lambda\IM\right)\)</span> obtemos um polinômio em <span class="math inline">\(\lambda\)</span>, que representamos como <span id="eq-sol2"><span class="math display">\[
\begin{equation}\label{eq:sol2}
p(\lambda)= \det\left( \AM-\lambda\IM\right)=\lambda^M+c_1\lambda^{M-1}+c_2\lambda^{M-2}\cdots c_M.
\end{equation}
\tag{4}\]</span></span></p>
<p>O polinômio <span class="math inline">\(p(\lambda)\)</span> é chamado de polinômio característico e <span class="math inline">\(p(\lambda)=0\)</span> é chamada de equação característica.</p></li>
<li><p>Nota-se que grau de <span class="math inline">\(p(\lambda)\)</span> é <span class="math inline">\(M\)</span>, portanto, { <span class="math inline">\(p(\lambda)=0\)</span>} possui <span class="math inline">\(M\)</span> soluções. Essas soluções podem ser distintas, repetidas, reais ou complexas. Os valores de <span class="math inline">\(\lambda\)</span> que satisfazem a <a href="#eq-sol2" class="quarto-xref">Equação&nbsp;4</a> são os autovalores da matriz <span class="math inline">\(\AM\)</span>. Portanto, <span class="math inline">\(\AM\)</span> possui <span class="math inline">\(M\)</span> autovalores que podem ser distintos, repetidos, reais ou complexos.</p></li>
<li><p>O conjunto de todas as soluções da <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a>, aqui denotada como <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é o espaço nulo de <span class="math inline">\(\AM-\lambda\IM\)</span> e todos os autovetores da matriz <span class="math inline">\(\AM\)</span> estão nesse espaço nulo. Assim, se <span class="math inline">\(\lambda\)</span> é um autovalor de <span class="math inline">\(\AM\)</span>, então, <span class="math inline">\({\cal{N}}\left( \AM-\lambda\IM\right)\neq \{ 0 \}\)</span> e qualquer subespaço não nulo em <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é um autovetor associado a <span class="math inline">\(\lambda\)</span>. O subespaço <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é chamado de autoespaço de <span class="math inline">\(\lambda\)</span>.</p></li>
</ul>

</div></section><section id="exemplos-com-m2" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Exemplos com <span class="math inline">\(M=2\)</span></h2><div class="quarto-appendix-contents">

<p>Nos exemplos a seguir, os autovalores são calculados a partir da <a href="#eq-sol1" class="quarto-xref">Equação&nbsp;3</a> e os autovetores associados são calculados resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=\lambda_1\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=\lambda_2\vM_2\)</span>.</p>
<ul>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cc}
     0 &amp; 0 \\
     0 &amp; 1 \\
   \end{array}
\right]
\]</span> os autovalores e os autovetores associados são <span class="math inline">\(\lambda_1=0\)</span> e <span class="math inline">\(\lambda_2=1\)</span>, e <span class="math display">\[
\begin{array}{ccc}
\vM_1=\left[
  \begin{array}{c}
     1 \\
     0 \\
   \end{array}
\right] &amp; \text{ e} &amp;  \vM_2= \left[
  \begin{array}{c}
     0 \\
     1 \\
   \end{array}
\right]\text{,}
\end{array}
\]</span> respectivamente. Esse exemplo ilustra o fato de que, embora o autovetor não possa ser um vetor nulo, o autovalor pode assumir o valor nulo.</p></li>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cr}
     2 &amp; 0 \\
     0 &amp; -3 \\
   \end{array}
\right]
\]</span> O polinômio característico de <span class="math inline">\(\AM\)</span> é $ p()=(2-)(-3-)$. Portanto, os autovalores de <span class="math inline">\(\AM\)</span> são <span class="math inline">\(\lambda_1=2\)</span> e <span class="math inline">\(\lambda_2=-3\)</span>. Resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=2\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=-3\vM_2\)</span> obtemos os autovetores <span class="math inline">\(\vM_1=[ 1\,\,0]^T\)</span> e <span class="math inline">\(\vM_2=[ 0\,\,1]^T\)</span>, respectivamente. Na <a href="#fig-pca-figxx" class="quarto-xref">Figura&nbsp;3</a>, são mostrados os vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e as suas projeções no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>. Como <span class="math inline">\(\AM\)</span> é uma matriz diagonal, os autovetores coincidem com as coordenadas do espaço Euclidiano.</p></li>
</ul>
<div id="fig-pca-figxx" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-figxx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/pca_figxx.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-figxx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;3: Vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e suas projeções, em que <span class="math inline">\(\AM\)</span> é uma matriz diagonal com autovalores <span class="math inline">\(\lambda_1=2\)</span> e <span class="math inline">\(\lambda_2=-3\)</span>.
</figcaption>
</figure>
</div>
<ul>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cr}
     1 &amp; 3 \\
     4 &amp; 2 \\
   \end{array}
\right].
\]</span> O polinômio característico de <span class="math inline">\(\AM\)</span> é <span class="math inline">\(p(\lambda)=\lambda^2-3\lambda-10=(\lambda-5)(\lambda+2)\)</span> e consequentemente os seus autovalores são <span class="math inline">\(\lambda_1=-2\)</span> e <span class="math inline">\(\lambda_2=5\)</span>. Resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=-2\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=5\vM_2\)</span> obtemos os autovetores <span class="math inline">\(\vM_1=[ 1\,\,-1]^T\)</span> e <span class="math inline">\(\vM_2=[ 1\,\,4/3]^T\)</span>, respectivamente.</p>
<p>Na <a href="#fig-pca-figxy" class="quarto-xref">Figura&nbsp;4</a>, são mostrados os vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e as suas projeções no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>. Nesse caso, como <span class="math inline">\(\AM\)</span> não é uma matriz diagonal, os autovetores não coincidem com as coordenadas do espaço Euclidiano.</p>
<p>De modo geral, nota-se que cada autovalor de <span class="math inline">\(\AM\)</span> possui uma infinidade de autovetores associados. Assim, são também autovetores de <span class="math inline">\(\AM\)</span> os vetores <span class="math inline">\(\bar\vM_1=\vM_1/||\vM_1||=[ 1\,\,-1]^T/\sqrt{2}\)</span> e <span class="math inline">\(\bar\vM_2=\vM_2/||\vM_2||=[ 3\,\,4]^T/5\)</span>.</p>
<p>Os autovetores <span class="math inline">\(\bar\vM_1\)</span> e <span class="math inline">\(\bar\vM_2\)</span> são particularmente interessantes porque possuem norma unitária.</p></li>
</ul>
<div id="fig-pca-figxy" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-figxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/pca_figxy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-figxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;4: Vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e suas projeções, em que <span class="math inline">\(\AM\)</span> não é diagonal com autovalores <span class="math inline">\(\lambda_1=-2\)</span> e <span class="math inline">\(\lambda_2=5\)</span>.
</figcaption>
</figure>
</div>
<ul>
<li>No MatLab faça <span class="math inline">\(help\,\, eig\)</span>. Use esse comando para conferir os autovalores e autovetores dos Exemplos <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span> e <span class="math inline">\(3\)</span>. Os autovetores fornecidos pelo MatLab possuem sempre norma unitária.</li>
</ul>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Referências</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bishop_pattern_2011" class="csl-entry" role="listitem">
Bishop, Christopher M. 2011. <em>Pattern <span>Recognition</span> and <span>Machine</span> <span>Learning</span></em>. 2006. Corr. 2nd Printing 2011 ed. edição. New York: Springer.
</div>
<div id="ref-jolliffe_principal_2002" class="csl-entry" role="listitem">
Jolliffe, I. T. 2002. <em>Principal <span>Component</span> <span>Analysis</span></em>. 2nd edition. New York: Springer.
</div>
</div></section></div></main> <!-- /main -->
<script>
var custom_title = document.querySelectorAll('.custom .theorem-title');

for (let i = 0; i < custom_title.length; i++ ) {
   var mod_name = custom_title[i].innerHTML;
   custom_title[i].innerHTML = mod_name.replace("Exemplo", "Algoritmo");
};
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiada");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
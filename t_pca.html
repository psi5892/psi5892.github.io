<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt-br" xml:lang="pt-br"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Análise de Componentes Principais – PSI5892</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-e0b2e4e5c4db31b3b64fdef51415d7d2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Procurar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">PSI5892</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Procurar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Alternar de navegação" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-teoria" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Teoria</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-teoria">    
        <li>
    <a class="dropdown-item" href="./t_introducao.html">
 <span class="dropdown-text">Introdução</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_regressao_linear.html">
 <span class="dropdown-text">Regressão linear</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_lms.html">
 <span class="dropdown-text">O algoritmo LMS</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_neuronio.html">
 <span class="dropdown-text">O modelo do neurônio</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_mlp.html">
 <span class="dropdown-text">A rede perceptron multicamada</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_hiperparametros.html">
 <span class="dropdown-text">Evitando mínimos locais e <em>overfitting</em></span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_autodiff.html">
 <span class="dropdown-text">Introdução à diferenciação automática</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_pytorch_topicos.html">
 <span class="dropdown-text">Tópicos sobre o <em>framework</em> PyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_pytorch_exemplo_mlp.html">
 <span class="dropdown-text">Implementação da rede MLP com PyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_medidas.html">
 <span class="dropdown-text">Medidas de desempenho</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_pca.html">
 <span class="dropdown-text">Análise de Componentes Principais</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./t_lda.html">
 <span class="dropdown-text">Análise de Discriminantes Lineares</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-material-de-apoio" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Material de apoio</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-material-de-apoio">    
        <li>
    <a class="dropdown-item" href="./ap_python_topicos.html">
 <span class="dropdown-text">Tópicos de programação com Python</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Análise de Componentes Principais</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div hidden="">
<p><span class="math display">\[
\newcommand{\AM}{{\mathbf A}}
\newcommand{\IM}{{\mathbf I}}
\newcommand{\vM}{{\mathbf v}}
\newcommand{\xM}{{\mathbf x}}
\newcommand{\yM}{{\mathbf y}}
\newcommand{\E}{{\rm E}}
\]</span></p>
</div>
<div class="justify">
<p>A redução do número de variáveis de entrada de um modelo preditivo é chamada de redução de dimensionalidade. Menos variáveis de entrada podem resultar em um modelo mais simples que, por sua vez, pode ter melhor desempenho ao fazer previsões de dados novos.</p>
<p>A técnica mais popular para redução de dimensionalidade em Aprendizado de Máquina é a <strong>Análise de Componentes Principais</strong> (<em>Principal Component Analysis</em> - PCA). Essa técnica de Álgebra Linear reduz a dimensionalidade de um conjunto de dados no qual há um grande número de dados correlacionados, mantendo o máximo possível da variação presente no conjunto. Essa redução é alcançada pela transformação para um novo conjunto de variáveis não correlacionadas e que são ordenadas de modo que as primeiras retenham a maior parte da variação das originais.</p>
<p>A seguir vamos detalhar essa técnica.</p>
<section id="maximizando-a-variância" class="level2">
<h2 class="anchored" data-anchor-id="maximizando-a-variância">Maximizando a variância</h2>
<p>Considere um conjunto de dados <span class="math inline">\(\{\mathbf{x}_n\}\)</span>, em que <span class="math inline">\(n=1, 2, \ldots, N\)</span> e <span class="math inline">\(\mathbf{x}_n \in \mathbb{R}^{D}\)</span>, ou seja, os vetores coluna <span class="math inline">\(\mathbf{x}_n\)</span> têm dimensão <span class="math inline">\(D\times 1\)</span> e elementos reais. O objetivo do PCA é projetar os dados em um espaço de dimensão <span class="math inline">\(M&lt;D\)</span> e ao mesmo tempo maximizar a variância dos dados projetados. Neste momento, vamos assumir que <span class="math inline">\(M\)</span> é conhecido. Há técnicas para determinar um valor apropriado para <span class="math inline">\(M\)</span>, como veremos posteriormente.</p>
<p>Considere a projeção em um espaço de uma dimensão <span class="math inline">\(M=1\)</span>. Podemos definir a direção deste espaço, usando um vetor coluna de dimensão <span class="math inline">\(D\times 1\)</span>, denotado por <span class="math inline">\(\mathbf{u}_1\)</span>. Por conveniência e sem perda de generalidade, vamos assumir que <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=\|\mathbf{u}_1\|^2=1\)</span>, já que estamos interessados na direção do vetor <span class="math inline">\(\mathbf{u}_1\)</span> e não em sua magnitude. Cada vetor <span class="math inline">\(\mathbf{x}_n\)</span> do conjunto de dados é então projetado no escalar <span class="math inline">\(p_{1n}=\mathbf{u}_1^{\rm T}\mathbf{x}_n\)</span>. A média dos dados projetados vale</p>
<p><span class="math display">\[
\overline{p}_1=\frac{1}{N}\sum_{n=1}^{N}p_{1n}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n=\mathbf{u}_1^{\rm T}\left[\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n\right]=\mathbf{u}_1^{\rm T}\overline{\mathbf{x}}
\]</span></p>
<p>em que <span class="math inline">\(\overline{\mathbf{x}}\)</span> é o valor médio do conjunto de dados. A variância dos dados projetados é dada por</p>
<p><span class="math display">\[
\sigma_{p_1}^2=\frac{1}{N}\sum_{n=1}^{N}(p_{1n}-\overline{p})^2=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n-\mathbf{u}_1^{\rm T}\overline{\mathbf{x}})^2
\]</span> <span class="math display">\[
=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n\mathbf{x}_n^{\rm T}\mathbf{u}_1 + \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1) - \frac{2}{N} \sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1.
\]</span></p>
<p>Note que o segundo somatório do lado direito da última igualdade dessa expressão vale <span class="math display">\[
\frac{2}{N} \sum_{n=1}^{N}\mathbf{u}_1^{\rm T}\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1=
2 \mathbf{u}_1^{\rm T}\left[\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n\right]\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1= 2 \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1.
\]</span></p>
<p>Assim, podemos escrever a variância <span class="math inline">\(\sigma_{p_1}^2\)</span> como <span class="math display">\[
\sigma_{p_1}^2=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{u}_1^{\rm T}\mathbf{x}_n\mathbf{x}_n^{\rm T}\mathbf{u}_1 - \mathbf{u}_1^{\rm T}\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\mathbf{u}_1)=
\mathbf{u}_1^{\rm T} \left[\frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n\mathbf{x}_n^{\rm T}-\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}) \right]\mathbf{u}_1.
\]</span></p>
<p>Definindo a matriz de covariância dos dados <span class="math display">\[
\mathbf{S}=\frac{1}{N}\sum_{n=1}^{N}(\mathbf{x}_n-\overline{\mathbf{x}})(\mathbf{x}_n-\overline{\mathbf{x}})^{\rm T},
\]</span></p>
<p>e identificando que ela é termo entre colchetes da expressão anterior, chega-se finalmente a</p>
<p><span class="math display">\[
\sigma_{p_1}^2=\mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1.
\]</span></p>
<p>Vamos agora maximizar a variância <span class="math inline">\(\sigma_{p_1}^2\)</span> com relação à <span class="math inline">\(\mathbf{u}_1\)</span>. Note que se trata de um critério com restrição, pois <span class="math inline">\(\|\mathbf{u}_1\|\rightarrow \infty\)</span> maximiza <span class="math inline">\(\sigma_p^2\)</span>, mas deve ser evitado. Com esse propósito, vamos considerar a seguinte restrição <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=\|\mathbf{u}_1\|^2=1\)</span>, o que leva ao critério <span class="math display">\[
\max_{\mathbf{u}_1} \mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1\;\;\text{sujeito a}\;\;\mathbf{u}_1^{\rm T}\mathbf{u}_1=1.
\]</span></p>
<p>Para maximizar a variância levando em conta a restrição, pode-se considerar um multiplicador de Lagrange, denotado por <span class="math inline">\(\lambda_1\)</span>, o que leva à maximização do seguinte critério sem restrição<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[
J(\mathbf{u}_1)=\mathbf{u}_1^{\rm T} \mathbf{S}\mathbf{u}_1+\lambda_1(1-\mathbf{u}_1^{\rm T}\mathbf{u}_1).
\]</span></p>
<p>Calculando a derivando de <span class="math inline">\(J(\mathbf{u}_1)\)</span> em relação à <span class="math inline">\(\mathbf{u}_1\)</span>, obtém-se <span class="math display">\[
\frac{d J(\mathbf{u}_1)}{d\mathbf{u}_1}=\mathbf{S}\mathbf{u}_1-\lambda_1\mathbf{u_1}.
\]</span></p>
<p>Igualando essa derivada a zero, chega-se a</p>
<p><span class="math display">\[\begin{equation*}
\fbox{$\displaystyle
\mathbf{S}\mathbf{u}_1=\lambda_1\mathbf{u}_1.
$}
\end{equation*}\]</span></p>
<p>Essa igualdade ocorre apenas quando <span class="math inline">\(\mathbf{u}_1\)</span> for o autovetor de <span class="math inline">\(\mathbf{S}\)</span> associado ao autovalor <span class="math inline">\(\lambda_1\)</span>. Para recordar isso, no Apêndice~A há uma recordação da teoria de Álgebra Linear relacionada a autovalores e autovetores.</p>
<p>Multiplicando ambos os lados da relação anterior à esquerda por <span class="math inline">\(\mathbf{u}_1^{\rm T}\)</span> e usando o fato de que <span class="math inline">\(\mathbf{u}_1^{\rm T}\mathbf{u}_1=1\)</span>, obtemos <span class="math display">\[
\sigma_{p_1}^2=\mathbf{u}_1^{\rm T}\mathbf{S}\mathbf{u}_1=\lambda_1.
\]</span></p>
<p>Então a variância <span class="math inline">\(\sigma_p^2\)</span> será máxima quando <span class="math inline">\(\mathbf{u}_1\)</span> for o autovetor de <span class="math inline">\(\mathbf{S}\)</span> relacionado ao maior autovalor <span class="math inline">\(\lambda_1\)</span>. Esse autovetor é conhecido como o <strong>primeiro componente principal</strong>.</p>
<p>Podemos adicionar componentes principais, escolhendo cada nova direção como aquela que maximiza a variância projetada entre todas as direções ortogonais possíveis às já consideradas. Dessa forma, é possível demonstrar por indução que ao considerar um espaço de dimensão <span class="math inline">\(M\)</span>, a projeção linear ótima para a qual a variância dos dados projetados é maximizada é definida pelos <span class="math inline">\(M\)</span> autovetores <span class="math inline">\(\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_M\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span>, correspondentes aos seus <span class="math inline">\(M\)</span> maiores autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_M\)</span>.</p>
<p>Resumindo, a análise de componentes principais envolve o cálculo da média <span class="math inline">\(\overline{\mathbf{x}}\)</span> e da matriz de covariância <span class="math inline">\(\mathbf{S}\)</span> do conjunto de dados. Em seguida, deve-se encontrar os <span class="math inline">\(M\)</span> autovetores de <span class="math inline">\(\mathbf{S}\)</span> relacionados aos seus <span class="math inline">\(M\)</span> maiores autovalores. Se levarmos em conta todos os <span class="math inline">\(D\)</span> autovetores de <span class="math inline">\(\mathbf{S}\)</span>, não haverá redução de dimensionalidade. Trata-se de uma transformação linear ortogonal que transforma os dados para um novo sistema de coordenadas de modo que a maior variância por qualquer projeção dos dados fica ao longo da primeira coordenada (primeiro componente principal), a segunda maior variância fica ao longo da segunda coordenada (segundo componente principal) e assim por diante. O primeiro componente principal é o mais importante porque explica a maior parcela da variância dos dados, o segundo componente é o segundo mais importante e assim sucessivamente. O objetivo é descrever a maxima variabilidade do conjunto de dados original com um conjunto menor de variáveis.</p>
</section>
<section id="gerando-um-conjunto-de-dados-não-correlacionados" class="level2">
<h2 class="anchored" data-anchor-id="gerando-um-conjunto-de-dados-não-correlacionados">Gerando um conjunto de dados não correlacionados</h2>
<p>Vimos que os <span class="math inline">\(M\)</span> autovetores <span class="math inline">\(\mathbf{u}_k=[u_{k1}\;u_{k2}\;\cdots\;u_{kD}]^{\rm T}\)</span>, associados aos <span class="math inline">\(M\)</span> maiores autovalores <span class="math inline">\(\lambda_k\)</span>, <span class="math inline">\(k=1, 2, \cdots, M\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span> são os componentes principais. Os dados projetados em cada um desses componentes são dados por <span class="math display">\[
p_{kn}=\mathbf{u}_k^{\rm T}\mathbf{x}_n=u_{k1}x_{1n}+u_{k2}x_{2n}+\cdots u_{kD}x_{Dn}
\]</span></p>
<p>para <span class="math inline">\(k=1, 2, \cdots, M\)</span> e <span class="math inline">\(n=1, 2 \cdots N\)</span>. Assim, os dados projetados são combinações lineares de todas as variáveis presentes nos vetores do conjunto de dados e os elementos dos autovetores, chamados de <em>loadings</em> na literatura, são os pesos dessas combinações.</p>
<p>Organizando o conjunto original de dados na matriz <span class="math display">\[
\mathbf{X}=[\mathbf{x}_1\;\mathbf{x}_2\;\cdots\;\mathbf{x}_N]
\]</span></p>
<p>e os componentes principais na matriz <span class="math display">\[
\mathbf{U}=\left[\begin{array}{c}
                    \mathbf{u}_1^{\rm T} \\
                    \mathbf{u}_2^{\rm T} \\
                    \vdots \\
                    \mathbf{u}_M^{\rm T}
                  \end{array}
\right],
\]</span></p>
<p>obtemos a matriz dos dados transformados</p>
<p><span class="math display">\[
\mathbf{P}=[\mathbf{p}_1\;\mathbf{p}_2\;\cdots\;\mathbf{p}_N]
\]</span></p>
<p>por meio da transformação linear <span class="math display">\[
\mathbf{P}=\mathbf{U}\mathbf{X},
\]</span></p>
<p>ou seja, <span class="math display">\[
\left[\begin{array}{cccc}
            p_{11} &amp;p_{12} &amp; \cdots &amp; p_{1N} \\
            p_{21} &amp;p_{22} &amp; \cdots &amp; p_{2N} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            p_{M1} &amp;p_{M2} &amp; \cdots &amp; p_{MN}
          \end{array}\right]_{M\times N}=\left[\begin{array}{cccc}
            u_{11} &amp;u_{12} &amp; \cdots &amp; u_{1D} \\
            u_{21} &amp;u_{22} &amp; \cdots &amp; u_{2D} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            u_{M1} &amp;u_{M2} &amp; \cdots &amp; u_{MD}
          \end{array}
        \right]_{M\times D}\left[\begin{array}{cccc}
            x_{11} &amp;x_{12} &amp; \cdots &amp; x_{1N} \\
            x_{21} &amp;x_{22} &amp; \cdots &amp; x_{2N} \\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            x_{D1} &amp;x_{D2} &amp; \cdots &amp; x_{DN}
          \end{array}
        \right]_{D\times N}.
\]</span></p>
<p>Devido à restrição do critério considerado para maximizar a variância dos dados projetados, os componentes principais, autovetores da matriz de covariância dos dados, possuem norma unitária. Além disso, de Álgebra Linear sabe-se que esses vetores são ortogonais entre si. Da teoria de Probabilidades, sabe-se que duas variáveis aleatórias são não correlacionadas quando sua covariância é nula<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>Considere os dados projetados por dois componentes principais distintos, ou seja, <span class="math inline">\(\mathbf{u}_k\)</span> e <span class="math inline">\(\mathbf{u}_{\ell}\)</span> com <span class="math inline">\(k\neq \ell\)</span> e <span class="math inline">\(\{k, \ell\} \in \{1, 2, \ldots, M\}\)</span>. A covariância entre esses dados é calculada como <span class="math display">\[
{\rm cov}(k,\ell)={\rm E}\{(\mathbf{u}_k^{\rm T}\mathbf{x}_n-\mathbf{u}_k^{\rm T}\overline{\mathbf{x}})(\mathbf{x}_n^{\rm T}\mathbf{u}_\ell-\overline{\mathbf{x}}^{\rm T}\mathbf{u}_\ell)\}
\]</span></p>
<p>em que <span class="math inline">\(\E\{\cdot\}\)</span> representa a esperança matemática. Os vetores <span class="math inline">\(\mathbf{u}_k\)</span> e <span class="math inline">\(\mathbf{u}_{\ell}\)</span> não são variáveis aleatórias e podem sair da esperança. Assim, <span class="math display">\[
{\rm cov}(k,\ell)=\mathbf{u}_k^{\rm T}\,\E\{\mathbf{x}_n\mathbf{x}_n^{\rm T}-\mathbf{x}_n\overline{\mathbf{x}}^{\rm T}-\overline{\mathbf{x}}\mathbf{x}_n^{\rm T}+\overline{\mathbf{x}}\,\overline{\mathbf{x}}^{\rm T}\}\,\mathbf{u}_\ell.
\]</span></p>
<p>A esperança que aparece nessa expressão é a matriz de covariância dos dados, o que leva a <span class="math display">\[
{\rm cov}(k,\ell)=\mathbf{u}_k^{\rm T}\,\mathbf{S}\,\mathbf{u}_\ell.
\]</span></p>
<p>Lembrando que <span class="math inline">\(\mathbf{S}\,\mathbf{u}_\ell=\lambda_\ell\)</span> e que <span class="math inline">\(\mathbf{u}_k^{\rm T}\mathbf{u}_\ell=0\)</span> (os autovetores são ortogonais), chega-se</p>
<p><span class="math display">\[
{\rm cov}(k,\ell)=\lambda_\ell\mathbf{u}_k^{\rm T}\mathbf{u}_\ell=0,
\]</span></p>
<p>o que mostra que os dados transformados por componentes principais distintos são <strong>não correlacionados</strong>.</p>
</section>
<section id="quantos-componentes-principais-usar" class="level2">
<h2 class="anchored" data-anchor-id="quantos-componentes-principais-usar">Quantos componentes principais usar?</h2>
<p>Vimos que os autovalores <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_D\)</span> da matriz de covariância dos dados <span class="math inline">\(\mathbf{S}\)</span> são as variâncias dos dados transformados. Assim, a soma de todos os autovalores é a variância total explicada, ou seja, <span class="math display">\[
\sigma^2_{\text{total}}=\sum_{k=1}^{D}\lambda_k.
\]</span></p>
<p>Consequentemente, a proporção da variância explicada (em %) por cada componente principal <span class="math inline">\(\ell\)</span>, <span class="math inline">\(\ell=1, 2, \cdots M\)</span> é dada por <span class="math display">\[
\%{\rm var}_\ell=100\displaystyle\frac{\lambda_\ell}{\displaystyle\sum_{k=1}^{D}\lambda_k}=100\displaystyle\frac{\lambda_\ell}{\displaystyle\sigma^2_{\text{total}}}.
\]</span></p>
<p>Como o PCA é um método usado para redução de dimensionalidade, deve-se considerar apenas os componentes que explicam a maior parte da variação dos dados. Não existe um ponto de corte absoluto para descartamos os componentes principais menos significativos. Em geral, considera-se o número de componentes principais para que a soma da proporção da variância explicada seja em torno de 80%.</p>
</section>
<section id="normalizando-o-conjunto-de-dados" class="level2">
<h2 class="anchored" data-anchor-id="normalizando-o-conjunto-de-dados">Normalizando o conjunto de dados</h2>
<p>Em geral, os elementos dos vetores de dados <span class="math inline">\(\mathbf{x}_n\)</span> podem representar variáveis com diferentes ordens de grandeza. Diante disso, é importante normalizar os dados antes de calcular o PCA. Suponha que cada vetor <span class="math inline">\(\mathbf{x}_n\)</span> do banco de dados seja dado por</p>
<p><span class="math display">\[
\mathbf{x}_n=[x_{1n}\;x_{2n}\;\cdots\; x_{Dn}]^{\rm T},
\]</span></p>
<p>em que cada variável <span class="math inline">\(x_{nk}\)</span>, <span class="math inline">\(k=1, 2, \ldots, D\)</span> representa uma grandeza. A normalização mais comum dos dados leva em conta a média</p>
<p><span class="math display">\[
\overline{x}_k=\frac{1}{N}\sum_{n=1}^{N}x_{kn},
\]</span></p>
<p>e o desvio padrão de cada variável <span class="math display">\[
\sigma_{x_k}=\sqrt{\frac{1}{N}\sum_{n=1}^{N}(x_{kn}-\overline{x}_k)^2},
\]</span></p>
<p>para <span class="math inline">\(k=1, 2, \ldots, D\)</span>. Assim, as variáveis são normalizadas como</p>
<p><span class="math display">\[
\widetilde{x}_{kn}=\frac{x_{kn}-\overline{x}_k}{\sigma_{x_k}}
\]</span></p>
<p>e os vetores de dados normalizados sobre os quais o PCA deve ser calculado é dado por</p>
<p><span class="math display">\[
\widetilde{\mathbf{x}}_n=[\widetilde{x}_{1n}\;\widetilde{x}_{2n}\;\cdots\; \widetilde{x}_{Dn}]^{\rm T}
\]</span></p>
<p>para <span class="math inline">\(n=1, 2, \ldots, N\)</span>.</p>
<p>Nas seções anteriores, a formulação do PCA foi feita sobre o conjunto de dados não normalizados <span class="math inline">\(\{\mathbf{x}_n\}\)</span>. No entanto, a normalização é aconselhável como forma de evitar enviesar a influência de certas variáveis quando as variáveis originais têm dispersões ou escalas significativamente diferentes. Quando as medidas originais já possuem dispersões semelhantes, a padronização tem pouco efeito.</p>
</section>
<section id="exemplos" class="level2">
<h2 class="anchored" data-anchor-id="exemplos">Exemplos</h2>
<p>Considere que os pontos azuis indicados na <a href="#fig-pca-umadim" class="quarto-xref">Figura&nbsp;1</a> pertencem a um conjunto de dados normalizados <span class="math inline">\(\{\widetilde{\mathbf{x}}_n\}\)</span> com <span class="math inline">\(N=10\)</span> e <span class="math inline">\(D=2\)</span>. Vamos considerar o subespaço gerado pelo primeiro componente principal, diminuindo a dimensão para <span class="math inline">\(M=1\)</span>. A matriz de covariância dos dados é dada por <span class="math display">\[
\mathbf{S}=\left[\begin{array}{cc}
                     0,900 &amp; 0,725 \\
                     0,725 &amp; 0,900
                   \end{array}
\right]
\]</span></p>
<p>cujos autovalores são <span class="math inline">\(\lambda_1=1,625\)</span> e <span class="math inline">\(\lambda_2=0,1750\)</span> e os autovetores de norma unitária associados são <span class="math inline">\(\mathbf{u}_1=[1/\sqrt{2}\;\;\;\; 1/\sqrt{2}]^{\rm T}\)</span> e <span class="math inline">\(\mathbf{u}_2=[-1/\sqrt{2}\;\;\;\; 1/\sqrt{2}]^{\rm T}\)</span>, respectivamente. Considerando o componente principal <span class="math inline">\(\mathbf{u}_1\)</span>, autovetor associado ao maior autovalor, os dados projetados são calculados como</p>
<p><span class="math display">\[
p_n=\frac{1}{\sqrt{2}}\widetilde{x}_{1n}+\frac{1}{\sqrt{2}}\widetilde{x}_{2n}.
\]</span></p>
<p>Esse componente explica <span class="math inline">\(\%{\rm var}_1=100(1,625)/(1,625+0.1750)=90,28\%\)</span> da variância total. O PCA busca um espaço de dimensão <span class="math inline">\(M=1\)</span>, denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.</p>
<div id="fig-pca-umadim" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-umadim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/fig_pca_umadim.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-umadim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;1: O PCA busca um espaço de dimensão menor conhecido como subespaço principal, denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.
</figcaption>
</figure>
</div>
<p>O PCA pode ser usado para compressão de imagens. Para ilustrar isso, na <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;2</a>(a), consideramos imagem média (<span class="math inline">\(\overline{\mathbf{x}}\)</span>) e os quatro primeiros componentes principais (<span class="math inline">\(\mathbf{u}_1,\cdots,\mathbf{u}_4\)</span>) com os seus autovalores correspondentes baseados em imagens do dígito três da base de dados MNIST. Na <a href="#fig-mnist" class="quarto-xref">Figura&nbsp;2</a>(b), são mostradas a imagem original e as imagens reconstruídas considerando 1, 10, 50 e 250 componentes principais. A medida que se aumenta o valor de <span class="math inline">\(M\)</span>, a reconstrução se torna mais precisa e é perfeita para <span class="math inline">\(M=D=784\)</span>.</p>
<div id="fig-mnist" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/fig_mnist.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;2: (a) Imagem média e os quatro primeiros componentes principais baseados em imagens do dígito três da base de dados MNIST; (b) Imagem original e reconstrução da imagem considerando 1, 10, 50 e 250 componentes principais. Fonte: <span class="citation" data-cites="bishop_pattern_2011">(<a href="#ref-bishop_pattern_2011" role="doc-biblioref">Bishop 2011</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="leitura-adicional" class="level2">
<h2 class="anchored" data-anchor-id="leitura-adicional">Leitura adicional</h2>
<p>O livro <span class="citation" data-cites="jolliffe_principal_2002">(<a href="#ref-jolliffe_principal_2002" role="doc-biblioref">Jolliffe 2002</a>)</span> é uma referência recomendada para quem quiser se aprofundar no assunto.</p>
</section>


</div>





<div id="quarto-appendix" class="default"><section id="autovalores-e-autovetores" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">Autovalores e Autovetores</h2><div class="quarto-appendix-contents">

<p>Seja <span class="math inline">\(\AM\)</span> uma matriz <span class="math inline">\(M\times M\)</span> com elementos constantes. Essa matriz, quando aplicada a um vetor <span class="math inline">\(\xM\)</span> com dimensão <span class="math inline">\(M\times 1\)</span>, resulta em um vetor <span class="math inline">\(\yM\)</span> com dimensão <span class="math inline">\(M\times 1\)</span>, ou seja,</p>
<p><span class="math display">\[\begin{equation}\label{eq:TL}
\yM=\AM\, \xM.
\end{equation}\]</span></p>
<p>Nota-se que a matriz <span class="math inline">\(\AM\)</span> representa uma transformação linear que transforma um vetor <span class="math inline">\(\xM\)</span> no vetor <span class="math inline">\(\yM\)</span>. O vetor <span class="math inline">\(\yM\)</span> pode ser interpretado como resultado da projeção do vetor <span class="math inline">\(\xM\)</span> nas colunas da matriz <span class="math inline">\(\AM\)</span>. De modo geral, essa transformação muda o módulo e a direção do vetor <span class="math inline">\(\xM\)</span>.</p>
<p>Um caso particular de grande interesse prático é aquele em que <span class="math inline">\(\xM\)</span> é um vetor não nulo e <span class="math inline">\(\AM\,\xM\)</span> é um múltiplo escalar de <span class="math inline">\(\xM\)</span>. Para destacar esse vetor <span class="math inline">\(\xM\)</span> dos demais vamos denotá-lo como <span class="math inline">\(\vM\)</span>, assim,</p>
<p><span id="eq-tl-eig"><span class="math display">\[
\begin{equation*}
\fbox{$\displaystyle
\AM\, \vM=\lambda\vM
$}
\end{equation*}
\tag{1}\]</span></span></p>
<p>em que <span class="math inline">\(\lambda\)</span> é uma constante real ou complexa. Nesse caso, a transformação linear aplicada em <span class="math inline">\(\vM\)</span> resulta em um múltiplo escalar dele mesmo. O vetor particular <span class="math inline">\(\xM=\vM\)</span> representa uma direção privilegiada no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>, tal que a ação da transformação <span class="math inline">\(\AM\)</span> sobre o vetor <span class="math inline">\(\vM\)</span> age apenas sobre o módulo desse vetor mantendo a sua direção. O escalar <span class="math inline">\(\lambda\)</span> é chamado de autovalor de <span class="math inline">\(\AM\)</span> e <span class="math inline">\(\vM\)</span> de autovetor associado a <span class="math inline">\(\lambda\)</span>. Cabe observar que para um dado autovalor <span class="math inline">\(\lambda\)</span> podem existir vários vetores não nulos <span class="math inline">\(\vM\)</span> que satisfazem a <a href="#eq-tl-eig" class="quarto-xref">Equação&nbsp;1</a>, como veremos a seguir. Além disso, o autovetor <span class="math inline">\(\vM\)</span> não pode ser nulo, porém, o autovalor <span class="math inline">\(\lambda\)</span> pode ser nulo.</p>
</div></section><section id="a-obtenção-dos-autovalores-e-autovetores" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">A obtenção dos autovalores e autovetores</h2><div class="quarto-appendix-contents">

<p>Por conveniência vamos reescrever a <a href="#eq-tl-eig" class="quarto-xref">Equação&nbsp;1</a> da seguinte forma,</p>
<p><span id="eq-eig-mod"><span class="math display">\[
\begin{equation}
% \AM\, \xM-\lambda\IM\xM=
\left( \AM-\lambda\IM\right)\vM=\mathbf{0},
\end{equation}
\tag{2}\]</span></span></p>
<p>em que <span class="math inline">\(\IM\)</span> denota a matriz identidade de dimensão <span class="math inline">\(M\times M\)</span> e <span class="math inline">\(**0**\)</span> um vetor de zeros de dimensão <span class="math inline">\(M\times 1\)</span>. Nota-se que <span class="math inline">\(\lambda\)</span> é um autovalor da matriz <span class="math inline">\(\AM\)</span> se e somente se a <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a> possui uma solução não trivial.</p>
<p>A partir da <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a>, usando conceitos de solução de sistemas de equações e particularizando para o caso de interesse, seguem as afirmações:</p>
<ul>
<li><p>A <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a> terá uma solução não trivial se e somente se <span class="math inline">\(\left( \AM-\lambda\IM\right)\)</span> for singular, ou seja, <span id="eq-sol1"><span class="math display">\[
\begin{equation*}
\fbox{$\displaystyle
\det\left( \AM-\lambda\IM\right)=0.
$}
\end{equation*}
\tag{3}\]</span></span></p>
<p>Ao aplicar a operação de determinante em <span class="math inline">\(\left( \AM-\lambda\IM\right)\)</span> obtemos um polinômio em <span class="math inline">\(\lambda\)</span>, que representamos como <span id="eq-sol2"><span class="math display">\[
\begin{equation}\label{eq:sol2}
p(\lambda)= \det\left( \AM-\lambda\IM\right)=\lambda^M+c_1\lambda^{M-1}+c_2\lambda^{M-2}\cdots c_M.
\end{equation}
\tag{4}\]</span></span></p>
<p>O polinômio <span class="math inline">\(p(\lambda)\)</span> é chamado de polinômio característico e <span class="math inline">\(p(\lambda)=0\)</span> é chamada de equação característica.</p></li>
<li><p>Nota-se que grau de <span class="math inline">\(p(\lambda)\)</span> é <span class="math inline">\(M\)</span>, portanto, { <span class="math inline">\(p(\lambda)=0\)</span>} possui <span class="math inline">\(M\)</span> soluções. Essas soluções podem ser distintas, repetidas, reais ou complexas. Os valores de <span class="math inline">\(\lambda\)</span> que satisfazem a <a href="#eq-sol2" class="quarto-xref">Equação&nbsp;4</a> são os autovalores da matriz <span class="math inline">\(\AM\)</span>. Portanto, <span class="math inline">\(\AM\)</span> possui <span class="math inline">\(M\)</span> autovalores que podem ser distintos, repetidos, reais ou complexos.</p></li>
<li><p>O conjunto de todas as soluções da <a href="#eq-eig-mod" class="quarto-xref">Equação&nbsp;2</a>, aqui denotada como <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é o espaço nulo de <span class="math inline">\(\AM-\lambda\IM\)</span> e todos os autovetores da matriz <span class="math inline">\(\AM\)</span> estão nesse espaço nulo. Assim, se <span class="math inline">\(\lambda\)</span> é um autovalor de <span class="math inline">\(\AM\)</span>, então, <span class="math inline">\({\cal{N}}\left( \AM-\lambda\IM\right)\neq \{ 0 \}\)</span> e qualquer subespaço não nulo em <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é um autovetor associado a <span class="math inline">\(\lambda\)</span>. O subespaço <span class="math inline">\(\cal{N}\left( \AM-\lambda\IM\right)\)</span> é chamado de autoespaço de <span class="math inline">\(\lambda\)</span>.</p></li>
</ul>

</div></section><section id="exemplos-com-m2" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Exemplos com <span class="math inline">\(M=2\)</span></h2><div class="quarto-appendix-contents">

<p>Nos exemplos a seguir, os autovalores são calculados a partir da <a href="#eq-sol1" class="quarto-xref">Equação&nbsp;3</a> e os autovetores associados são calculados resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=\lambda_1\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=\lambda_2\vM_2\)</span>.</p>
<ul>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cc}
     0 &amp; 0 \\
     0 &amp; 1 \\
   \end{array}
\right]
\]</span> os autovalores e os autovetores associados são <span class="math inline">\(\lambda_1=0\)</span> e <span class="math inline">\(\lambda_2=1\)</span>, e <span class="math display">\[
\begin{array}{ccc}
\vM_1=\left[
  \begin{array}{c}
     1 \\
     0 \\
   \end{array}
\right] &amp; \text{ e} &amp;  \vM_2= \left[
  \begin{array}{c}
     0 \\
     1 \\
   \end{array}
\right]\text{,}
\end{array}
\]</span> respectivamente. Esse exemplo ilustra o fato de que, embora o autovetor não possa ser um vetor nulo, o autovalor pode assumir o valor nulo.</p></li>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cr}
     2 &amp; 0 \\
     0 &amp; -3 \\
   \end{array}
\right]
\]</span> O polinômio característico de <span class="math inline">\(\AM\)</span> é $ p()=(2-)(-3-)$. Portanto, os autovalores de <span class="math inline">\(\AM\)</span> são <span class="math inline">\(\lambda_1=2\)</span> e <span class="math inline">\(\lambda_2=-3\)</span>. Resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=2\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=-3\vM_2\)</span> obtemos os autovetores <span class="math inline">\(\vM_1=[ 1\,\,0]^T\)</span> e <span class="math inline">\(\vM_2=[ 0\,\,1]^T\)</span>, respectivamente. Na <a href="#fig-pca-figxx" class="quarto-xref">Figura&nbsp;3</a>, são mostrados os vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e as suas projeções no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>. Como <span class="math inline">\(\AM\)</span> é uma matriz diagonal, os autovetores coincidem com as coordenadas do espaço Euclidiano.</p></li>
</ul>
<div id="fig-pca-figxx" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-figxx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/pca_figxx.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-figxx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;3: Vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e suas projeções, em que <span class="math inline">\(\AM\)</span> é uma matriz diagonal com autovalores <span class="math inline">\(\lambda_1=2\)</span> e <span class="math inline">\(\lambda_2=-3\)</span>.
</figcaption>
</figure>
</div>
<ul>
<li><p>Seja a matriz <span class="math display">\[
\AM=\left[
   \begin{array}{cr}
     1 &amp; 3 \\
     4 &amp; 2 \\
   \end{array}
\right].
\]</span> O polinômio característico de <span class="math inline">\(\AM\)</span> é <span class="math inline">\(p(\lambda)=\lambda^2-3\lambda-10=(\lambda-5)(\lambda+2)\)</span> e consequentemente os seus autovalores são <span class="math inline">\(\lambda_1=-2\)</span> e <span class="math inline">\(\lambda_2=5\)</span>. Resolvendo os sistemas de equações <span class="math inline">\(\AM\, \vM_1=-2\vM_1\)</span> e <span class="math inline">\(\AM\, \vM_2=5\vM_2\)</span> obtemos os autovetores <span class="math inline">\(\vM_1=[ 1\,\,-1]^T\)</span> e <span class="math inline">\(\vM_2=[ 1\,\,4/3]^T\)</span>, respectivamente.</p>
<p>Na <a href="#fig-pca-figxy" class="quarto-xref">Figura&nbsp;4</a>, são mostrados os vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e as suas projeções no espaço formado pelas colunas da matriz <span class="math inline">\(\AM\)</span>. Nesse caso, como <span class="math inline">\(\AM\)</span> não é uma matriz diagonal, os autovetores não coincidem com as coordenadas do espaço Euclidiano.</p>
<p>De modo geral, nota-se que cada autovalor de <span class="math inline">\(\AM\)</span> possui uma infinidade de autovetores associados. Assim, são também autovetores de <span class="math inline">\(\AM\)</span> os vetores <span class="math inline">\(\bar\vM_1=\vM_1/||\vM_1||=[ 1\,\,-1]^T/\sqrt{2}\)</span> e <span class="math inline">\(\bar\vM_2=\vM_2/||\vM_2||=[ 3\,\,4]^T/5\)</span>.</p>
<p>Os autovetores <span class="math inline">\(\bar\vM_1\)</span> e <span class="math inline">\(\bar\vM_2\)</span> são particularmente interessantes porque possuem norma unitária.</p></li>
</ul>
<div id="fig-pca-figxy" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-figxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="././images/pca_figxy.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-figxy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;4: Vetores <span class="math inline">\(\vM_1\)</span> e <span class="math inline">\(\vM_2\)</span> e suas projeções, em que <span class="math inline">\(\AM\)</span> não é diagonal com autovalores <span class="math inline">\(\lambda_1=-2\)</span> e <span class="math inline">\(\lambda_2=5\)</span>.
</figcaption>
</figure>
</div>
<ul>
<li>No MatLab faça <span class="math inline">\(help\,\, eig\)</span>. Use esse comando para conferir os autovalores e autovetores dos Exemplos <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span> e <span class="math inline">\(3\)</span>. Os autovetores fornecidos pelo MatLab possuem sempre norma unitária.</li>
</ul>
</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Referências</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bishop_pattern_2011" class="csl-entry" role="listitem">
Bishop, Christopher M. 2011. <em>Pattern <span>Recognition</span> and <span>Machine</span> <span>Learning</span></em>. 2006. Corr. 2nd Printing 2011 ed. edição. New York: Springer.
</div>
<div id="ref-jolliffe_principal_2002" class="csl-entry" role="listitem">
Jolliffe, I. T. 2002. <em>Principal <span>Component</span> <span>Analysis</span></em>. 2nd edition. New York: Springer.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Notas de rodapé</h2>

<ol>
<li id="fn1"><p>{O método dos multiplicadores de Lagrange é uma estratégia usada para encontrar mínimos e máximos locais de uma função sujeita a restrições. Para mais detalhes, ver por exemplo, <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">https://en.wikipedia.org/wiki/Lagrange_multiplier</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Lembre que se duas variáveis aleatórias são independentes, elas são não correlacionadas, mas o contrário nem sempre é verdade, ou seja, podemos ter duas variáveis aleatórias não correlacionadas que são dependentes. No entanto, se as variáveis aleatórias forem gaussianas, a não correlação implica independência.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script>
var custom_title = document.querySelectorAll('.custom .theorem-title');

for (let i = 0; i < custom_title.length; i++ ) {
   var mod_name = custom_title[i].innerHTML;
   custom_title[i].innerHTML = mod_name.replace("Exemplo", "Algoritmo");
};
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiada");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiada");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
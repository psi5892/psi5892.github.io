[
  {
    "objectID": "t_medidas.html",
    "href": "t_medidas.html",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Avaliar o modelo é essencial em aprendizado de máquina. Seu modelo pode fornecer resultados satisfatórios quando avaliado por uma determinada métrica, mas ruins quando avaliado por outra. Em problemas de classificação, a precisão geralmente é usada para medir o desempenho do modelo, porém essa métrica não é suficiente. A seguir, vamos abordar diferentes tipos de métricas de avaliação. As métricas estão divididas de acordo com o tipo de problema.\n\n\nVamos voltar ao problema das meias-luas, em que se deseja classificar um determinado ponto como pertencente à Região A ou à Região B. Assim, deve-se responder à pergunta: Dado um determinado ponto, ele pertence à Região A? Há duas possíveis respostas: Sim ou Não, sendo que quando ocorre a resposta negativa, subentende-se que o ponto pertence à Região B. Neste caso, os elementos relevantes são os da Região A. Por exemplo, quando se deseja detectar câncer de mama utilizando imagens de mamografia, os exames classificados como “presença de câncer”, ou seja, como positivos, são os elementos relevantes do problema. Os tipos de erros de classificação estão mostrados na Tabela 1.\n\n\n\nTabela 1: Tipos de erro de classificação.\n\n\n\n\n\n\n\n\n\n\nRótulo verdadeiro\nRótulo predito\nTipo de Erro\n\n\n\n\nSim\nSim\nVerdadeiro Positivo (VP)\n\n\nNão\nNão\nVerdadeiro Negativo (VN)\n\n\nNão\nSim\nFalso Positivo (FP)\n\n\nSim\nNão\nFalso Negativo (FN)\n\n\n\n\n\n\nNa Figura 1, é mostrado um diagrama dos erros de classificação. Os elementos classificados como positivos (elementos recuperados) são os que se encontram no interior da circunferência e são divididos em dois grupos: verdadeiros positivos e falsos positivos. Os elementos classificados como negativos estão fora da circunferência e também são divididos em dois grupos: falsos negativos e verdadeiros negativos.\n\n\n\n\n\n\nFigura 1: Diagrama dos erros de classificação. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n\n\n\n\nA matriz de confusão é uma tabela que contém o resumo dos resultados de um problema de classificação. Os números de predições corretas e incorretas são resumidos com valores de contagem e divididos por cada classe. Ela mostra as maneiras pelas quais seu modelo de classificação fica confuso quando faz predições, fornecendo informações sobre tipos de erros cometidos pelo classificador.\nConsiderando o problema das meias-luas e os tipos de erro de classificação descritos anteriormente a matriz de confusão é dada por\n\n\n\n\n\nPara exemplificar, considere o exemplo de classificação da Tabela 2, em que há 10 predições, sendo \\(\\text{VP}=3\\), \\(\\text{VN}=4\\), \\(\\text{FP}=1\\) e \\(\\text{FN}=2\\).\n\n\n\nTabela 2: Exemplo de classificação.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRótulo Correto\nS\nN\nN\nS\nS\nN\nS\nS\nN\nN\n\n\nRótulo Predito\nN\nN\nN\nS\nS\nN\nN\nS\nN\nS\n\n\nTipo de erro\nFN\nVN\nVN\nVP\nVP\nVN\nFN\nVP\nVN\nFP\n\n\n\n\n\n\nA matriz de confusão do exemplo da Tabela 2 é dada por\n\n\n\n\n\nPara um bom classificador a matriz de confusão deve ter valores na diagonal principal muito maiores que os valores da antidiagonal, que idealmente deveriam ser iguais a zero.\nVale notar que, apesar desta ser a definição mais usual da matriz de confusão, essa definição pode variar em algumas referências ou em algumas bibliotecas de software, como é o caso do scikit-learn, que define a matriz de confusão como a transposta da matriz aqui apresentada, ou seja, com as linhas usadas para representar os valores verdadeiros e as colunas para representar os valores preditos.\n\n\n\nUma vez determinados os erros de classificação, as seguintes métricas podem ser calculadas:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Acurácia}=\\frac{\\# \\text{predições corretas}}{\\# \\text{predições totais}}=\\frac{\\text{VP}+\\text{VN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}\n$}\n\\end{equation*}\\]\ne\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Taxa de erro}=\\frac{\\# \\text{predições incorretas}}{\\# \\text{predições totais}}=\\frac{\\text{FP}+\\text{FN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}=1-\\text{Acurácia}.\n$}\n\\end{equation*}\\]\nConsiderando o exemplo de classificação da Tabela 2, temos\n\\[\n\\text{Acurácia}=\\frac{\\text{VP}+\\text{VN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}=\\frac{3+4}{10}=0,7=70\\%\n\\]\ne\n\\[\n\\text{Taxa de erro}=1-\\text{Acurácia}=1-0,7=0,3=30\\%.\n\\]\nA acurácia não é uma métrica adequada para analisar a classificação quando as classes estão desbalanceadas. Suponha, por exemplo, que \\(97\\%\\) dos exemplos de treinamento sejam da Região B e apenas \\(3\\%\\) da Região A. Se o modelo consegue predizer apenas os pontos da Região B, a acurácia será de \\(97\\%\\) e nenhum ponto da Região A será detectado. Portanto, o modelo parece ter um ótimo desempenho com base na acurácia, mas falha ao detectar pontos da Região A.\n\n\n\nVoltando à Figura 1, considerando que os elementos relevantes são os pontos das Região A, a precisão mede a fração dos elementos relevantes entre os elementos recuperados, ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Precisão}=\\frac{\\# \\text{amostras positivas preditas corretamente}}{\\# \\text{todas as amostras preditas como positivas}}=\\frac{\\text{VP}}{\\text{VP}+\\text{FP}}.\n$}\n\\end{equation*}\\]\nA precisão é uma medida que nos diz qual é a proporção dos pontos que foram classificados como pertencentes à Região A que de fato são da Região A.\nEm contrapartida, a sensibilidade, também conhecida como taxa de verdadeiros positivos (True Positive Rate - TPR) ou revocação (recall), calcula a fração dos elementos relevantes que foram recuperados, ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Sensibilidade}=\\frac{\\# \\text{amostras positivas preditas corretamente}}{\\# \\text{todas as amostras com rótulos positivos}}=\\frac{\\text{VP}}{\\text{VP}+\\text{FN}}.\n$}\n\\end{equation*}\\]\nA sensibilidade é uma medida que nos diz qual é a proporção dos pontos que de fato são da Região A que foram classificados como pertencentes a essa região.\nUsando o diagrama da Figura 1, as métricas acima estão ilustradas na Figura 2.\n\n\n\n\n\n\nFigura 2: Ilustração do cálculo da precisão e sensibilidade seguindo o esquema da Figura 1. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n\nA sensibilidade fornece informação sobre o desempenho do classificador com relação aos falsos negativos, enquanto a precisão fornece informação sobre seu desempenho em relação aos falsos positivos. Dessa forma, se desejamos minimizar os falsos negativos, a sensibilidade deve ser o mais próxima possível de 100% sem que a precisão seja muito ruim. Em contrapartida, se desejamos minimizar os falsos positivos, deve-se fazer a precisão tão próxima de 100% quanto possível.\nVale notar que existe um compromisso entre o desempenho em termos de precisão e sensibilidade. Em geral, quando busca-se aumentar a precisão, diminuindo o número de falsos positivos, diminui-se sensibilidade, pois o número de falsos negativos aumenta e vice-versa. Dependendo do problema, pode ser mais interessante priorizar uma ou outra métrica, alterando-se o limiar utilizado para classificar um exemplo positivo. Um classificador com alta precisão tende a deixar alguns exemplos de fora (falsos negativos) mas aqueles classificados como positivo tem uma alta qualidade (poucos falsos positivos). Já um classificador com uma sensibilidade alta, deixa poucos exemplos de fora (falsos negativos) mas aqueles classificados como positivo não tem tanta qualidade (mais falsos positivos).\nUma métrica que combina a precisão e a sensibilidade é a \\(F_1\\textit{-score}\\) definida como a média harmônica dessas métricas, isto é,\n\\[\nF_1\\textit{-score}=2\\frac{\\text{Precisão}\\times\\text{Sensibilidade}}{\\text{Precisão}+\\text{Sensibilidade}}.\n\\]\nA média harmônica é uma espécie de média quando a precisão e a sensibilidade são iguais. Mas quando elas são diferentes, essa métrica fica mais próxima do menor valor em comparação com o maior. Portanto, se a precisão ou sensibilidade for muito pequena, o \\(F_1\\textit{-score}\\) levanta uma bandeira e fica mais próximo do menor valor, dando ao modelo uma pontuação apropriada em vez de apenas uma simples média aritmética.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{Precisão}=\\frac{\\text{VP}}{\\text{VP}+\\text{FP}}=\\frac{3}{3+1}=0,75=75\\%,\n\\]\n\\[\n\\text{Sensibilidade}=\\frac{\\text{VP}}{\\text{VP}+\\text{FN}}=\\frac{3}{3+2}=0,6=60\\%,\n\\]\ne\n\\[\nF_1\\textit{-score}=2\\frac{0,75\\times 0,60}{0,75+0,60}=0,6667=66,67\\%.\n\\]\nNa literatura, existem outras medidas do tipo \\(F\\) que envolvem a precisão e sensibilidade, mas a \\(F_1\\textit{-score}\\) é a mais utilizada.\n\n\n\nA especificidade, em contraste à sensibilidade, é uma medida que fornece a proporção de pontos que não pertencem à Região A e que foram previstos pelo modelo como não pertencentes à essa região. Considera-se então a razão entre o número de pontos classificados corretamente como não pertencentes à Região A, ou seja, os pontos que são VN, e o número de pontos que de fato não pertencem à Região~A, ou seja, os pontos que são FP e VN. Dessa forma, define-se essa métrica como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Especificidade}=\\frac{\\# \\text{amostras negativas preditas corretamente}}{\\# \\text{todas as amostras preditas com rótulos negativos}}=\\frac{\\text{VN}}{\\text{VN}+\\text{FP}}.\n$}\n\\end{equation*}\\]\nUsando o diagrama da Figura 1, a especificidade está ilustrada na Figura 3.\n\n\n\n\n\n\nFigura 3: Ilustração do cálculo da especificidade seguindo o esquema da Figura 1. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n\nTambém é comum definir a taxa de falsos positivos (False Positive Rate - FPR), ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{FPR}=\\frac{\\text{FP}}{\\text{VN}+\\text{FP}}=1-\\text{Especificidade}.\n$}\n\\end{equation*}\\]\nEssa taxa fornece a proporção da classe negativa (pontos da Região B no nosso exemplo), que foi classificada incorretamente. Uma FPR baixa é desejável uma vez que se deseja classificar corretamente os elementos da classe negativa.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{Especificidade}=\\frac{\\text{VN}}{\\text{VN}+\\text{FP}}=\\frac{4}{4+1}=0,8=80\\%.\n\\]\n\n\n\nPara classificação binária, uma métrica pouco utilizada na prática, mas muito interessante é o coeficiente de correlação de Matthew (Matthews Correlation Coefficient - MCC). Para obter essa métrica, trate a classe verdadeira e a classe prevista como duas variáveis aleatórias binárias e calcule seu coeficiente de correlação. Quanto maior a correlação entre os valores verdadeiros e preditos, melhor a predição. Esse coeficiente de correlação é chamado de MCC quando aplicado a classificadores e é definido como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{MCC}=\\frac{\\text{VP}\\times\\text{VN}-\\text{FP}\\times\\text{FN}}{\\sqrt{(\\text{VP}+\\text{FP})(\\text{VP}+\\text{FN})(\\text{VN}+\\text{FP})(\\text{VN}+\\text{FN})}}.\n$}\n\\end{equation*}\\]\nAlgumas propriedades interessantes dessa métrica podem ser derivadas de sua definição: quando o classificador é perfeito (\\(\\text{FP} = \\text{FN} = 0\\)) o valor de MCC é \\(1\\), indicando correlação positiva perfeita. Em contrapartida, quando o classificador sempre classifica mal (\\(\\text{VP} = \\text{VN} = 0\\)), obtemos um valor de \\(-1\\), representando uma correlação negativa perfeita (neste caso, você pode simplesmente reverter o resultado do classificador para obter o classificador ideal). O valor do MCC está sempre entre \\(-1\\) e \\(1\\), com \\(0\\) significando que o classificador não é melhor do que um lançamento aleatório de uma moeda honesta. O MCC também é perfeitamente simétrico, portanto, nenhuma classe é mais importante que a outra. Por fim, o MCC leva em conta os quatro valores da matriz de confusão, e um valor alto (próximo de 1) significa que ambas as classes são bem preditas, mesmo que uma classe esteja sub (ou super) representada.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{MCC}=\n\\frac{3\\times4-1\\times2}{\\sqrt{(3+1)(3+2)(4+1)(4+2)}}=\\frac{10}{\\sqrt{600}}=0,408.\n\\]\n\n\n\nNo problema das meias-luas, adotamos o limiar igual a 0. Ou seja, quando a saída do classificador é maior ou igual a zero, o ponto é classificado como pertencente à Região A e quando é menor que zero como pertencente à Região B. Esse limiar é razoável neste caso, uma vez que adotamos \\(d\\in \\{-1, +1\\}\\) como sinal desejado. No entanto, suponha que a saída de um determinado classificador treinado para este problema siga duas distribuições gaussianas com variância unitária, mas com médias diferentes: uma igual \\(-4\\) e outra igual a \\(1,5\\), como mostrado na Figura 4. A gaussiana com média negativa corresponde aos pontos que devem ser classificados como pertencentes à Região B. Enquanto a gaussiana com média positiva corresponde aos pontos que devem ser classificados como pertencentes à Região A. Neste caso, para minimizar o erro de classificação, o limiar deve ser igual a \\(-1,25\\) e não \\(0\\). O valor do limiar influencia nas medidas de desempenho do classificador. Neste exemplo, qualquer outro limiar diferente de \\(-1,25\\) levará a um maior erro de classificação. Na escolha do limiar, devem ser feitos testes para cada valor, ou seja, pode-se gerar a matriz de confusão e comparar as métricas discutidas até agora. No entanto, o melhor a se fazer é considerar a área sob a curva ROC como veremos a seguir.\n\n\n\n\n\n\nFigura 4: Exemplo de função densidade de probabilidade da saída do classificador. A curva vermelha mostra a distribuição da saída ser negativa e a da direita da saída ser positiva.\n\n\n\nDado um limiar, a curva ROC (Receiver Operator Characteristic) é um gráfico da taxa de verdadeiros positivos (sensibilidade) em função da taxa de falsos positivos (\\(\\text{FPR}=1-\\text{Especificidade}\\)). A área sob essa curva (area under the curve - AUC) é uma medida da habilidade do classificador de distinguir entre as classes e é usada frequentemente como métrica. Quanto maior a AUC, melhor o desempenho do classificador. Na Figura 5, são mostrados exemplos de curvas ROC com três valores distintos de AUC: \\(1\\), \\(0,\\!5\\) e \\(0,\\!8\\). As áreas sob as curvas ROC estão pintadas de amarelo. Quando \\(\\text{AUC}=1\\) (Figura 5 (a)), o classificador é capaz de distinguir perfeitamente os pontos da Região A dos pontos da Região B. Se \\(\\text{AUC}=0\\), o modelo classifica todos os pontos da Região A como pertencentes a Região B e vice-versa. Quando \\(\\text{AUC}=0,5\\) (Figura 5 (b)), o classificador não é capaz de distinguir entre as classes: é como se uma moeda fosse lançada para gerar a classificação. Por fim, para \\(0,\\!5 &lt; \\text{AUC}&lt; 1\\) (Figura 5 (c)), há uma grande chance de que o classificador seja capaz de distinguir as classes. Isso ocorre porque o classificador detecta mais verdadeiros positivos e verdadeiros negativos do que falsos negativos e falsos positivos.\n\n\n\n\n\n\nFigura 5: Exemplos de curvas ROC com três valores distintos de AUC: (a) \\(\\text{AUC}=1\\), (b) \\(\\text{AUC}=0,\\!5\\) e (c) \\(\\text{AUC}=0,\\!8\\).\n\n\n\nEm uma curva ROC, um valor mais alto do eixo \\(x\\) indica um maior número de falsos positivos do que de verdadeiros negativos. Enquanto um valor mais alto do eixo \\(y\\) indica um maior número de verdadeiros positivos do que falsos negativos. Assim, a escolha do limiar deve levar em conta um compromisso entre o número de falsos positivos e falsos negativos.\n\n\n\nAté agora, quando abordamos as métricas de desempenho, focamos em problemas de classificação binária. No entanto, essas métricas podem ser estendidas para problemas de classificação multiclasse. Um bom tutorial sobre o assunto pode ser encontrado em (Grandini, Bagli, e Visani 2020).\n\n\n\nAs medidas de desempenho vistas até o momento não servem para problemas de regressão. A seguir vamos abordar três medidas que podem ser usadas para avaliar modelos de regressão.\n\n\nO erro absoluto médio (mean absolute error - MAE) é a média do módulo da diferença entre os valores desejados e os valores preditos. Ele fornece uma medida de quão longe as predições estão dos valores verdadeiro, mas não fornece nenhuma ideia da direção do erro, ou seja, se o modelo está subestimando ou superestimando os dados. O erro absoluto médio é calculado como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm MAE}=\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}|d_n-y_n|.\n$}\n\\end{equation*}\\]\n\n\n\nO erro quadrático médio (mean square error - MSE) é semelhante ao erro absoluto médio. A única diferença é que o MSE toma a média do quadrado da diferença entre os valores desejados e os valores preditos. Como se calcula o quadrado do erro, o efeito de erros maiores se torna mais pronunciado do que o de erros menores. O MSE é calculado como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm MSE}=\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2.\n$}\n\\end{equation*}\\]\nÉ comum usar também a raiz quadrada do MSE (root mean square error - RMSE) como métrica, ou seja, \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm RMSE}=\\sqrt{{\\rm MSE}}=\\sqrt{\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2}.\n$}\n\\end{equation*}\\]\nA vantagem é que tanto essa métrica como o MAE possuem a mesma unidade da variável predita, o que torna sua interpretação mais simples.\n\n\n\nA métrica \\(R^2\\) é usada para fins explicativos e fornece uma indicação da qualidade ou ajuste de um conjunto de valores de saída previstos aos valores desejados. Essa métrica é calculada como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\nR^2=1-\\displaystyle\\frac{\\displaystyle\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2}{\\displaystyle\\sum_{n=1}^{N_{\\text{teste}}}(\\overline{d}-d_n)^2},\n$}\n\\end{equation*}\\]\nem que\n\\[\n\\overline{d}=\\frac{1}{N_{\\text{teste}}}\\sum_{n=1}^{N_{\\text{teste}}}d_n\n\\]\né a média dos valores desejados do conjunto de teste. Note que o denominador da fração que aparece na definição de \\(R^2\\) é proporcional à variância dos dados de teste. No melhor caso, os valores preditos são exatamente iguais aos valores desejados, o que leva a \\(R^2=1\\). Caso o modelo leve a \\(y_n=\\overline{d}\\), \\(n=1, 2, \\cdots, N_{\\text{teste}}\\), que é conhecido como modelo base, teremos \\(R^2=0\\). Modelos cujas predições são piores que as do modelo base podem levar a \\(R^2\\) negativo."
  },
  {
    "objectID": "t_medidas.html#problemas-de-classificação",
    "href": "t_medidas.html#problemas-de-classificação",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Vamos voltar ao problema das meias-luas, em que se deseja classificar um determinado ponto como pertencente à Região A ou à Região B. Assim, deve-se responder à pergunta: Dado um determinado ponto, ele pertence à Região A? Há duas possíveis respostas: Sim ou Não, sendo que quando ocorre a resposta negativa, subentende-se que o ponto pertence à Região B. Neste caso, os elementos relevantes são os da Região A. Por exemplo, quando se deseja detectar câncer de mama utilizando imagens de mamografia, os exames classificados como “presença de câncer”, ou seja, como positivos, são os elementos relevantes do problema. Os tipos de erros de classificação estão mostrados na Tabela 1.\n\n\n\nTabela 1: Tipos de erro de classificação.\n\n\n\n\n\n\n\n\n\n\nRótulo verdadeiro\nRótulo predito\nTipo de Erro\n\n\n\n\nSim\nSim\nVerdadeiro Positivo (VP)\n\n\nNão\nNão\nVerdadeiro Negativo (VN)\n\n\nNão\nSim\nFalso Positivo (FP)\n\n\nSim\nNão\nFalso Negativo (FN)\n\n\n\n\n\n\nNa Figura 1, é mostrado um diagrama dos erros de classificação. Os elementos classificados como positivos (elementos recuperados) são os que se encontram no interior da circunferência e são divididos em dois grupos: verdadeiros positivos e falsos positivos. Os elementos classificados como negativos estão fora da circunferência e também são divididos em dois grupos: falsos negativos e verdadeiros negativos.\n\n\n\n\n\n\nFigura 1: Diagrama dos erros de classificação. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall"
  },
  {
    "objectID": "t_medidas.html#matriz-de-confusão",
    "href": "t_medidas.html#matriz-de-confusão",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "A matriz de confusão é uma tabela que contém o resumo dos resultados de um problema de classificação. Os números de predições corretas e incorretas são resumidos com valores de contagem e divididos por cada classe. Ela mostra as maneiras pelas quais seu modelo de classificação fica confuso quando faz predições, fornecendo informações sobre tipos de erros cometidos pelo classificador.\nConsiderando o problema das meias-luas e os tipos de erro de classificação descritos anteriormente a matriz de confusão é dada por\n\n\n\n\n\nPara exemplificar, considere o exemplo de classificação da Tabela 2, em que há 10 predições, sendo \\(\\text{VP}=3\\), \\(\\text{VN}=4\\), \\(\\text{FP}=1\\) e \\(\\text{FN}=2\\).\n\n\n\nTabela 2: Exemplo de classificação.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRótulo Correto\nS\nN\nN\nS\nS\nN\nS\nS\nN\nN\n\n\nRótulo Predito\nN\nN\nN\nS\nS\nN\nN\nS\nN\nS\n\n\nTipo de erro\nFN\nVN\nVN\nVP\nVP\nVN\nFN\nVP\nVN\nFP\n\n\n\n\n\n\nA matriz de confusão do exemplo da Tabela 2 é dada por\n\n\n\n\n\nPara um bom classificador a matriz de confusão deve ter valores na diagonal principal muito maiores que os valores da antidiagonal, que idealmente deveriam ser iguais a zero.\nVale notar que, apesar desta ser a definição mais usual da matriz de confusão, essa definição pode variar em algumas referências ou em algumas bibliotecas de software, como é o caso do scikit-learn, que define a matriz de confusão como a transposta da matriz aqui apresentada, ou seja, com as linhas usadas para representar os valores verdadeiros e as colunas para representar os valores preditos."
  },
  {
    "objectID": "t_medidas.html#acurácia-e-taxa-de-erro",
    "href": "t_medidas.html#acurácia-e-taxa-de-erro",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Uma vez determinados os erros de classificação, as seguintes métricas podem ser calculadas:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Acurácia}=\\frac{\\# \\text{predições corretas}}{\\# \\text{predições totais}}=\\frac{\\text{VP}+\\text{VN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}\n$}\n\\end{equation*}\\]\ne\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Taxa de erro}=\\frac{\\# \\text{predições incorretas}}{\\# \\text{predições totais}}=\\frac{\\text{FP}+\\text{FN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}=1-\\text{Acurácia}.\n$}\n\\end{equation*}\\]\nConsiderando o exemplo de classificação da Tabela 2, temos\n\\[\n\\text{Acurácia}=\\frac{\\text{VP}+\\text{VN}}{\\text{VP}+\\text{VN}+\\text{FP}+\\text{FN}}=\\frac{3+4}{10}=0,7=70\\%\n\\]\ne\n\\[\n\\text{Taxa de erro}=1-\\text{Acurácia}=1-0,7=0,3=30\\%.\n\\]\nA acurácia não é uma métrica adequada para analisar a classificação quando as classes estão desbalanceadas. Suponha, por exemplo, que \\(97\\%\\) dos exemplos de treinamento sejam da Região B e apenas \\(3\\%\\) da Região A. Se o modelo consegue predizer apenas os pontos da Região B, a acurácia será de \\(97\\%\\) e nenhum ponto da Região A será detectado. Portanto, o modelo parece ter um ótimo desempenho com base na acurácia, mas falha ao detectar pontos da Região A."
  },
  {
    "objectID": "t_medidas.html#precisão-sensibilidade-e-f_1-score",
    "href": "t_medidas.html#precisão-sensibilidade-e-f_1-score",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Voltando à Figura 1, considerando que os elementos relevantes são os pontos das Região A, a precisão mede a fração dos elementos relevantes entre os elementos recuperados, ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Precisão}=\\frac{\\# \\text{amostras positivas preditas corretamente}}{\\# \\text{todas as amostras preditas como positivas}}=\\frac{\\text{VP}}{\\text{VP}+\\text{FP}}.\n$}\n\\end{equation*}\\]\nA precisão é uma medida que nos diz qual é a proporção dos pontos que foram classificados como pertencentes à Região A que de fato são da Região A.\nEm contrapartida, a sensibilidade, também conhecida como taxa de verdadeiros positivos (True Positive Rate - TPR) ou revocação (recall), calcula a fração dos elementos relevantes que foram recuperados, ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Sensibilidade}=\\frac{\\# \\text{amostras positivas preditas corretamente}}{\\# \\text{todas as amostras com rótulos positivos}}=\\frac{\\text{VP}}{\\text{VP}+\\text{FN}}.\n$}\n\\end{equation*}\\]\nA sensibilidade é uma medida que nos diz qual é a proporção dos pontos que de fato são da Região A que foram classificados como pertencentes a essa região.\nUsando o diagrama da Figura 1, as métricas acima estão ilustradas na Figura 2.\n\n\n\n\n\n\nFigura 2: Ilustração do cálculo da precisão e sensibilidade seguindo o esquema da Figura 1. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n\nA sensibilidade fornece informação sobre o desempenho do classificador com relação aos falsos negativos, enquanto a precisão fornece informação sobre seu desempenho em relação aos falsos positivos. Dessa forma, se desejamos minimizar os falsos negativos, a sensibilidade deve ser o mais próxima possível de 100% sem que a precisão seja muito ruim. Em contrapartida, se desejamos minimizar os falsos positivos, deve-se fazer a precisão tão próxima de 100% quanto possível.\nVale notar que existe um compromisso entre o desempenho em termos de precisão e sensibilidade. Em geral, quando busca-se aumentar a precisão, diminuindo o número de falsos positivos, diminui-se sensibilidade, pois o número de falsos negativos aumenta e vice-versa. Dependendo do problema, pode ser mais interessante priorizar uma ou outra métrica, alterando-se o limiar utilizado para classificar um exemplo positivo. Um classificador com alta precisão tende a deixar alguns exemplos de fora (falsos negativos) mas aqueles classificados como positivo tem uma alta qualidade (poucos falsos positivos). Já um classificador com uma sensibilidade alta, deixa poucos exemplos de fora (falsos negativos) mas aqueles classificados como positivo não tem tanta qualidade (mais falsos positivos).\nUma métrica que combina a precisão e a sensibilidade é a \\(F_1\\textit{-score}\\) definida como a média harmônica dessas métricas, isto é,\n\\[\nF_1\\textit{-score}=2\\frac{\\text{Precisão}\\times\\text{Sensibilidade}}{\\text{Precisão}+\\text{Sensibilidade}}.\n\\]\nA média harmônica é uma espécie de média quando a precisão e a sensibilidade são iguais. Mas quando elas são diferentes, essa métrica fica mais próxima do menor valor em comparação com o maior. Portanto, se a precisão ou sensibilidade for muito pequena, o \\(F_1\\textit{-score}\\) levanta uma bandeira e fica mais próximo do menor valor, dando ao modelo uma pontuação apropriada em vez de apenas uma simples média aritmética.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{Precisão}=\\frac{\\text{VP}}{\\text{VP}+\\text{FP}}=\\frac{3}{3+1}=0,75=75\\%,\n\\]\n\\[\n\\text{Sensibilidade}=\\frac{\\text{VP}}{\\text{VP}+\\text{FN}}=\\frac{3}{3+2}=0,6=60\\%,\n\\]\ne\n\\[\nF_1\\textit{-score}=2\\frac{0,75\\times 0,60}{0,75+0,60}=0,6667=66,67\\%.\n\\]\nNa literatura, existem outras medidas do tipo \\(F\\) que envolvem a precisão e sensibilidade, mas a \\(F_1\\textit{-score}\\) é a mais utilizada."
  },
  {
    "objectID": "t_medidas.html#especificidade-e-taxa-de-falsos-positivos",
    "href": "t_medidas.html#especificidade-e-taxa-de-falsos-positivos",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "A especificidade, em contraste à sensibilidade, é uma medida que fornece a proporção de pontos que não pertencem à Região A e que foram previstos pelo modelo como não pertencentes à essa região. Considera-se então a razão entre o número de pontos classificados corretamente como não pertencentes à Região A, ou seja, os pontos que são VN, e o número de pontos que de fato não pertencem à Região~A, ou seja, os pontos que são FP e VN. Dessa forma, define-se essa métrica como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{Especificidade}=\\frac{\\# \\text{amostras negativas preditas corretamente}}{\\# \\text{todas as amostras preditas com rótulos negativos}}=\\frac{\\text{VN}}{\\text{VN}+\\text{FP}}.\n$}\n\\end{equation*}\\]\nUsando o diagrama da Figura 1, a especificidade está ilustrada na Figura 3.\n\n\n\n\n\n\nFigura 3: Ilustração do cálculo da especificidade seguindo o esquema da Figura 1. Fonte: Adaptado de https://en.wikipedia.org/wiki/Precision_and_recall\n\n\n\nTambém é comum definir a taxa de falsos positivos (False Positive Rate - FPR), ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{FPR}=\\frac{\\text{FP}}{\\text{VN}+\\text{FP}}=1-\\text{Especificidade}.\n$}\n\\end{equation*}\\]\nEssa taxa fornece a proporção da classe negativa (pontos da Região B no nosso exemplo), que foi classificada incorretamente. Uma FPR baixa é desejável uma vez que se deseja classificar corretamente os elementos da classe negativa.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{Especificidade}=\\frac{\\text{VN}}{\\text{VN}+\\text{FP}}=\\frac{4}{4+1}=0,8=80\\%.\n\\]"
  },
  {
    "objectID": "t_medidas.html#coeficiente-de-correlação-de-matthew",
    "href": "t_medidas.html#coeficiente-de-correlação-de-matthew",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Para classificação binária, uma métrica pouco utilizada na prática, mas muito interessante é o coeficiente de correlação de Matthew (Matthews Correlation Coefficient - MCC). Para obter essa métrica, trate a classe verdadeira e a classe prevista como duas variáveis aleatórias binárias e calcule seu coeficiente de correlação. Quanto maior a correlação entre os valores verdadeiros e preditos, melhor a predição. Esse coeficiente de correlação é chamado de MCC quando aplicado a classificadores e é definido como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\text{MCC}=\\frac{\\text{VP}\\times\\text{VN}-\\text{FP}\\times\\text{FN}}{\\sqrt{(\\text{VP}+\\text{FP})(\\text{VP}+\\text{FN})(\\text{VN}+\\text{FP})(\\text{VN}+\\text{FN})}}.\n$}\n\\end{equation*}\\]\nAlgumas propriedades interessantes dessa métrica podem ser derivadas de sua definição: quando o classificador é perfeito (\\(\\text{FP} = \\text{FN} = 0\\)) o valor de MCC é \\(1\\), indicando correlação positiva perfeita. Em contrapartida, quando o classificador sempre classifica mal (\\(\\text{VP} = \\text{VN} = 0\\)), obtemos um valor de \\(-1\\), representando uma correlação negativa perfeita (neste caso, você pode simplesmente reverter o resultado do classificador para obter o classificador ideal). O valor do MCC está sempre entre \\(-1\\) e \\(1\\), com \\(0\\) significando que o classificador não é melhor do que um lançamento aleatório de uma moeda honesta. O MCC também é perfeitamente simétrico, portanto, nenhuma classe é mais importante que a outra. Por fim, o MCC leva em conta os quatro valores da matriz de confusão, e um valor alto (próximo de 1) significa que ambas as classes são bem preditas, mesmo que uma classe esteja sub (ou super) representada.\nNo exemplo da Tabela 2, temos\n\\[\n\\text{MCC}=\n\\frac{3\\times4-1\\times2}{\\sqrt{(3+1)(3+2)(4+1)(4+2)}}=\\frac{10}{\\sqrt{600}}=0,408.\n\\]"
  },
  {
    "objectID": "t_medidas.html#área-sob-a-curva-roc",
    "href": "t_medidas.html#área-sob-a-curva-roc",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "No problema das meias-luas, adotamos o limiar igual a 0. Ou seja, quando a saída do classificador é maior ou igual a zero, o ponto é classificado como pertencente à Região A e quando é menor que zero como pertencente à Região B. Esse limiar é razoável neste caso, uma vez que adotamos \\(d\\in \\{-1, +1\\}\\) como sinal desejado. No entanto, suponha que a saída de um determinado classificador treinado para este problema siga duas distribuições gaussianas com variância unitária, mas com médias diferentes: uma igual \\(-4\\) e outra igual a \\(1,5\\), como mostrado na Figura 4. A gaussiana com média negativa corresponde aos pontos que devem ser classificados como pertencentes à Região B. Enquanto a gaussiana com média positiva corresponde aos pontos que devem ser classificados como pertencentes à Região A. Neste caso, para minimizar o erro de classificação, o limiar deve ser igual a \\(-1,25\\) e não \\(0\\). O valor do limiar influencia nas medidas de desempenho do classificador. Neste exemplo, qualquer outro limiar diferente de \\(-1,25\\) levará a um maior erro de classificação. Na escolha do limiar, devem ser feitos testes para cada valor, ou seja, pode-se gerar a matriz de confusão e comparar as métricas discutidas até agora. No entanto, o melhor a se fazer é considerar a área sob a curva ROC como veremos a seguir.\n\n\n\n\n\n\nFigura 4: Exemplo de função densidade de probabilidade da saída do classificador. A curva vermelha mostra a distribuição da saída ser negativa e a da direita da saída ser positiva.\n\n\n\nDado um limiar, a curva ROC (Receiver Operator Characteristic) é um gráfico da taxa de verdadeiros positivos (sensibilidade) em função da taxa de falsos positivos (\\(\\text{FPR}=1-\\text{Especificidade}\\)). A área sob essa curva (area under the curve - AUC) é uma medida da habilidade do classificador de distinguir entre as classes e é usada frequentemente como métrica. Quanto maior a AUC, melhor o desempenho do classificador. Na Figura 5, são mostrados exemplos de curvas ROC com três valores distintos de AUC: \\(1\\), \\(0,\\!5\\) e \\(0,\\!8\\). As áreas sob as curvas ROC estão pintadas de amarelo. Quando \\(\\text{AUC}=1\\) (Figura 5 (a)), o classificador é capaz de distinguir perfeitamente os pontos da Região A dos pontos da Região B. Se \\(\\text{AUC}=0\\), o modelo classifica todos os pontos da Região A como pertencentes a Região B e vice-versa. Quando \\(\\text{AUC}=0,5\\) (Figura 5 (b)), o classificador não é capaz de distinguir entre as classes: é como se uma moeda fosse lançada para gerar a classificação. Por fim, para \\(0,\\!5 &lt; \\text{AUC}&lt; 1\\) (Figura 5 (c)), há uma grande chance de que o classificador seja capaz de distinguir as classes. Isso ocorre porque o classificador detecta mais verdadeiros positivos e verdadeiros negativos do que falsos negativos e falsos positivos.\n\n\n\n\n\n\nFigura 5: Exemplos de curvas ROC com três valores distintos de AUC: (a) \\(\\text{AUC}=1\\), (b) \\(\\text{AUC}=0,\\!5\\) e (c) \\(\\text{AUC}=0,\\!8\\).\n\n\n\nEm uma curva ROC, um valor mais alto do eixo \\(x\\) indica um maior número de falsos positivos do que de verdadeiros negativos. Enquanto um valor mais alto do eixo \\(y\\) indica um maior número de verdadeiros positivos do que falsos negativos. Assim, a escolha do limiar deve levar em conta um compromisso entre o número de falsos positivos e falsos negativos."
  },
  {
    "objectID": "t_medidas.html#problemas-de-classificação-multiclasse",
    "href": "t_medidas.html#problemas-de-classificação-multiclasse",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "Até agora, quando abordamos as métricas de desempenho, focamos em problemas de classificação binária. No entanto, essas métricas podem ser estendidas para problemas de classificação multiclasse. Um bom tutorial sobre o assunto pode ser encontrado em (Grandini, Bagli, e Visani 2020)."
  },
  {
    "objectID": "t_medidas.html#problemas-de-regressão",
    "href": "t_medidas.html#problemas-de-regressão",
    "title": "Medidas de desempenho",
    "section": "",
    "text": "As medidas de desempenho vistas até o momento não servem para problemas de regressão. A seguir vamos abordar três medidas que podem ser usadas para avaliar modelos de regressão.\n\n\nO erro absoluto médio (mean absolute error - MAE) é a média do módulo da diferença entre os valores desejados e os valores preditos. Ele fornece uma medida de quão longe as predições estão dos valores verdadeiro, mas não fornece nenhuma ideia da direção do erro, ou seja, se o modelo está subestimando ou superestimando os dados. O erro absoluto médio é calculado como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm MAE}=\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}|d_n-y_n|.\n$}\n\\end{equation*}\\]\n\n\n\nO erro quadrático médio (mean square error - MSE) é semelhante ao erro absoluto médio. A única diferença é que o MSE toma a média do quadrado da diferença entre os valores desejados e os valores preditos. Como se calcula o quadrado do erro, o efeito de erros maiores se torna mais pronunciado do que o de erros menores. O MSE é calculado como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm MSE}=\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2.\n$}\n\\end{equation*}\\]\nÉ comum usar também a raiz quadrada do MSE (root mean square error - RMSE) como métrica, ou seja, \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n{\\rm RMSE}=\\sqrt{{\\rm MSE}}=\\sqrt{\\frac{1}{N_\\text{teste}}\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2}.\n$}\n\\end{equation*}\\]\nA vantagem é que tanto essa métrica como o MAE possuem a mesma unidade da variável predita, o que torna sua interpretação mais simples.\n\n\n\nA métrica \\(R^2\\) é usada para fins explicativos e fornece uma indicação da qualidade ou ajuste de um conjunto de valores de saída previstos aos valores desejados. Essa métrica é calculada como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\nR^2=1-\\displaystyle\\frac{\\displaystyle\\sum_{n=1}^{N_{\\text{teste}}}(d_n-y_n)^2}{\\displaystyle\\sum_{n=1}^{N_{\\text{teste}}}(\\overline{d}-d_n)^2},\n$}\n\\end{equation*}\\]\nem que\n\\[\n\\overline{d}=\\frac{1}{N_{\\text{teste}}}\\sum_{n=1}^{N_{\\text{teste}}}d_n\n\\]\né a média dos valores desejados do conjunto de teste. Note que o denominador da fração que aparece na definição de \\(R^2\\) é proporcional à variância dos dados de teste. No melhor caso, os valores preditos são exatamente iguais aos valores desejados, o que leva a \\(R^2=1\\). Caso o modelo leve a \\(y_n=\\overline{d}\\), \\(n=1, 2, \\cdots, N_{\\text{teste}}\\), que é conhecido como modelo base, teremos \\(R^2=0\\). Modelos cujas predições são piores que as do modelo base podem levar a \\(R^2\\) negativo."
  },
  {
    "objectID": "ex_aula_medidas.html",
    "href": "ex_aula_medidas.html",
    "title": "Exercício - Medidas de desempenho",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_lda.html",
    "href": "t_lda.html",
    "title": "Análise de Discriminantes Lineares",
    "section": "",
    "text": "\\[\n\\newcommand{\\Her}{\\scriptscriptstyle H}\n\\newcommand{\\T}{\\rm T}\n%\\newcommand{\\R}{\\scriptscriptstyle R}\n\\newcommand{\\I}{\\scriptscriptstyle I}\n\\newcommand{\\mT}{\\scriptscriptstyle -T}\n\\newcommand{\\vir}{,\\hspace{-0.15ex}}\n\\newcommand{\\M}{\\scriptscriptstyle M}\n\\newcommand{\\RNN}{\\rm \\scriptscriptstyle RNN}\n\\newcommand{\\E}{{\\rm E}}\n\\newcommand{\\uM}{{\\mathbf u}}\n\\newcommand{\\aM}{{\\mathbf a}}\n\\newcommand{\\bM}{{\\mathbf b}}\n\\newcommand{\\cM}{{\\mathbf c}}\n\\newcommand{\\sM}{{\\mathbf s}}\n\\newcommand{\\SM}{{\\mathbf S}}\n\\newcommand{\\vM}{{\\mathbf v}}\n\\newcommand{\\UM}{{\\mathbf U}}\n\\newcommand{\\VM}{{\\mathbf V}}\n\\newcommand{\\RM}{{\\mathbf R}}\n\\newcommand{\\DM}{{\\mathbf D}}\n\\newcommand{\\PM}{{\\mathbf P}}\n\\newcommand{\\HM}{{\\mathbf H}}\n\\newcommand{\\LM}{{\\mathbf L}}\n\\newcommand{\\AM}{{\\mathbf A}}\n\\newcommand{\\FM}{{\\mathbf F}}\n\\newcommand{\\BM}{{\\mathbf B}}\n\\newcommand{\\QM}{{\\mathbf Q}}\n\\newcommand{\\SigmaM}{{\\boldsymbol{\\Sigma}}}\n\\newcommand{\\IM}{{\\mathbf I}}\n\\newcommand{\\JM}{{\\mathbf J}}\n\\newcommand{\\yM}{{\\mathbf y}}\n\\newcommand{\\yhM}{\\hat{\\mathbf y}}\n\\newcommand{\\xhM}{\\hat{\\mathbf x}}\n\\newcommand{\\zM}{{\\mathbf z}}\n\\newcommand{\\xM}{{\\mathbf x}}\n\\newcommand{\\eM}{{\\mathbf e}}\n\\newcommand{\\zeroM}{{\\mathbf 0}}\n\\newcommand{\\rM}{{\\mathbf r}}\n\\newcommand{\\w}{{\\mathbf w}}\n\\newcommand{\\wtil}{\\widetilde{\\mathbf w}}\n\\newcommand{\\Dw}{\\boldsymbol{\\Delta}{\\mathbf w}}\n\\newcommand{\\Dewp}{\\boldsymbol{\\Delta}{\\mathbf{\\dot{w}}}}\n\\newcommand{\\wo}{{\\mathbf w}_{\\rm o}}\n\\newcommand{\\q}{{\\mathbf q}}\n\\newcommand{\\Nuquad}{\\|{\\mathbf u(n)}\\|^2}\n\\newcommand{\\traco}{{\\rm Tr}}\n\\newcommand{\\snum}{\\mathbf{s}_{n\\!-\\!1}}\n\\]\n\nEmbora a PCA encontre componentes úteis para representar o conjunto de dados, não há razão para supor que esses componentes são úteis para classificar dados de diferentes classes. Se reunirmos todas as amostras, as direções descartadas pela PCA podem ser exatamente as direções necessárias para distinguir entre as classes. Por exemplo, se tivéssemos dados para as letras maiúsculas impressas O e Q, a PCA pode descobrir as características grosseiras que caracterizam O’s e Q’s, mas pode ignorar a cauda que distingue um O de um Q. Enquanto a PCA busca direções que são eficientes para a representação, a análise de discriminante linear busca direções que são eficientes para discriminação.\nA seguir, vamos detalhar essa técnica, começando com o caso particular de duas classes.\n\n\nVamos começar considerando o problema de projetar um vetor de dados com \\(D\\) elementos em uma reta. Mesmo que as amostras formassem agrupamentos compactos e bem separados em no espaço de dimensão \\(D\\), a projeção em uma reta arbitrária geralmente produzirá uma mistura confusa de amostras de todas as classes, o que leva a um desempenho de classificação ruim. No entanto, girando-se a reta, pode-se encontrar uma direção para a qual as amostras projetadas estão bem separadas. Este é exatamente o objetivo da Análise de Discriminante Linear.\nConsidere um problema de classificação com \\(K=2\\) classes e um conjunto de dados formado por \\(N\\) vetores coluna \\(\\mathbf{x}_n\\) de dimensão \\(D\\times 1\\) e elementos reais, ou seja, \\(\\{\\mathbf{x}_n\\in \\mathbb{R}^{D}\\}\\), \\(n=1, 2, \\ldots, N\\). Considere ainda que desses \\(N\\) vetores, há \\(N_1\\) vetores no subconjunto \\({\\cal D}_1\\) rotulados como pertencentes à classe \\(C_1\\) e \\(N_2\\) vetores no subconjunto \\({\\cal D}_2\\) rotulados como pertencentes à classe \\(C_2\\). Uma combinação linear dos componentes do vetor \\(\\mathbf{x}_n\\) leva ao produto escalar\n\\[\ny_n=\\mathbf{w}^{\\rm T}\\mathbf{x}_n.\n\\]\nSupondo que \\(\\|\\mathbf{w}\\|=1\\), cada \\(y_n\\) é a projeção do correspondente \\(\\mathbf{x}_n\\) na direção de \\(\\mathbf{w}\\). A norma de \\(\\mathbf{w}\\) não tem importância porque ela meramente escalona \\(y_n\\), mas a sua a direção é importante. Cabe observar que o conjunto das \\(N\\) amostras \\(y_1, y_2, \\cdots, y_N\\) pode ser dividido em dois subconjuntos \\({\\cal Y}_1\\) e \\({\\cal Y}_2\\), correspondentes às classes \\(C_1\\) e \\(C_2\\), respectivamente.\nSe as amostras rotuladas como \\(C_1\\) estão em um agrupamento e as rotuladas como \\(C_2\\) em outro, deseja-se que as projeções na direção de \\(\\mathbf{w}\\) sejam bem separadas. A Figura~\\(\\ref{fig:Fisher}\\) ilustra o efeito de escolher direções diferentes para \\(\\mathbf{w}\\) em um exemplo bidimensional. Se as distribuições originais forem multimodais e altamente sobrepostas, mesmo o “melhor” \\(\\mathbf{w}\\) pode ser incapaz de fornecer uma separação adequada e, portanto, esse método será de pouca utilidade.\n\n\n\n\n\n\nFigura 1: Projeção do mesmo conjunto de amostras em duas direções diferentes definidas pelo vetor \\(\\mathbf{w}\\). No gráfico da direita, é possível observar que há uma separação maior entre os pontos vermelhos e os pontos pretos. Fonte: (Duda, Hart, e Stork 2000)\n\n\n\nVamos agora encontrar a melhor direção \\(\\mathbf{w}\\) supondo que seja possível obter uma boa classificação. Uma medida da separação entre as pontos projetados é a diferença entre as médias dos dois subconjuntos. Seja \\[\n\\mathbf{m}_i=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{x}\n\\]\no vetor \\(D\\times 1\\) que representa a média dos dados da classe \\(C_i\\). Então, a média dos dados projetados da classe \\(C_i\\) é dada por \\[\n\\widetilde{m}_i=\\frac{1}{N_i}\\sum_{y\\in {\\cal Y}_i}y=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{w}^{\\rm T}\\mathbf{x}=\\mathbf{w}^{\\rm T}\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{x}=\\mathbf{w}^{\\rm T}\\mathbf{m}_i,\n\\]\nque corresponde à projeção do vetor \\(\\mathbf{m}_i\\) na direção de \\(\\mathbf{w}\\).\nA distância entre as médias projetadas é \\[\n|\\widetilde{m}_1-\\widetilde{m}_2|=|\\mathbf{w}^{\\rm T}(\\mathbf{m}_1-\\mathbf{m}_2)|.\n\\]\nEssa diferença depende da norma de \\(\\mathbf{w}\\) e pode ser tão grande quanto se queira. Para se obter uma boa separação dos dados projetados, desejamos que a diferença entre as médias seja grande e que os dados projetados apresentem uma pequena variação em torno da média de cada classe. Definindo\n\\[\n\\widetilde{s}_i^2=\\sum_{y\\in {\\cal Y}_i}(y-\\widetilde{m}_i)^2,\n\\]\nentão \\[\n\\frac{\\widetilde{s}_1^2+\\widetilde{s}_2^2}{N}\n\\]\né uma estimativa da variância dos dados agrupados e a soma \\(\\widetilde{s}_1^2+\\widetilde{s}_2^2\\) é chamada de dispersão total das amostras projetadas dentro da classe (within-class scatter).\nO Discriminante Linear de Fisher busca o vetor \\(\\mathbf{w}\\) (independente de sua norma) que maximiza o seguinte critério\n\\[\nJ(\\mathbf{w})=\\frac{(\\widetilde{m}_1-\\widetilde{m}_2)^2}{\\widetilde{s}_1^2+\\widetilde{s}_2^2}.\n\\]\nPara obter \\(J(\\cdot)\\) como uma função explicita de \\(\\mathbf{w}\\), vamos definir as matrizes de dispersão \\(\\mathbf{S}_i\\) e \\(\\mathbf{S}_W\\), ou seja, \\[\n\\mathbf{S}_i=\\sum_{x\\in{\\cal D}_i}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\n\\]\ne \\[\n\\mathbf{S}_{W}=\\mathbf{S}_1+\\mathbf{S}_2.\n\\]\nEntão, podemos escrever\n\\[\n\\widetilde{s}_i^2=\\sum_{x\\in{\\cal D}_i}(\\mathbf{w}^{\\rm T}\\mathbf{x}-\\mathbf{w}^{\\rm T}\\mathbf{m}_i)^2=\\sum_{x\\in{\\cal D}_i}\\mathbf{w}^{\\rm T}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\\mathbf{w}=\\mathbf{w}^{\\rm T}\\mathbf{S}_i\\mathbf{w}.\n\\]\nConsequentemente, a dispersão total pode ser escrita como \\[\n\\widetilde{s}_1^2+\\widetilde{s}_2^2=\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}.\n\\]\nSimilarmente, \\[\n(\\widetilde{m}_1-\\widetilde{m}_2)^2=(\\mathbf{w}^{\\rm T}\\mathbf{m}_1-\\mathbf{w}^{\\rm T}\\mathbf{m}_2)^2=\n\\mathbf{w}^{\\rm T}(\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^{\\T}\\mathbf{w}=\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}\n\\]\nem que\n\\[\n\\mathbf{S}_B\\triangleq (\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^{\\T}.\n\\]\nA matriz \\(\\mathbf{S}_W\\) é chamada de matriz de dispersão dentro da classe. Ela é proporcional à matriz de covariância dos dados de dimensão \\(D\\) agrupados. Ela é simétrica e positiva semidefinida, e geralmente é não singular se \\(N &gt; D\\). Analogamente, \\(\\mathbf{S}_B\\) é chamada de matriz de dispersão entre classes. No entanto, apesar de ser simétrica, por ser o produto externo de dois vetores, seu posto é no máximo um. Em particular, para qualquer \\(\\mathbf{w}\\), \\(\\mathbf{S}_B\\mathbf{w}\\) está na direção de \\((\\mathbf{m}_1-\\mathbf{m}_2)\\) e \\(\\mathbf{S}_B\\) é singular.\nEm termos dessas matrizes, \\(J(\\mathbf{w})\\) pode ser escrito como\n\\[\nJ(\\mathbf{w})=\\frac{\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}},\n\\]\nque é conhecida na física matemática como coeficiente de Rayleigh generalizado. O vetor \\(\\mathbf{w}\\) que maximiza \\(J(\\mathbf{w})\\) deve satisfazer o critério \\[\n\\max_{\\mathbf{w}\\neq \\mathbf{0}}\\frac{\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}},\n\\]\nque é equivalente ao seguinte critério com restrição\n\\[\n\\max_{\\mathbf{w}\\in \\mathbb{R}^{D}} \\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w},\\;\\;\\text{sujeito a}\\;\\;\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}=1.\n\\]\nPara resolver esse critério, podemos usar a técnica de multiplicadores de Lagrange. Assim, obtemos a seguinte função custo\n\\[\nJ_{\\lambda}(\\mathbf{w})=\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}+\\lambda(1-\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}).\n\\]\nCalculando a derivada em relação \\(\\mathbf{w}\\), obtém-se\n\\[\n\\frac{d J_{\\lambda}(\\mathbf{w})}{d \\mathbf{w}}=\\mathbf{S}_B\\mathbf{w}-\\lambda\\mathbf{S}_W\\mathbf{w}.\n\\]\nIgualando essa derivada a zero e se \\(\\mathbf{S}_{W}\\) for não singular, podemos escrever\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\mathbf{w}=\\lambda \\mathbf{w}\n$}\n\\end{equation*}\\]\nAssim, \\(\\mathbf{w}\\) é o autovetor relacionado ao único autovalor não nulo, \\(\\lambda\\), da matriz \\(\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\). Cabe observar que a matriz \\(\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\) continua com posto igual a um e por isso tem um único autovalor não nulo. No caso particular, como \\(\\mathbf{S}_B\\mathbf{w}\\) está na direção de \\((\\mathbf{m}_1-\\mathbf{m}_2)\\), pode-se calcular \\(\\mathbf{w}\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}=\\mathbf{S}_{W}^{-1}(\\mathbf{m}_1-\\mathbf{m}_2).\n$}\n\\end{equation*}\\]\nAssim, a classificação foi convertida de um problema \\(D\\)-dimensional para um problema de classificação em uma dimensão, ou seja, \\(K-1\\) (quantidade de classes menos um).\nEm suma, o Discriminante Linear de Fisher projeta o vetor \\(\\mathbf{x}\\) de dimensão \\(D\\) em uma dimensão, ou seja, \\(y=\\mathbf{w}^{\\rm T}\\mathbf{x}\\). Comparando \\(y\\) com um limiar, este método pode classificar \\(\\mathbf{x}\\) como pertencente à classe \\(C_1\\) ou à classe \\(C_2\\). Para esse propósito, \\(\\mathbf{w}\\) deve ser calculado tal que as médias dos dados projetados das duas classes fiquem o mais distante possível uma da outra. Além disso, a variância dos dados projetos de cada classe deve ser a menor possível, o que possibilita uma grande concentração dos dados de cada classe em torno de sua média e a possibilidade de uma classificação correta.\n\n\n\nA Análise de Discriminante Linear (LDA - Linear Discriminant Analysis) é a generalização do Discriminante Linear de Fisher para mais de duas classes (\\(K&gt;2\\)). Nessa generalização, o vetor \\(\\mathbf{x}_n\\) de dimensão \\(D&gt;K\\) é projetado no espaço de dimensão \\((K-1)\\) por meio da transformação\n\\[\n\\mathbf{y}_n=\\mathbf{W}^{\\rm T}\\mathbf{x}_n\n\\]\nem que\n\\[\n\\mathbf{W}=[\\mathbf{w}_1\\;\\mathbf{w}_2\\;\\cdots\\;\\mathbf{w}_{K-1}]\n\\]\né uma matriz com dimensões \\(D \\times (K-1)\\), formada por vetores coluna \\(\\mathbf{w}_k\\), \\(k=1,2,\\cdots, K-1\\). Assim, o vetor \\(\\mathbf{y}_n\\) passa a ter a dimensão do número de classes do problema menos um.\nComo no Discriminante Linear de Fisher, a LDA busca maximizar a separação entre as classes e ao mesmo tempo reduzir a sobreposição entre elas. Para isso, deve-se estender as definições das matrizes de dispersão entre classes \\(\\mathbf{S}_B\\) e de dispersão dentro das classes \\(\\mathbf{S}_W\\), vistas anteriormente. Assim, obtém-se para o caso de \\(K\\) classes\n\\[\\begin{align}\n\\mathbf{S}_B&=\\sum_{k=1}^{K}N_k(\\mathbf{m}_k-\\mathbf{m})(\\mathbf{m}_k-\\mathbf{m})^{\\rm T}\\nonumber\\\\\n\\mathbf{S}_W&=\\sum_{k=1}^{K}\\sum_{\\mathbf{x}\\in{\\cal D}_i}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\\nonumber\n\\end{align}\\]\nem que\n\\[\n\\mathbf{m}=\\frac{1}{N}\\sum_{n=1}^{N}{\\mathbf{x}_n}\n\\]\nrepresenta a média global do conjunto de dados. Cabe observar que o termo \\((\\mathbf{m}_k-\\mathbf{m})\\) que aparece em \\(\\mathbf{S}_B\\) é o desvio da média de cada classe da média global. Esses desvios formam um conjunto de \\(K\\) vetores no espaço de características. Entretanto, esses vetores não são independentes. A média global é uma média ponderada das médias das classes, ou seja,\n\\[\n\\mathbf{m}=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\mathbf{m}_k.\n\\]\nEssa relação introduz uma dependência entre os vetores das \\(K\\) classes, o que faz com que a matriz \\(\\mathbf{S}_B\\) tenha posto no máximo igual a \\(K-1\\). Consequentemente, a matriz \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\) também tem posto no máximo igual a \\(K-1\\).\nA função custo neste caso é dada por\n\\[\nJ(\\mathbf{W})=  \\text{Tr}\\{(\\mathbf{W}^{\\rm T} \\mathbf{S}_W\\mathbf{W})^{-1}(\\mathbf{W}^{\\rm T} \\mathbf{S}_B\\mathbf{W})\\},\n\\]\nem que \\(\\text{Tr}\\{\\cdot\\}\\) denota o traço de uma matriz. Essa função se reduz à vista anteriormente para \\(K=2\\). A matriz \\(\\mathbf{W}\\) que maximiza \\(J(\\mathbf{W})\\) é formada pelos autovetores \\(\\mathbf{w}_k\\), \\(k=1, 2, \\cdots, K-1\\) relacionados aos autovalores não nulos de \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\). Assim, o vetor \\(\\mathbf{x}_n\\) pode ser mapeado em um subespaço de dimensão \\((K-1)\\) gerado pelos \\(K-1\\) autovetores correspondentes aos autovalores não nulos. Consequentemente, a otimização de \\(J(\\mathbf{W})\\) produz \\(K-1\\) características sem perda da classificabilidade. Essas características podem ser consideradas como entrada de um classificador como uma rede neural, o que em geral diminui a dimensionalidade do problema."
  },
  {
    "objectID": "t_lda.html#discriminante-linear-de-fisher",
    "href": "t_lda.html#discriminante-linear-de-fisher",
    "title": "Análise de Discriminantes Lineares",
    "section": "",
    "text": "Vamos começar considerando o problema de projetar um vetor de dados com \\(D\\) elementos em uma reta. Mesmo que as amostras formassem agrupamentos compactos e bem separados em no espaço de dimensão \\(D\\), a projeção em uma reta arbitrária geralmente produzirá uma mistura confusa de amostras de todas as classes, o que leva a um desempenho de classificação ruim. No entanto, girando-se a reta, pode-se encontrar uma direção para a qual as amostras projetadas estão bem separadas. Este é exatamente o objetivo da Análise de Discriminante Linear.\nConsidere um problema de classificação com \\(K=2\\) classes e um conjunto de dados formado por \\(N\\) vetores coluna \\(\\mathbf{x}_n\\) de dimensão \\(D\\times 1\\) e elementos reais, ou seja, \\(\\{\\mathbf{x}_n\\in \\mathbb{R}^{D}\\}\\), \\(n=1, 2, \\ldots, N\\). Considere ainda que desses \\(N\\) vetores, há \\(N_1\\) vetores no subconjunto \\({\\cal D}_1\\) rotulados como pertencentes à classe \\(C_1\\) e \\(N_2\\) vetores no subconjunto \\({\\cal D}_2\\) rotulados como pertencentes à classe \\(C_2\\). Uma combinação linear dos componentes do vetor \\(\\mathbf{x}_n\\) leva ao produto escalar\n\\[\ny_n=\\mathbf{w}^{\\rm T}\\mathbf{x}_n.\n\\]\nSupondo que \\(\\|\\mathbf{w}\\|=1\\), cada \\(y_n\\) é a projeção do correspondente \\(\\mathbf{x}_n\\) na direção de \\(\\mathbf{w}\\). A norma de \\(\\mathbf{w}\\) não tem importância porque ela meramente escalona \\(y_n\\), mas a sua a direção é importante. Cabe observar que o conjunto das \\(N\\) amostras \\(y_1, y_2, \\cdots, y_N\\) pode ser dividido em dois subconjuntos \\({\\cal Y}_1\\) e \\({\\cal Y}_2\\), correspondentes às classes \\(C_1\\) e \\(C_2\\), respectivamente.\nSe as amostras rotuladas como \\(C_1\\) estão em um agrupamento e as rotuladas como \\(C_2\\) em outro, deseja-se que as projeções na direção de \\(\\mathbf{w}\\) sejam bem separadas. A Figura~\\(\\ref{fig:Fisher}\\) ilustra o efeito de escolher direções diferentes para \\(\\mathbf{w}\\) em um exemplo bidimensional. Se as distribuições originais forem multimodais e altamente sobrepostas, mesmo o “melhor” \\(\\mathbf{w}\\) pode ser incapaz de fornecer uma separação adequada e, portanto, esse método será de pouca utilidade.\n\n\n\n\n\n\nFigura 1: Projeção do mesmo conjunto de amostras em duas direções diferentes definidas pelo vetor \\(\\mathbf{w}\\). No gráfico da direita, é possível observar que há uma separação maior entre os pontos vermelhos e os pontos pretos. Fonte: (Duda, Hart, e Stork 2000)\n\n\n\nVamos agora encontrar a melhor direção \\(\\mathbf{w}\\) supondo que seja possível obter uma boa classificação. Uma medida da separação entre as pontos projetados é a diferença entre as médias dos dois subconjuntos. Seja \\[\n\\mathbf{m}_i=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{x}\n\\]\no vetor \\(D\\times 1\\) que representa a média dos dados da classe \\(C_i\\). Então, a média dos dados projetados da classe \\(C_i\\) é dada por \\[\n\\widetilde{m}_i=\\frac{1}{N_i}\\sum_{y\\in {\\cal Y}_i}y=\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{w}^{\\rm T}\\mathbf{x}=\\mathbf{w}^{\\rm T}\\frac{1}{N_i}\\sum_{\\mathbf{x}\\in {\\cal D}_i}\\mathbf{x}=\\mathbf{w}^{\\rm T}\\mathbf{m}_i,\n\\]\nque corresponde à projeção do vetor \\(\\mathbf{m}_i\\) na direção de \\(\\mathbf{w}\\).\nA distância entre as médias projetadas é \\[\n|\\widetilde{m}_1-\\widetilde{m}_2|=|\\mathbf{w}^{\\rm T}(\\mathbf{m}_1-\\mathbf{m}_2)|.\n\\]\nEssa diferença depende da norma de \\(\\mathbf{w}\\) e pode ser tão grande quanto se queira. Para se obter uma boa separação dos dados projetados, desejamos que a diferença entre as médias seja grande e que os dados projetados apresentem uma pequena variação em torno da média de cada classe. Definindo\n\\[\n\\widetilde{s}_i^2=\\sum_{y\\in {\\cal Y}_i}(y-\\widetilde{m}_i)^2,\n\\]\nentão \\[\n\\frac{\\widetilde{s}_1^2+\\widetilde{s}_2^2}{N}\n\\]\né uma estimativa da variância dos dados agrupados e a soma \\(\\widetilde{s}_1^2+\\widetilde{s}_2^2\\) é chamada de dispersão total das amostras projetadas dentro da classe (within-class scatter).\nO Discriminante Linear de Fisher busca o vetor \\(\\mathbf{w}\\) (independente de sua norma) que maximiza o seguinte critério\n\\[\nJ(\\mathbf{w})=\\frac{(\\widetilde{m}_1-\\widetilde{m}_2)^2}{\\widetilde{s}_1^2+\\widetilde{s}_2^2}.\n\\]\nPara obter \\(J(\\cdot)\\) como uma função explicita de \\(\\mathbf{w}\\), vamos definir as matrizes de dispersão \\(\\mathbf{S}_i\\) e \\(\\mathbf{S}_W\\), ou seja, \\[\n\\mathbf{S}_i=\\sum_{x\\in{\\cal D}_i}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\n\\]\ne \\[\n\\mathbf{S}_{W}=\\mathbf{S}_1+\\mathbf{S}_2.\n\\]\nEntão, podemos escrever\n\\[\n\\widetilde{s}_i^2=\\sum_{x\\in{\\cal D}_i}(\\mathbf{w}^{\\rm T}\\mathbf{x}-\\mathbf{w}^{\\rm T}\\mathbf{m}_i)^2=\\sum_{x\\in{\\cal D}_i}\\mathbf{w}^{\\rm T}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\\mathbf{w}=\\mathbf{w}^{\\rm T}\\mathbf{S}_i\\mathbf{w}.\n\\]\nConsequentemente, a dispersão total pode ser escrita como \\[\n\\widetilde{s}_1^2+\\widetilde{s}_2^2=\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}.\n\\]\nSimilarmente, \\[\n(\\widetilde{m}_1-\\widetilde{m}_2)^2=(\\mathbf{w}^{\\rm T}\\mathbf{m}_1-\\mathbf{w}^{\\rm T}\\mathbf{m}_2)^2=\n\\mathbf{w}^{\\rm T}(\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^{\\T}\\mathbf{w}=\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}\n\\]\nem que\n\\[\n\\mathbf{S}_B\\triangleq (\\mathbf{m}_1-\\mathbf{m}_2)(\\mathbf{m}_1-\\mathbf{m}_2)^{\\T}.\n\\]\nA matriz \\(\\mathbf{S}_W\\) é chamada de matriz de dispersão dentro da classe. Ela é proporcional à matriz de covariância dos dados de dimensão \\(D\\) agrupados. Ela é simétrica e positiva semidefinida, e geralmente é não singular se \\(N &gt; D\\). Analogamente, \\(\\mathbf{S}_B\\) é chamada de matriz de dispersão entre classes. No entanto, apesar de ser simétrica, por ser o produto externo de dois vetores, seu posto é no máximo um. Em particular, para qualquer \\(\\mathbf{w}\\), \\(\\mathbf{S}_B\\mathbf{w}\\) está na direção de \\((\\mathbf{m}_1-\\mathbf{m}_2)\\) e \\(\\mathbf{S}_B\\) é singular.\nEm termos dessas matrizes, \\(J(\\mathbf{w})\\) pode ser escrito como\n\\[\nJ(\\mathbf{w})=\\frac{\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}},\n\\]\nque é conhecida na física matemática como coeficiente de Rayleigh generalizado. O vetor \\(\\mathbf{w}\\) que maximiza \\(J(\\mathbf{w})\\) deve satisfazer o critério \\[\n\\max_{\\mathbf{w}\\neq \\mathbf{0}}\\frac{\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}},\n\\]\nque é equivalente ao seguinte critério com restrição\n\\[\n\\max_{\\mathbf{w}\\in \\mathbb{R}^{D}} \\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w},\\;\\;\\text{sujeito a}\\;\\;\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}=1.\n\\]\nPara resolver esse critério, podemos usar a técnica de multiplicadores de Lagrange. Assim, obtemos a seguinte função custo\n\\[\nJ_{\\lambda}(\\mathbf{w})=\\mathbf{w}^{\\rm T}\\mathbf{S}_B\\mathbf{w}+\\lambda(1-\\mathbf{w}^{\\rm T}\\mathbf{S}_W\\mathbf{w}).\n\\]\nCalculando a derivada em relação \\(\\mathbf{w}\\), obtém-se\n\\[\n\\frac{d J_{\\lambda}(\\mathbf{w})}{d \\mathbf{w}}=\\mathbf{S}_B\\mathbf{w}-\\lambda\\mathbf{S}_W\\mathbf{w}.\n\\]\nIgualando essa derivada a zero e se \\(\\mathbf{S}_{W}\\) for não singular, podemos escrever\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\mathbf{w}=\\lambda \\mathbf{w}\n$}\n\\end{equation*}\\]\nAssim, \\(\\mathbf{w}\\) é o autovetor relacionado ao único autovalor não nulo, \\(\\lambda\\), da matriz \\(\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\). Cabe observar que a matriz \\(\\mathbf{S}_{W}^{-1}\\mathbf{S}_{B}\\) continua com posto igual a um e por isso tem um único autovalor não nulo. No caso particular, como \\(\\mathbf{S}_B\\mathbf{w}\\) está na direção de \\((\\mathbf{m}_1-\\mathbf{m}_2)\\), pode-se calcular \\(\\mathbf{w}\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}=\\mathbf{S}_{W}^{-1}(\\mathbf{m}_1-\\mathbf{m}_2).\n$}\n\\end{equation*}\\]\nAssim, a classificação foi convertida de um problema \\(D\\)-dimensional para um problema de classificação em uma dimensão, ou seja, \\(K-1\\) (quantidade de classes menos um).\nEm suma, o Discriminante Linear de Fisher projeta o vetor \\(\\mathbf{x}\\) de dimensão \\(D\\) em uma dimensão, ou seja, \\(y=\\mathbf{w}^{\\rm T}\\mathbf{x}\\). Comparando \\(y\\) com um limiar, este método pode classificar \\(\\mathbf{x}\\) como pertencente à classe \\(C_1\\) ou à classe \\(C_2\\). Para esse propósito, \\(\\mathbf{w}\\) deve ser calculado tal que as médias dos dados projetados das duas classes fiquem o mais distante possível uma da outra. Além disso, a variância dos dados projetos de cada classe deve ser a menor possível, o que possibilita uma grande concentração dos dados de cada classe em torno de sua média e a possibilidade de uma classificação correta."
  },
  {
    "objectID": "t_lda.html#extensão-para-mais-de-duas-classes",
    "href": "t_lda.html#extensão-para-mais-de-duas-classes",
    "title": "Análise de Discriminantes Lineares",
    "section": "",
    "text": "A Análise de Discriminante Linear (LDA - Linear Discriminant Analysis) é a generalização do Discriminante Linear de Fisher para mais de duas classes (\\(K&gt;2\\)). Nessa generalização, o vetor \\(\\mathbf{x}_n\\) de dimensão \\(D&gt;K\\) é projetado no espaço de dimensão \\((K-1)\\) por meio da transformação\n\\[\n\\mathbf{y}_n=\\mathbf{W}^{\\rm T}\\mathbf{x}_n\n\\]\nem que\n\\[\n\\mathbf{W}=[\\mathbf{w}_1\\;\\mathbf{w}_2\\;\\cdots\\;\\mathbf{w}_{K-1}]\n\\]\né uma matriz com dimensões \\(D \\times (K-1)\\), formada por vetores coluna \\(\\mathbf{w}_k\\), \\(k=1,2,\\cdots, K-1\\). Assim, o vetor \\(\\mathbf{y}_n\\) passa a ter a dimensão do número de classes do problema menos um.\nComo no Discriminante Linear de Fisher, a LDA busca maximizar a separação entre as classes e ao mesmo tempo reduzir a sobreposição entre elas. Para isso, deve-se estender as definições das matrizes de dispersão entre classes \\(\\mathbf{S}_B\\) e de dispersão dentro das classes \\(\\mathbf{S}_W\\), vistas anteriormente. Assim, obtém-se para o caso de \\(K\\) classes\n\\[\\begin{align}\n\\mathbf{S}_B&=\\sum_{k=1}^{K}N_k(\\mathbf{m}_k-\\mathbf{m})(\\mathbf{m}_k-\\mathbf{m})^{\\rm T}\\nonumber\\\\\n\\mathbf{S}_W&=\\sum_{k=1}^{K}\\sum_{\\mathbf{x}\\in{\\cal D}_i}(\\mathbf{x}-\\mathbf{m}_i)(\\mathbf{x}-\\mathbf{m}_i)^{\\rm T}\\nonumber\n\\end{align}\\]\nem que\n\\[\n\\mathbf{m}=\\frac{1}{N}\\sum_{n=1}^{N}{\\mathbf{x}_n}\n\\]\nrepresenta a média global do conjunto de dados. Cabe observar que o termo \\((\\mathbf{m}_k-\\mathbf{m})\\) que aparece em \\(\\mathbf{S}_B\\) é o desvio da média de cada classe da média global. Esses desvios formam um conjunto de \\(K\\) vetores no espaço de características. Entretanto, esses vetores não são independentes. A média global é uma média ponderada das médias das classes, ou seja,\n\\[\n\\mathbf{m}=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\mathbf{m}_k.\n\\]\nEssa relação introduz uma dependência entre os vetores das \\(K\\) classes, o que faz com que a matriz \\(\\mathbf{S}_B\\) tenha posto no máximo igual a \\(K-1\\). Consequentemente, a matriz \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\) também tem posto no máximo igual a \\(K-1\\).\nA função custo neste caso é dada por\n\\[\nJ(\\mathbf{W})=  \\text{Tr}\\{(\\mathbf{W}^{\\rm T} \\mathbf{S}_W\\mathbf{W})^{-1}(\\mathbf{W}^{\\rm T} \\mathbf{S}_B\\mathbf{W})\\},\n\\]\nem que \\(\\text{Tr}\\{\\cdot\\}\\) denota o traço de uma matriz. Essa função se reduz à vista anteriormente para \\(K=2\\). A matriz \\(\\mathbf{W}\\) que maximiza \\(J(\\mathbf{W})\\) é formada pelos autovetores \\(\\mathbf{w}_k\\), \\(k=1, 2, \\cdots, K-1\\) relacionados aos autovalores não nulos de \\(\\mathbf{S}_W^{-1}\\mathbf{S}_B\\). Assim, o vetor \\(\\mathbf{x}_n\\) pode ser mapeado em um subespaço de dimensão \\((K-1)\\) gerado pelos \\(K-1\\) autovetores correspondentes aos autovalores não nulos. Consequentemente, a otimização de \\(J(\\mathbf{W})\\) produz \\(K-1\\) características sem perda da classificabilidade. Essas características podem ser consideradas como entrada de um classificador como uma rede neural, o que em geral diminui a dimensionalidade do problema."
  },
  {
    "objectID": "t_lms.html",
    "href": "t_lms.html",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Na regressão linear multivariada, conhecendo-se o conjunto de dados de treinamento\n\\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1} ,d_1), (x_{12}, x_{22}, \\cdots, x_{M2} ,d_2),\\cdots, (x_{1N_t}, x_{2N_t}, \\cdots, x_{MN_t} ,d_{N_t})\\},\n\\]\nobtém-se um modelo de hiperplano do tipo\n\\[\ny=b+w_1x_1+w_2x_2+\\cdots+w_Mx_M\\approx d,\n\\]\nem que \\(N_t\\) é o número de dados utilizados no treinamento, \\(b\\) o viés (bias), \\(d\\) o sinal desejado, \\(y\\) a estimativa de \\(d\\), \\(x_k\\) o sinal de entrada e \\(w_k\\), \\(k=1,\\cdots, M\\) os pesos do regressor.\nPara obter o modelo, utilizamos os dados de treinamento e calculamos a solução\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1}\\mathbf{X}^{\\rm T}\\mathbf{d}\n$}\n\\end{equation*}\\]\nem que\n\\[\n\\mathbf{w}^{\\rm o}=\\left[\n  \\begin{array}{c}\n    b^{\\rm o} \\\\\n    w_1^{\\rm o} \\\\\n    \\vdots \\\\\n    w_M^{\\rm o} \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{X}=\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N_t} & x_{2N_t} & \\cdots & x_{MN_t} \\\\\n  \\end{array}\n\\right]\\;\\;\\;\\;\\text{e}\\;\\;\\;\\;\n\\mathbf{d}\n=\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n    d_{N_t} \\\\\n  \\end{array}\n\\right].\n\\]\nA solução \\(\\mathbf{w}^{\\rm o}\\) minimiza a norma ao quadrado do vetor de erros, que aqui vamos denotar por \\(J(\\mathbf{w})\\), ou seja,\n\\[\nJ(\\mathbf{w})=\\|\\mathbf{e}\\|^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2,\n\\]\nde modo que \\(\\mathbf{w}^{\\rm o}={\\rm argmin}_{\\mathbf{w}} J(\\mathbf{w})\\).\nO vetor de pesos \\(\\mathbf{w}^{\\rm o}\\) pode ser obtido a partir de um treinamento iterativo, em que cada amostra \\((x_{1n}, x_{2n}, \\cdots, x_{Mn} ,d_n),\\) \\(n=1,2,\\cdots, N_t\\) é apresentada a um algoritmo por vez. Para obter esse algoritmo, vamos primeiramente denotar o vetor de pesos da iteração \\(n\\) por\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T}.\n\\]\nFazendo o mesmo com os dados de treinamento, teremos na iteração \\(n\\) o sinal desejado \\(d(n)=d_n\\), \\(n=1,2,\\cdots,N_t\\) e o vetor de entrada\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}.\n\\]\nO sinal de “saída” desse regressor iterativo é então calculado como\n\\[\ny(n)=\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)=b(n-1)+\\sum_{k=1}^{M}x_{kn}w_k(n-1),\n\\]\nem que \\(\\mathbf{w}(0)=\\mathbf{0}\\). É importante observar que como se trata de um algoritmo iterativo, precisamos inicializar o vetor de pesos. Uma possibilidade é considerar o vetor nulo, embora também seja possível inicializar os pesos de forma aleatória.\nNa regressão linear multivariada, o melhor hiperplano é obtido ao se minimizar o quadrado da norma do vetor de erros, ou seja, deve-se minimizar \\(J(\\mathbf{w})=\\|\\mathbf{e}\\|^2\\), que é comumente chamada de função custo. Aqui, devemos fazer algo semelhante. No entanto, não dispomos de um vetor de erros, pois estamos buscando a solução de forma iterativa, mas podemos calcular o erro de “estimação” em cada iteração, ou seja,\n\\[\ne(n)=d(n)-y(n)=d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)=d(n)- b(n-1)-\\sum_{k=1}^{M}x_{kn}w_k(n-1).\n\\]\nAssim, podemos ajustar os pesos para minimizar o erro quadrático médio (do inglês, mean-square error - MSE), definido como\n\\[\nJ_{\\rm MSE}(\\mathbf{w})={\\rm E}\\{e^2(n)\\},\n\\]\nem que \\({\\rm E}\\{\\cdot\\}\\) representa o operador esperança matemática. Para minimizar essa função, como no caso da regressão linear multivariada, devemos primeiramente derivá-la em relação ao vetor de pesos, o que leva ao vetor gradiente\n\\[\n\\begin{align*}\n\\boldsymbol{\\nabla}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(n-1))&=\\frac{\\partial {\\rm E}\\{e^2(n)\\}}{\\partial \\mathbf{w}(n-1)}=2{\\rm E}\\left\\{e(n)\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}\\right\\}=\n2{\\rm E}\\left\\{e(n)\\left[\\begin{array}{c}\n        \\frac{de(n)}{db(n-1)} \\\\\n        \\\\\n        \\frac{de(n)}{dw_1(n-1)} \\\\\n                \\vdots \\\\\n        \\frac{de(n)}{dw_M(n-1)}\n      \\end{array}\n\\right]\\right\\}\\nonumber\\\\\n&=2{\\rm E}\\left\\{e(n)\\left[\\begin{array}{c}\n        -1 \\\\\n         -x_{1n} \\\\\n                \\vdots \\\\\n        -x_{Mn}\n      \\end{array}\n\\right]\\right\\}\n=-2{\\rm E}\\{e(n)\\mathbf{x}(n)\\}.\n\\end{align*}\n\\]\nIgualando o vetor gradiente ao vetor nulo, obtemos\n\\[\n{\\rm E}\\{e(n)\\mathbf{x}(n)\\}={\\rm E}\\{\\mathbf{x}(n)[d(n)-y(n)]\\}=\\boldsymbol{0},\n\\]\nou ainda\n\\[\n{\\rm E}\\{\\mathbf{x}(n)[d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)]\\}=\\boldsymbol{0}\\Rightarrow\n\\underbrace{{\\rm E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\rm T}(n)\\}}_{\\mathbf{R}}\\mathbf{w}^{\\rm wiener}=\\underbrace{{\\rm E}\\{d(n)\\mathbf{x}(n)\\}}_{\\mathbf{p}}.\n\\]\nA solução dessa equação leva ao MSE mínimo e é conhecida na literatura como solução de Wiener-Hopf, ou simplesmente, solução de Wiener. Por isso, vamos denotá-la como \\(\\mathbf{w}^{\\rm wiener}\\), ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm wiener}=\\mathbf{R}^{-1}\\mathbf{p},\n$}\n\\end{equation*}\\]\nem que \\(\\mathbf{R}\\) é a matriz de autocorrelação dos dados de entrada e \\(\\mathbf{p}\\) o vetor de correlação cruzada entre o sinal desejado \\(d(n)\\) e os dados de entrada. Tanto a matriz \\(\\mathbf{R}\\) como o vetor \\(\\mathbf{p}\\) têm em suas definições o operador esperança matemática. Podemos estimar \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\) utilizando todos os dados de treinamento, o que leva respectivamente a\n\\[\n\\widehat{\\mathbf{R}}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}\\mathbf{x}(n)\\mathbf{x}^{\\rm T}(n)\\;\\;\\;\\text{e}\\;\\;\\;\\widehat{\\mathbf{p}}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}d(n)\\mathbf{x}(n).\n\\]\nNeste caso, a solução obtida com a regressão linear multivariada coincide com a solução de Wiener, ou seja, \\(\\mathbf{w}^{\\rm o}=\\mathbf{w}^{\\rm wiener}\\). Além disso, essa solução é única para um dado conjunto de treinamento.\nAo acompanhar esse cálculo, você pode estar se perguntando: onde está o algoritmo iterativo para o cálculo dos pesos? Ele pode ser obtido utilizando o método do gradiente. Em Cálculo, aprendemos que o gradiente de uma função aponta para a direção de maior variação da mesma. Como a solução é única, basta considerar o sentido contrário do gradiente, o que leva a\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)-\\frac{\\eta}{2}\\boldsymbol{\\nabla}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(n-1)),\n\\]\nem que \\(\\eta\\) é um passo de adaptação. Substituindo a expressão do gradiente, chega-se a\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta{\\rm E}\\{e(n)\\mathbf{x}(n)\\},\n\\]\nou ainda\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta{\\rm E}\\{\\mathbf{x}(n)[d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)]\\}.\n\\]\nIdentificando a matriz \\(\\mathbf{R}\\) e o vetor \\(\\mathbf{p}\\) na equação acima, obtemos\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\left[\\mathbf{p}-\\mathbf{R}\\mathbf{w}(n-1)\\right].\n$}\n\\end{equation*}\\]\nEsse algoritmo iterativo é conhecido na literatura como steepest descent algorithm ou algoritmo do gradiente exato. O passo de adaptação \\(\\eta\\) tem um papel fundamental em sua convergência. É possível demonstrar que se o intervalo \\(0&lt;\\eta&lt;2/{\\lambda_{\\max}}\\) for atendido, em que \\(\\lambda_{\\max}\\) é o autovalor máximo da matriz \\(\\mathbf{R}\\), essa equação converge exatamente para a solução de Wiener (Nascimento e Silva 2014),(Haykin 2014),(Sayed 2008). Apesar de chegar exatamente à solução que minimiza o MSE, ele não é adequado porque é necessário conhecer \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\). A única vantagem é evitar calcular a inversa da matriz \\(\\mathbf{R}\\), o que representa uma economia em custo computacional. Apesar de ser pouco utilizado na prática, esse algoritmo é fundamental para entendermos o algoritmo LMS a seguir.\n\n\n\nUma maneira de simplificar os cálculos para evitar ter de conhecer \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\), é estimar essas grandezas instantâneamente, o que leva respectivamente a\n\\[\n\\widehat{\\mathbf{R}}(n)=\\mathbf{x}(n)\\mathbf{x}^{{\\rm T}}(n)\\;\\;\\;\\text{e}\\;\\;\\;\\widehat{\\mathbf{p}}(n)=d(n)\\mathbf{x}(n).\n\\]\nSubstituindo essas aproximações no algoritmo steepest descent, chega-se a\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n),\n$}\n\\end{equation*}\\]\nque é a equação de atualização do conhecido algoritmo LMS (least-mean-square), cujo pseudocódigo está mostrado no Algoritmo 1. O fluxo de sinal do LMS é mostrado na Figura 1. Novamente, o passo de adaptação \\(\\eta\\) tem um papel fundamental na convergência desse algoritmo. Quanto menor o valor de \\(\\eta\\), mais próximo da solução de Wiener o algoritmo LMS estará quando atingir o regime estacionário. No entanto, quanto menor o passo, mais lentamente o algoritmo atingirá o regime. Em contrapartida, passos grandes podem representar convergências rápidas, mas também podem levar o algoritmo à divergência. Neste caso, os pesos podem ir para infinito1. Diante disso, deve-se atentar ao compromisso entre precisão da solução e velocidade de convergência. O problema é que o intervalo \\(0&lt;\\eta&lt;2/{\\lambda_{\\max}}\\) que vale para o algoritmo exato, é em geral maior do que o intervalo de passo admitido no algoritmo aproximado. Pode-se demonstrar que \\(0&lt;\\eta&lt;2/(3{\\lambda_{\\max}})\\) é um intervalo mais razoável para o algoritmo LMS, mas ainda não garante sua convergência. Devido à sua simplicidade, ele é muito usado em diversas aplicações de filtragem adaptativa que exigem solução em tempo real. As principais aplicações incluem cancelamento de eco acústico, equalização de canais de comunicação, controle ativo de ruído e identificação de sistemas (Nascimento e Silva 2014),(Haykin 2014),(Sayed 2008).\nPara finalizar esta seção, é importante observar que em várias aplicações de filtragem adaptativa não se considera o bias. Além disso, o vetor de entrada \\(\\mathbf{x}(n)\\) muitas vezes é extraído de uma sequência de números, considerando uma linha de atrasos. Neste caso, ele é chamado de vetor regressor. Para exemplificar, vamos supor que temos a seguinte sequência de números\n\\[\n\\begin{array}{c}\n\\vdots\\\\\nx(n+1)=9\\\\\n    \\;\\;\\;\\;\\;\\;x(n)=1 \\\\\n    x(n-1)=2 \\\\\n    x(n-2)=3 \\\\\n    x(n-3)=4 \\\\\n\\!\\!\\!\\!\\!\\vdots\n  \\end{array},\n\\]\nem que \\(n\\) representa um instante de tempo ou uma posição. Considerando \\(M=3\\) e levando em conta a linha de atrasos sem o bias, os vetores de entrada do LMS nos instantes \\(n-1\\), \\(n\\) e \\(n+1\\) são dados respectivamente por\n\\[\n\\mathbf{x}(n-1)=[\\,2\\;\\; 3\\;\\;4\\,]^{{\\rm T}},\\;\\;\\;\\mathbf{x}(n)=[\\,1\\;\\; 2\\;\\;3\\,]^{{\\rm T}}\\;\\;\\;\\text{e}\\;\\;\\;\\mathbf{x}(n+1)=[\\,9\\;\\; 1\\;\\;2\\,]^{{\\rm T}}.\n\\]\nGeneralizando, tem-se\n\\[\n\\mathbf{x}(n)=[\\,x(n)\\;\\;x(n-1)\\;\\;\\cdots\\;\\;x(n-M+1)\\,]^{{\\rm T}}.\n\\]\nPara quem já estudou Processamento de Sinais, o LMS com esse vetor de entrada é um filtro com resposta ao impulso de duração finita (FIR - finite impulse response), cujos coeficientes variam ao longo do tempo.\n\n\nExemplo 1 Sumário do algoritmo LMS.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(n=1,2,\\ldots,\\) calcule:      \\({y}(n)=\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(n-1)\\)      \\(e(n)=d(n)-y(n)\\)      \\(\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n)\\)    Fim\n\n\n\n\n\n\n\n\nFigura 1: Fluxo de sinal do algoritmo LMS.\n\n\n\n\n\n\nConsidere o exemplo de classificação das “meias-luas” da Figura 2. A meia-lua chamada de “Região A” está posicionada simetricamente em relação ao eixo \\(x_2\\), enquanto a meia-lua chamada de “Região B” está deslocada de \\(r_1\\) à direita do eixo \\(x_2\\) e de \\(r_2\\) abaixo do eixo \\(x_1\\). As duas meias-luas têm raio \\(r_1\\) e largura \\(r_3\\) idênticos. A distância vertical \\(r_2\\) que separa as duas meias-luas é ajustável e medida em relação ao eixo \\(x_1\\). Para \\(r_2&gt;0\\), quanto maior o valor de \\(r_2\\), maior a separação entre as meias-luas. Já para \\(r_2&lt;0\\), quando mais negativo for \\(r_2\\), mais próximas ficam as meias-luas (Haykin 2009).\n\n\n\n\n\n\nFigura 2: O problema de classificação das meias-luas (Haykin 2009).\n\n\n\nO conjunto de treinamento consiste em 5000 pontos, 2500 pertencentes à Região A e 2500 à Região B. Esses pontos são sorteados aleatoriamente. Assim, os pontos da Região A são sorteados considerando\n\\[\n(\\rho\\cos\\theta,\\;\\rho\\,{\\rm sen\\,}\\theta),\n\\]\nem que \\(\\theta\\) é uma variável aleatória uniformemente distribuída no intervalo \\([0,\\;\\pi]\\) e \\(\\rho\\) é outra variável aleatória uniformemente distribuída no intervalo \\([r_1-r_3/2,\\;\\;r_1+r_3/2]\\). Para essa região, considera-se que o sinal desejado é igual a um (\\(d=1\\)). Para gerar os pontos da Região B, basta considerar os deslocamentos, ou seja,\n\\[\n(\\rho\\cos\\theta+r_1,\\;\\;-\\rho\\,{\\rm sen}\\theta-r_2)\n\\]\ne \\(d=-1\\) como sinal desejado. O conjunto de teste consiste em 2000 pontos, 1000 pontos de cada região, gerados de forma independente do conjunto de treinamento.\nPara \\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\), considerou-se o algoritmo LMS com passo \\(\\eta=10^{-3}\\) e \\(M=2\\). Os dados de treinamento estão mostrados na Figura 3. Na Figura Figura 4, são mostrados a saída do algoritmo, o erro quadrático em dB e os pesos e bias ao longo das iterações. São mostrados também os pesos e bias da solução de Wiener (retas tracejadas em vermelho). Como esperado, os pesos e bias do algoritmo LMS se aproximam dos valores obtidos com a solução de Wiener, mas não convergem exatamente para eles. É possível observar que a velocidade de convergência dos pesos e bias do LMS são diferentes. O bias do LMS só atinge um valor próximo de\n\\(b=-0,086\\) (bias da solução de Wiener) perto de \\(n=5000\\), enquanto os pesos chegam próximos dos de Wiener perto de \\(n=200\\). Em geral, não se usa o bias em aplicações de filtragem adaptativa. Considera-se esse parâmetro aqui apenas para obter um modelo de neurônio linear. A saída do LMS no treinamento não mostra uma separação clara entre os dois valores possíveis para o sinal desejado (\\(\\pm 1\\)). Apesar disso, é possível verificar na Figura 5 com os dados de teste que há apenas uma pequena quantidade de dados da Região B que foram classificados erroneamente como pertencentes à Região A, o que leva a uma taxa de erro de aproximadamente 0,6%. Considerando os pesos da última iteração do LMS (\\(n=5000\\)), obtém-se a solução “linear” dada pela separação das regiões, mostrada na Figura 5.\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNA=2500 # número de pontos de treinamento da Região A\nNB=2500 # número de pontos de treinamento da Região B\nNt=NA+NB # total de dados de treinamento\n\n# dados das meia luas \nr1=10\nr3=6\nr2=1\nrmin=r1-r3/2\nrmax=r1+r3/2\n\n# Pontos da Região A\na=np.pi*np.random.rand(NA, 1)\nrxy=np.random.uniform(rmin,rmax,(NA,1))\nxA=rxy*np.cos(a)\nyA=rxy*np.sin(a)\ndA=np.ones((NA, 1))\npontosA=np.hstack((xA, yA, dA))\n\n# Pontos da Região B\na=np.pi*np.random.rand(NB, 1)\nrxy=np.random.uniform(rmin,rmax,(NB, 1))\nxB=rxy*np.cos(a)+r1\nyB=-rxy*np.sin(a)-r2\ndB=-np.ones((NB, 1))\npontosB=np.hstack((xB, yB, dB))\n\n#Concatenando e embaralhando os dados de treinamento\ndados_treino=np.vstack((pontosA, pontosB))\nnp.random.shuffle(dados_treino)\n\n# Figura para mostrar os dados de treino\nfig, ax1 = plt.subplots()\nax1.plot(xA,yA,'.b')\nax1.plot(xB,yB,'.r')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.grid(axis='x', color='0.5')\nplt.grid(axis='y', color='0.5')\n\n\n\n\n\n\n\n\n\n\nFigura 3: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Dados de treinamento (\\(N_t=1000\\)).\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 4: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Algoritmo LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)): saída do algoritmo, erro quadrático em dB e pesos e bias ao longo das iterações. As retas vermelhas tracejadas representam os valores dos pesos e bias da solução de Wiener.\n\n\n\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Dados de teste (\\(N_{\\rm teste}=2000\\)) e separação das duas regiões obtida com o LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)). Os dados da Região B classificados erroneamente são os pontos vermelhos sob o fundo azul (próximos de \\(x_2=1\\)), que neste caso leva a uma taxa de erro de 0,6%.\n\n\n\nAo diminuir o passo do algoritmo LMS para \\(\\eta=5\\times 10^{-5}\\), é possível observar na Figura 6 que o algoritmo LMS tem uma convergência mais lenta: os pesos levam cerca de \\(n=10^3\\) iterações para convergir enquanto o bias leva cerca de \\(n=10^5\\) iterações. Neste caso, foram considerados \\(N_t=10^5\\) dados de treinamento para que o algoritmo atingisse o regime permanente. Em contrapartida, a solução do algoritmo se torna mais próxima da de Wiener, como esperado. Assim, o projetista deve sempre ter em mente o compromisso entre passo de adaptação e precisão da solução. Apesar de mais precisa, a solução atingida ainda leva a erros na classificação como ocorre na Figura 5.\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Algoritmo LMS (\\(M=2\\), \\(\\eta=10^{-5}\\)): pesos ao longo das iterações. As retas vermelhas tracejadas representam os valores dos pesos da solução de Wiener.\n\n\n\nConsiderando agora \\(r_2=-4\\), as meias-luas se tornam mais próximas, o que faz com que o algoritmo LMS chegue a uma solução que leva a mais erros: pontos da Região A são classificados erroneamente como pertencentes à Região B e vice-versa, como é possível observar na Figura 7. Neste caso, a taxa de erro aumenta para aproximadamente 11,5%. Para se obter uma solução sem erros para \\(r_2=-4\\), é necessário considerar um classificador não linear.\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Dados de teste (\\(N_{\\rm teste}=2000\\)) e separação das duas regiões obtida com o LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)). Os dados classificados erroneamente são os pontos azuis sob o fundo vermelho e os pontos vermelhos sob o fundo azul, que neste caso leva a uma taxa de erro de 11,5%.\n\n\n\n\n\n\nEm diversas aplicações, o banco de dados é limitado. Esse é o caso, por exemplo, do problema de classificação de arritmias cardíacas utilizando sinais de eletrocardiograma (ECG). A aquisição de novos sinais deve seguir o padrão do banco de dados existente: os sinais precisam ser amostrados com a mesma frequência, os sensores devem ser os mesmos, o exame deve seguir o mesmo protocolo etc. Vamos supor que seja possível garantir o mesmo padrão de aquisição dos sinais. Depois de serem adquiridos, os novos sinais de ECG precisam ser classificados por especialistas. Para manter o padrão, é ideal ter os mesmos especialistas que trabalharam na classificação dos sinais do banco de dados existente. Dá para notar que o aumento de alguns bancos de dados é complexo. Deve ser por isso que o banco de dados de ECG do MIT-BIH (Massachusetts Institute of Technology - Boston’s Beth Israel Hospital Arrhythmia Database) não recebe novos sinais desde 1980.\nO que fazer quando a quantidade de dados é limitada e insuficiente para possibilitar a convergência dos algoritmos no treinamento? A solução é utilizar os dados de treinamento mais de uma vez. O treinamento realizado com o conjunto completo dos dados é chamado de época. Os algoritmos podem levar várias épocas até convergir. Como os dados utilizados em cada época são os mesmos, para gerar diversidade entre épocas, os dados de treinamento são misturados antes de se iniciar uma nova época (Haykin 2009).\nO ajuste dos pesos do algoritmo LMS, descrito no Algoritmo 1, ocorre de maneira estocástica. O gradiente da função custo é estimado de maneira instantânea, a cada dado de treinamento. Assim, considerando uma época, haverá \\(N_t\\) atualizações dos pesos do LMS e o algoritmo minimiza o erro quadrático instantâneo, ou seja, \\(\\widehat{J}_{MSE}(\\mathbf{w}(n-1))=e^2(n)\\). Cabe definir aqui o conceito de iteração. A iteração do algoritmo ocorre toda vez que os pesos são atualizados. No caso estocástico, temos \\(N_t\\) iterações por época. Note que neste caso, o índice \\(n\\) coincide com iteração, pois o vetor de pesos é atualizado a cada \\(n\\), ou seja, a cada dado de treinamento. Essa forma de atualização estocástica é útil em problemas de tempo real, uma vez que a cada dado de entrada se deseja ter o dado de saída correspondente com o menor atraso possível. Em cancelamento de eco acústico, por exemplo, é essencial que isso ocorra para não gerar atrasos indesejados no sinal de voz. Neste tipo de aplicação, o treinamento ocorre junto com a inferência, ou seja, a saída e o erro calculados no treinamento são utilizados para atualizar os pesos e ao mesmo tempo para se obter a estimativa ou classificação desejada. No entanto, problemas de tempo real não são a maioria entre os problemas de aprendizado de máquina.\nEm aprendizado de máquina, geralmente não estamos interessados em fazer a inferência durante o treinamento. A saída e o erro são utilizados no treinamento apenas para atualizar os pesos do algoritmo. Depois do treinamento, fixam-se os pesos para então se fazer a inferência e testar o classificador ou regressor. Por isso, vamos agora analisar outro caso extremo, em que todos os dados de treinamento são utilizados para estimar o vetor gradiente. Neste caso, o vetor de pesos será atualizado apenas uma vez a cada época. Portanto, teremos apenas uma iteração por época. Vamos supor que o vetor de pesos do LMS acabou de ser atualizado no final da época \\(k-1\\), ou seja, dispomos de \\(\\mathbf{w}(k-1)\\). Assim, ele será atualizado novamente apenas no final da época \\(k\\). Durante a época \\(k\\), estima-se o vetor gradiente como\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1))=-\\frac{2}{N_t}\\sum_{n=1}^{N_t}\\left[d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)\\right]\\mathbf{x}(n).\n\\]\nEsse gradiente deve ser então utilizado no final da época \\(k\\) para atualizar \\(\\mathbf{w}(k-1)\\), ou seja,\n\\[\n\\mathbf{w}(k)=\\mathbf{w}(k-1)-\\frac{\\eta}{2}\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1)).\n\\]\nNa sequência, o vetor \\(\\mathbf{w}(k)\\) é utilizado para estimar o gradiente na época \\(k+1\\) e assim sucessivamente. Essa forma de atualização do vetor de pesos é chamada de modo batch. Neste caso, o algoritmo LMS busca minimizar em cada época a seguinte aproximação da função custo:\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(k-1))=\\frac{1}{N_t}\\sum_{n=1}^{N_t}{e_{k-1}^2(n)}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}[d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)]^2,\n\\]\nem que \\(k=1,2,\\cdots,N_e\\), sendo \\(N_e\\) o número de épocas.\nCabem aqui algumas observações:\n\nO treinamento em modo batch não é utilizado em aplicações de tempo real, pois gera um atraso inaceitável em aplicações desse tipo.\nO índice \\(n\\) neste modo de treinamento não representa iteração e sim a posição do dado no conjunto de treinamento. Dessa forma, para \\(n=5\\) temos \\(\\mathbf{x}(5)\\), que representa o quinto dado do conjunto de treinamento, que por sua vez, contém ao todo \\(N_t\\) dados.\nComo mencionado, quando utilizado o modo estocástico de treinamento, é importante misturar os dados de uma época para outra. Dessa forma o vetor \\(\\mathbf{x}(5)\\) da época \\(k\\) pode ser o vetor \\(\\mathbf{x}(200)\\) da época \\(k-1\\). No entanto, no modo batch, a mistura dos dados de uma época para a outra não tem efeito, já que o valor da função custo e o de seu gradiente são calculados utilizando todo o conjunto de dados.\nNa formulação anterior, a iteração foi representada por \\(k\\), que coincide com as épocas do treinamento.\nOs índices \\(k-1\\) e \\(n\\) no erro \\(e_{k-1}(n)\\) foram utilizados para indicar que ele é calculado com o vetor de pesos \\(\\mathbf{w}(k-1)\\) e com os dados de treinamento \\(\\mathbf{x}(n)\\) e \\(d(n)\\) da posição \\(n\\), respectivamente.\n\nDadas essas observações, na formulação do modo de treinamento batch, é mais conveniente usar a notação matricial, similar à da regressão linear multivariada. Assim, definindo-se na iteração (ou época) \\(k\\) os vetores\n\\[\n\\mathbf{w}(k-1)=\\left[\n  \\begin{array}{c}\n    b(k-1) \\\\\n    w_1(k-1) \\\\\n    \\vdots \\\\\n    w_M(k-1) \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{d}(k)\n=\\left[\n  \\begin{array}{c}\n    d(1) \\\\\n    d(2) \\\\\n    \\vdots \\\\\n    d(N_t) \\\\\n  \\end{array}\n\\right],\n\\;\\;\\;\\;\n\\mathbf{e}(k)\n=\\left[\n  \\begin{array}{c}\n    e_{k-1}(1) \\\\\n    e_{k-1}(2) \\\\\n    \\vdots \\\\\n    e_{k-1}(N_t) \\\\\n  \\end{array}\n\\right]\n\\]\ne a matriz\n\\[\n\\mathbf{X}(k)=\\left[\\begin{array}{c}\n                   \\mathbf{x}^{{\\rm T}}(1) \\\\\n                   \\mathbf{x}^{{\\rm T}}(2) \\\\\n                   \\vdots \\\\\n                   \\mathbf{x}^{{\\rm T}}(N_t)\n                 \\end{array}\n\\right]=\n\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N_t} & x_{2N_t} & \\cdots & x_{MN_t} \\\\\n  \\end{array}\n\\right],\n\\]\npode-se calcular o vetor de erros \\(\\mathbf{e}(k)\\) como\n\\[\n\\mathbf{e}(k)=\\mathbf{d}(k)-\\mathbf{X}(k)\\mathbf{w}(k-1),\n\\]\ne a estimativa do vetor gradiente como\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1))=-\\frac{2}{N_t}\\mathbf{X}^{{\\rm T}}(k)\\mathbf{e}(k).\n\\]\nEssa estimativa do gradiente leva à seguinte atualização dos pesos:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(k)=\\mathbf{w}(k-1)+\\frac{\\eta}{N_t}\\mathbf{X}^{{\\rm T}}(k)\\mathbf{e}(k).\n$}\n\\end{equation*}\\]\nCom essa notação, a aproximação da função custo que o LMS busca minimizar a cada época neste modo pode ser reescrita como\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(k-1))=\\frac{1}{N_t}\\|\\mathbf{e}(k)\\|^2.\n\\]\nComo o treinamento em modo batch não é utilizado em aplicações de tempo real e todos os dados de treinamento estão disponíveis, é mais eficiente atualizar os pesos de forma matricial, o que permite que as contas sejam feitas em paralelo. Na formulação não matricial, o erro \\[e_{k-1}(n)=d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)\\]\né calculado para cada dado de treinamento e utilizado no cálculo \\(e_{k-1}(n)\\mathbf{x}(n)\\) para estimar o gradiente em um loop, o que torna o cálculo ineficiente.\nAinda é possível encontrar uma solução intermediária. Considere que, em toda época, os dados de treinamento sejam divididos em conjuntos de tamanho \\(N_b&lt;N_t\\), que é chamado na literatura de tamanho do mini-batch. Neste caso, teremos \\(N_{mb}\\triangleq \\lfloor N_t/N_b \\rfloor\\) conjuntos de dados a cada época2. Considere que o algoritmo utilize cada um desses conjuntos para estimar o vetor gradiente e com essa estimativa atualize os pesos. Dessa forma, os pesos serão atualizados \\(N_{mb}\\) vezes por época, a cada \\(N_b\\) dados de treinamento. Em outras palavras, o algoritmo terá \\(N_{mb}\\) iterações por época. Apesar dos pesos serem atualizados mais vezes por época que no modo de treinamento batch, o modo mini-batch também não é usado em aplicações de tempo real, o que faz com que a formulação matricial seja mais eficiente. Assim, na iteração \\(m\\), vamos definir os vetores\n\\[\n\\mathbf{w}(m-1)=\\left[\n  \\begin{array}{c}\n    b(m-1) \\\\\n    w_1(m-1) \\\\\n    \\vdots \\\\\n    w_M(m-1) \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{d}(\\ell)\n=\\left[\n  \\begin{array}{c}\n    d({\\ell N_b+1}) \\\\\n    d({\\ell N_b+2}) \\\\\n    \\vdots \\\\\n    d({\\ell N_b+ N_b}) \\\\\n  \\end{array}\n\\right],\n\\;\\;\\;\\;\n\\mathbf{e}_{m-1}(\\ell)\n=\\left[\n  \\begin{array}{c}\n    e_{m-1}({\\ell N_b+1}) \\\\\n    e_{m-1}({\\ell N_b+2}) \\\\\n    \\vdots \\\\\n    e_{m-1}({\\ell N_b+N_b}) \\\\\n  \\end{array}\n\\right]\n\\]\ne a matriz\n\\[\n\\mathbf{X}(\\ell)=\\left[\\begin{array}{c}\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+1) \\\\\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+2) \\\\\n                   \\vdots \\\\\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+N_b)\n                 \\end{array}\n\\right]=\n\\left[\n  \\begin{array}{ccccc}\n    1      & x_{1(\\ell N_b+1)} & x_{2(\\ell N_b+1)} & \\cdots & x_{M(\\ell N_b+1)} \\\\\n    1      & x_{1(\\ell N_b+2)} & x_{2(\\ell N_b+2)} & \\cdots & x_{M(\\ell N_b+2)} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1(\\ell N_b+N_b)} & x_{2(\\ell N_b+N_b)} & \\cdots & x_{M(\\ell N_b+N_b)} \\\\\n  \\end{array}\n\\right],\n\\]\nem que \\(m=1, 2, \\cdots, N_eN_{mb}\\) e \\(\\ell=0, 1, 2, \\cdots, N_{mb}-1\\). Diferente do modo de treinamento batch, iteração no modo mini-batch não coincide com época. Em cada época, temos \\(N_{mb}\\) iterações. Portanto, considerando \\(N_e\\) épocas, teremos \\(N_eN_{mb}\\) iterações no total3.\nUtilizando essas definições, o vetor de erros é dado por\n\\[\n\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\n\\]\ne a estimativa do vetor gradiente por\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(m-1))=-\\frac{2}{N_b}\\mathbf{X}^{{\\rm T}}(\\ell)\\mathbf{e}_{m-1}(\\ell).\n\\]\nEssa estimativa do gradiente leva à seguinte atualização dos pesos:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\frac{\\eta}{N_b}\\mathbf{X}^{{\\rm T}}(\\ell)\\mathbf{e}_{m-1}(\\ell).\n$}\n\\end{equation*}\\]\nPor fim, a aproximação da função custo que o LMS busca minimizar a cada mini-batch pode ser escrita como\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(m-1))=\\frac{1}{N_b}\\|\\mathbf{e}_{m-1}(\\ell)\\|^2.\n\\]\nÉ muito comum na literatura usar o modo mini-batch, já que se obtém uma melhor estimativa do gradiente e consequentemente uma melhor precisão para se alcançar o mínimo da função custo em comparação com o caso estocástico e um menor custo computacional em comparação com o modo batch. O pseudocódigo do algoritmo LMS no modo mini-batch está no Algoritmo 2. Observe que \\(N_b=1\\) leva ao modo de treinamento estocástico e \\(N_b=N_t\\) ao modo batch.\n\n\nExemplo 2 Sumário do algoritmo LMS com mini-batch. \\(N_e\\) é o número de épocas, \\(N_b\\) o tamanho do mini-batch, \\(N_t\\) o número de dados de treinamento e \\(N_{mb}= \\lfloor N_t/N_b \\rfloor\\) o número de mini-batches por época.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(k=1,2,\\ldots, N_e\\), calcule:      Misture os dados de treinamento      Organize os dados na matriz \\(\\mathbf{X}(\\ell)\\) e no vetor \\(\\mathbf{d}(\\ell)\\) para \\(\\ell=0, 1,2,\\ldots, N_{mb}-1\\)      Para \\(\\ell=0, 1,2,\\ldots, N_{mb} - 1\\) calcule:       \\(m=(k-1)N_{mb}+\\ell+1\\)       \\(\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\\)       \\(\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\displaystyle\\frac{\\eta}{N_b}\\mathbf{X}^{\\rm T}(\\ell)\\mathbf{e}_{m-1}(\\ell)\\)      Fim    Fim\n\n\n\n\n\nPara exemplificar os três modos de treinamento do LMS, vamos considerar novamente o problema de classificação das meias-luas. No modo de treinamento estocástico com \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=1\\) e \\(N_b=1\\). Os pesos e bias ao longo das iterações estão mostrados na Figura 8. Pode-se observar que esses parâmetros se aproximam dos valores obtidos com a solução de Wiener, mas como a estimativa do gradiente é instantânea, ocorrem variações em torno desses valores ótimos. No caso, como consideramos apenas uma época, temos \\(5000\\) iterações, valor que coincide com o número de dados de treinamento.\n\n\n\n\n\n\nFigura 8: Pesos e bias do algoritmo LMS no modo de treinamento estocástico (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=1\\) e \\(N_b=1\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nVamos agora considerar o algoritmo LMS no modo de treinamento mini-batch com \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=100\\) e \\(N_b=20\\). Os pesos e bias ao longo das iterações estão mostrados na Figura 9. Pode-se observar que os pesos variam menos em torno dos valores ótimos em comparação com o caso estocástico. Isso ocorre, pois a estimativa do gradiente é feita a cada \\(N_b=20\\) dados do conjunto de treinamento. Como foram consideradas \\(100\\) épocas, temos \\(N_e\\lfloor N_t/N_b\\rfloor=25000\\) iterações.\n\n\n\n\n\n\nFigura 9: Pesos e bias do algoritmo LMS no modo de treinamento mini-batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=100\\) e \\(N_b=20\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nConsiderando agora o algoritmo LMS no modo de treinamento batch com \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_e=1000\\) e \\(N_b=5000\\), os pesos e bias ao longo das iterações estão mostrados na Figura 10. Como o gradiente é estimado a cada época com todos os dados de treinamento, os pesos convergem exatamente para os valores ótimos. Como foram consideradas \\(1000\\) épocas, temos \\(1000\\) iterações.\n\n\n\n\n\n\nFigura 10: Pesos e bias do algoritmo LMS no modo de treinamento batch (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_e=1000\\) e \\(N_b=N_t=5000\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nAs trajetórias dos pesos do algoritmo LMS nesses três modos de treinamento estão mostradas na Figura 11. Pelas trajetórias, é possível ver que o caminho do batch é mais direto e atinge exatamente a solução ótima. Já o caminho do mini-batch é menos direto e varia mais em torno da solução ótima. Por fim, o estocástico é o que mais varia ao longo do caminho e também quando se aproxima da solução ótima. Comparando esses três modos de treinamento, o modo mini-batch é o que apresenta o melhor compromisso entre custo computacional e precisão da resposta e por isso é o mais utilizado em aplicações de aprendizado de máquina.\n\n\n\n\n\n\nFigura 11: Trajetória dos pesos do algoritmo LMS (\\(M=2\\), \\(\\eta\\) epecificado na Figura 8, Figura 9 e Figura 10) nos três modos de treinamento (\\(N_t=5000\\)): estocástico (\\(N_e=1\\) e \\(N_b=1\\)), mini-batch (\\(N_e=100\\) e \\(N_b=20\\)) e batch (\\(N_e=1000\\) e \\(N_b=5000\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\))."
  },
  {
    "objectID": "t_lms.html#o-algoritmo-steepest-descent",
    "href": "t_lms.html#o-algoritmo-steepest-descent",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Na regressão linear multivariada, conhecendo-se o conjunto de dados de treinamento\n\\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1} ,d_1), (x_{12}, x_{22}, \\cdots, x_{M2} ,d_2),\\cdots, (x_{1N_t}, x_{2N_t}, \\cdots, x_{MN_t} ,d_{N_t})\\},\n\\]\nobtém-se um modelo de hiperplano do tipo\n\\[\ny=b+w_1x_1+w_2x_2+\\cdots+w_Mx_M\\approx d,\n\\]\nem que \\(N_t\\) é o número de dados utilizados no treinamento, \\(b\\) o viés (bias), \\(d\\) o sinal desejado, \\(y\\) a estimativa de \\(d\\), \\(x_k\\) o sinal de entrada e \\(w_k\\), \\(k=1,\\cdots, M\\) os pesos do regressor.\nPara obter o modelo, utilizamos os dados de treinamento e calculamos a solução\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1}\\mathbf{X}^{\\rm T}\\mathbf{d}\n$}\n\\end{equation*}\\]\nem que\n\\[\n\\mathbf{w}^{\\rm o}=\\left[\n  \\begin{array}{c}\n    b^{\\rm o} \\\\\n    w_1^{\\rm o} \\\\\n    \\vdots \\\\\n    w_M^{\\rm o} \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{X}=\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N_t} & x_{2N_t} & \\cdots & x_{MN_t} \\\\\n  \\end{array}\n\\right]\\;\\;\\;\\;\\text{e}\\;\\;\\;\\;\n\\mathbf{d}\n=\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n    d_{N_t} \\\\\n  \\end{array}\n\\right].\n\\]\nA solução \\(\\mathbf{w}^{\\rm o}\\) minimiza a norma ao quadrado do vetor de erros, que aqui vamos denotar por \\(J(\\mathbf{w})\\), ou seja,\n\\[\nJ(\\mathbf{w})=\\|\\mathbf{e}\\|^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2,\n\\]\nde modo que \\(\\mathbf{w}^{\\rm o}={\\rm argmin}_{\\mathbf{w}} J(\\mathbf{w})\\).\nO vetor de pesos \\(\\mathbf{w}^{\\rm o}\\) pode ser obtido a partir de um treinamento iterativo, em que cada amostra \\((x_{1n}, x_{2n}, \\cdots, x_{Mn} ,d_n),\\) \\(n=1,2,\\cdots, N_t\\) é apresentada a um algoritmo por vez. Para obter esse algoritmo, vamos primeiramente denotar o vetor de pesos da iteração \\(n\\) por\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T}.\n\\]\nFazendo o mesmo com os dados de treinamento, teremos na iteração \\(n\\) o sinal desejado \\(d(n)=d_n\\), \\(n=1,2,\\cdots,N_t\\) e o vetor de entrada\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}.\n\\]\nO sinal de “saída” desse regressor iterativo é então calculado como\n\\[\ny(n)=\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)=b(n-1)+\\sum_{k=1}^{M}x_{kn}w_k(n-1),\n\\]\nem que \\(\\mathbf{w}(0)=\\mathbf{0}\\). É importante observar que como se trata de um algoritmo iterativo, precisamos inicializar o vetor de pesos. Uma possibilidade é considerar o vetor nulo, embora também seja possível inicializar os pesos de forma aleatória.\nNa regressão linear multivariada, o melhor hiperplano é obtido ao se minimizar o quadrado da norma do vetor de erros, ou seja, deve-se minimizar \\(J(\\mathbf{w})=\\|\\mathbf{e}\\|^2\\), que é comumente chamada de função custo. Aqui, devemos fazer algo semelhante. No entanto, não dispomos de um vetor de erros, pois estamos buscando a solução de forma iterativa, mas podemos calcular o erro de “estimação” em cada iteração, ou seja,\n\\[\ne(n)=d(n)-y(n)=d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)=d(n)- b(n-1)-\\sum_{k=1}^{M}x_{kn}w_k(n-1).\n\\]\nAssim, podemos ajustar os pesos para minimizar o erro quadrático médio (do inglês, mean-square error - MSE), definido como\n\\[\nJ_{\\rm MSE}(\\mathbf{w})={\\rm E}\\{e^2(n)\\},\n\\]\nem que \\({\\rm E}\\{\\cdot\\}\\) representa o operador esperança matemática. Para minimizar essa função, como no caso da regressão linear multivariada, devemos primeiramente derivá-la em relação ao vetor de pesos, o que leva ao vetor gradiente\n\\[\n\\begin{align*}\n\\boldsymbol{\\nabla}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(n-1))&=\\frac{\\partial {\\rm E}\\{e^2(n)\\}}{\\partial \\mathbf{w}(n-1)}=2{\\rm E}\\left\\{e(n)\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}\\right\\}=\n2{\\rm E}\\left\\{e(n)\\left[\\begin{array}{c}\n        \\frac{de(n)}{db(n-1)} \\\\\n        \\\\\n        \\frac{de(n)}{dw_1(n-1)} \\\\\n                \\vdots \\\\\n        \\frac{de(n)}{dw_M(n-1)}\n      \\end{array}\n\\right]\\right\\}\\nonumber\\\\\n&=2{\\rm E}\\left\\{e(n)\\left[\\begin{array}{c}\n        -1 \\\\\n         -x_{1n} \\\\\n                \\vdots \\\\\n        -x_{Mn}\n      \\end{array}\n\\right]\\right\\}\n=-2{\\rm E}\\{e(n)\\mathbf{x}(n)\\}.\n\\end{align*}\n\\]\nIgualando o vetor gradiente ao vetor nulo, obtemos\n\\[\n{\\rm E}\\{e(n)\\mathbf{x}(n)\\}={\\rm E}\\{\\mathbf{x}(n)[d(n)-y(n)]\\}=\\boldsymbol{0},\n\\]\nou ainda\n\\[\n{\\rm E}\\{\\mathbf{x}(n)[d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)]\\}=\\boldsymbol{0}\\Rightarrow\n\\underbrace{{\\rm E}\\{\\mathbf{x}(n)\\mathbf{x}^{\\rm T}(n)\\}}_{\\mathbf{R}}\\mathbf{w}^{\\rm wiener}=\\underbrace{{\\rm E}\\{d(n)\\mathbf{x}(n)\\}}_{\\mathbf{p}}.\n\\]\nA solução dessa equação leva ao MSE mínimo e é conhecida na literatura como solução de Wiener-Hopf, ou simplesmente, solução de Wiener. Por isso, vamos denotá-la como \\(\\mathbf{w}^{\\rm wiener}\\), ou seja,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm wiener}=\\mathbf{R}^{-1}\\mathbf{p},\n$}\n\\end{equation*}\\]\nem que \\(\\mathbf{R}\\) é a matriz de autocorrelação dos dados de entrada e \\(\\mathbf{p}\\) o vetor de correlação cruzada entre o sinal desejado \\(d(n)\\) e os dados de entrada. Tanto a matriz \\(\\mathbf{R}\\) como o vetor \\(\\mathbf{p}\\) têm em suas definições o operador esperança matemática. Podemos estimar \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\) utilizando todos os dados de treinamento, o que leva respectivamente a\n\\[\n\\widehat{\\mathbf{R}}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}\\mathbf{x}(n)\\mathbf{x}^{\\rm T}(n)\\;\\;\\;\\text{e}\\;\\;\\;\\widehat{\\mathbf{p}}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}d(n)\\mathbf{x}(n).\n\\]\nNeste caso, a solução obtida com a regressão linear multivariada coincide com a solução de Wiener, ou seja, \\(\\mathbf{w}^{\\rm o}=\\mathbf{w}^{\\rm wiener}\\). Além disso, essa solução é única para um dado conjunto de treinamento.\nAo acompanhar esse cálculo, você pode estar se perguntando: onde está o algoritmo iterativo para o cálculo dos pesos? Ele pode ser obtido utilizando o método do gradiente. Em Cálculo, aprendemos que o gradiente de uma função aponta para a direção de maior variação da mesma. Como a solução é única, basta considerar o sentido contrário do gradiente, o que leva a\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)-\\frac{\\eta}{2}\\boldsymbol{\\nabla}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(n-1)),\n\\]\nem que \\(\\eta\\) é um passo de adaptação. Substituindo a expressão do gradiente, chega-se a\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta{\\rm E}\\{e(n)\\mathbf{x}(n)\\},\n\\]\nou ainda\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta{\\rm E}\\{\\mathbf{x}(n)[d(n)-\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)]\\}.\n\\]\nIdentificando a matriz \\(\\mathbf{R}\\) e o vetor \\(\\mathbf{p}\\) na equação acima, obtemos\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\left[\\mathbf{p}-\\mathbf{R}\\mathbf{w}(n-1)\\right].\n$}\n\\end{equation*}\\]\nEsse algoritmo iterativo é conhecido na literatura como steepest descent algorithm ou algoritmo do gradiente exato. O passo de adaptação \\(\\eta\\) tem um papel fundamental em sua convergência. É possível demonstrar que se o intervalo \\(0&lt;\\eta&lt;2/{\\lambda_{\\max}}\\) for atendido, em que \\(\\lambda_{\\max}\\) é o autovalor máximo da matriz \\(\\mathbf{R}\\), essa equação converge exatamente para a solução de Wiener (Nascimento e Silva 2014),(Haykin 2014),(Sayed 2008). Apesar de chegar exatamente à solução que minimiza o MSE, ele não é adequado porque é necessário conhecer \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\). A única vantagem é evitar calcular a inversa da matriz \\(\\mathbf{R}\\), o que representa uma economia em custo computacional. Apesar de ser pouco utilizado na prática, esse algoritmo é fundamental para entendermos o algoritmo LMS a seguir."
  },
  {
    "objectID": "t_lms.html#o-algoritmo-lms",
    "href": "t_lms.html#o-algoritmo-lms",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Uma maneira de simplificar os cálculos para evitar ter de conhecer \\(\\mathbf{R}\\) e \\(\\mathbf{p}\\), é estimar essas grandezas instantâneamente, o que leva respectivamente a\n\\[\n\\widehat{\\mathbf{R}}(n)=\\mathbf{x}(n)\\mathbf{x}^{{\\rm T}}(n)\\;\\;\\;\\text{e}\\;\\;\\;\\widehat{\\mathbf{p}}(n)=d(n)\\mathbf{x}(n).\n\\]\nSubstituindo essas aproximações no algoritmo steepest descent, chega-se a\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n),\n$}\n\\end{equation*}\\]\nque é a equação de atualização do conhecido algoritmo LMS (least-mean-square), cujo pseudocódigo está mostrado no Algoritmo 1. O fluxo de sinal do LMS é mostrado na Figura 1. Novamente, o passo de adaptação \\(\\eta\\) tem um papel fundamental na convergência desse algoritmo. Quanto menor o valor de \\(\\eta\\), mais próximo da solução de Wiener o algoritmo LMS estará quando atingir o regime estacionário. No entanto, quanto menor o passo, mais lentamente o algoritmo atingirá o regime. Em contrapartida, passos grandes podem representar convergências rápidas, mas também podem levar o algoritmo à divergência. Neste caso, os pesos podem ir para infinito1. Diante disso, deve-se atentar ao compromisso entre precisão da solução e velocidade de convergência. O problema é que o intervalo \\(0&lt;\\eta&lt;2/{\\lambda_{\\max}}\\) que vale para o algoritmo exato, é em geral maior do que o intervalo de passo admitido no algoritmo aproximado. Pode-se demonstrar que \\(0&lt;\\eta&lt;2/(3{\\lambda_{\\max}})\\) é um intervalo mais razoável para o algoritmo LMS, mas ainda não garante sua convergência. Devido à sua simplicidade, ele é muito usado em diversas aplicações de filtragem adaptativa que exigem solução em tempo real. As principais aplicações incluem cancelamento de eco acústico, equalização de canais de comunicação, controle ativo de ruído e identificação de sistemas (Nascimento e Silva 2014),(Haykin 2014),(Sayed 2008).\nPara finalizar esta seção, é importante observar que em várias aplicações de filtragem adaptativa não se considera o bias. Além disso, o vetor de entrada \\(\\mathbf{x}(n)\\) muitas vezes é extraído de uma sequência de números, considerando uma linha de atrasos. Neste caso, ele é chamado de vetor regressor. Para exemplificar, vamos supor que temos a seguinte sequência de números\n\\[\n\\begin{array}{c}\n\\vdots\\\\\nx(n+1)=9\\\\\n    \\;\\;\\;\\;\\;\\;x(n)=1 \\\\\n    x(n-1)=2 \\\\\n    x(n-2)=3 \\\\\n    x(n-3)=4 \\\\\n\\!\\!\\!\\!\\!\\vdots\n  \\end{array},\n\\]\nem que \\(n\\) representa um instante de tempo ou uma posição. Considerando \\(M=3\\) e levando em conta a linha de atrasos sem o bias, os vetores de entrada do LMS nos instantes \\(n-1\\), \\(n\\) e \\(n+1\\) são dados respectivamente por\n\\[\n\\mathbf{x}(n-1)=[\\,2\\;\\; 3\\;\\;4\\,]^{{\\rm T}},\\;\\;\\;\\mathbf{x}(n)=[\\,1\\;\\; 2\\;\\;3\\,]^{{\\rm T}}\\;\\;\\;\\text{e}\\;\\;\\;\\mathbf{x}(n+1)=[\\,9\\;\\; 1\\;\\;2\\,]^{{\\rm T}}.\n\\]\nGeneralizando, tem-se\n\\[\n\\mathbf{x}(n)=[\\,x(n)\\;\\;x(n-1)\\;\\;\\cdots\\;\\;x(n-M+1)\\,]^{{\\rm T}}.\n\\]\nPara quem já estudou Processamento de Sinais, o LMS com esse vetor de entrada é um filtro com resposta ao impulso de duração finita (FIR - finite impulse response), cujos coeficientes variam ao longo do tempo.\n\n\nExemplo 1 Sumário do algoritmo LMS.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(n=1,2,\\ldots,\\) calcule:      \\({y}(n)=\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(n-1)\\)      \\(e(n)=d(n)-y(n)\\)      \\(\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n)\\)    Fim\n\n\n\n\n\n\n\n\nFigura 1: Fluxo de sinal do algoritmo LMS."
  },
  {
    "objectID": "t_lms.html#exemplo-de-classificação-com-o-lms",
    "href": "t_lms.html#exemplo-de-classificação-com-o-lms",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Considere o exemplo de classificação das “meias-luas” da Figura 2. A meia-lua chamada de “Região A” está posicionada simetricamente em relação ao eixo \\(x_2\\), enquanto a meia-lua chamada de “Região B” está deslocada de \\(r_1\\) à direita do eixo \\(x_2\\) e de \\(r_2\\) abaixo do eixo \\(x_1\\). As duas meias-luas têm raio \\(r_1\\) e largura \\(r_3\\) idênticos. A distância vertical \\(r_2\\) que separa as duas meias-luas é ajustável e medida em relação ao eixo \\(x_1\\). Para \\(r_2&gt;0\\), quanto maior o valor de \\(r_2\\), maior a separação entre as meias-luas. Já para \\(r_2&lt;0\\), quando mais negativo for \\(r_2\\), mais próximas ficam as meias-luas (Haykin 2009).\n\n\n\n\n\n\nFigura 2: O problema de classificação das meias-luas (Haykin 2009).\n\n\n\nO conjunto de treinamento consiste em 5000 pontos, 2500 pertencentes à Região A e 2500 à Região B. Esses pontos são sorteados aleatoriamente. Assim, os pontos da Região A são sorteados considerando\n\\[\n(\\rho\\cos\\theta,\\;\\rho\\,{\\rm sen\\,}\\theta),\n\\]\nem que \\(\\theta\\) é uma variável aleatória uniformemente distribuída no intervalo \\([0,\\;\\pi]\\) e \\(\\rho\\) é outra variável aleatória uniformemente distribuída no intervalo \\([r_1-r_3/2,\\;\\;r_1+r_3/2]\\). Para essa região, considera-se que o sinal desejado é igual a um (\\(d=1\\)). Para gerar os pontos da Região B, basta considerar os deslocamentos, ou seja,\n\\[\n(\\rho\\cos\\theta+r_1,\\;\\;-\\rho\\,{\\rm sen}\\theta-r_2)\n\\]\ne \\(d=-1\\) como sinal desejado. O conjunto de teste consiste em 2000 pontos, 1000 pontos de cada região, gerados de forma independente do conjunto de treinamento.\nPara \\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\), considerou-se o algoritmo LMS com passo \\(\\eta=10^{-3}\\) e \\(M=2\\). Os dados de treinamento estão mostrados na Figura 3. Na Figura Figura 4, são mostrados a saída do algoritmo, o erro quadrático em dB e os pesos e bias ao longo das iterações. São mostrados também os pesos e bias da solução de Wiener (retas tracejadas em vermelho). Como esperado, os pesos e bias do algoritmo LMS se aproximam dos valores obtidos com a solução de Wiener, mas não convergem exatamente para eles. É possível observar que a velocidade de convergência dos pesos e bias do LMS são diferentes. O bias do LMS só atinge um valor próximo de\n\\(b=-0,086\\) (bias da solução de Wiener) perto de \\(n=5000\\), enquanto os pesos chegam próximos dos de Wiener perto de \\(n=200\\). Em geral, não se usa o bias em aplicações de filtragem adaptativa. Considera-se esse parâmetro aqui apenas para obter um modelo de neurônio linear. A saída do LMS no treinamento não mostra uma separação clara entre os dois valores possíveis para o sinal desejado (\\(\\pm 1\\)). Apesar disso, é possível verificar na Figura 5 com os dados de teste que há apenas uma pequena quantidade de dados da Região B que foram classificados erroneamente como pertencentes à Região A, o que leva a uma taxa de erro de aproximadamente 0,6%. Considerando os pesos da última iteração do LMS (\\(n=5000\\)), obtém-se a solução “linear” dada pela separação das regiões, mostrada na Figura 5.\n\nCódigo\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNA=2500 # número de pontos de treinamento da Região A\nNB=2500 # número de pontos de treinamento da Região B\nNt=NA+NB # total de dados de treinamento\n\n# dados das meia luas \nr1=10\nr3=6\nr2=1\nrmin=r1-r3/2\nrmax=r1+r3/2\n\n# Pontos da Região A\na=np.pi*np.random.rand(NA, 1)\nrxy=np.random.uniform(rmin,rmax,(NA,1))\nxA=rxy*np.cos(a)\nyA=rxy*np.sin(a)\ndA=np.ones((NA, 1))\npontosA=np.hstack((xA, yA, dA))\n\n# Pontos da Região B\na=np.pi*np.random.rand(NB, 1)\nrxy=np.random.uniform(rmin,rmax,(NB, 1))\nxB=rxy*np.cos(a)+r1\nyB=-rxy*np.sin(a)-r2\ndB=-np.ones((NB, 1))\npontosB=np.hstack((xB, yB, dB))\n\n#Concatenando e embaralhando os dados de treinamento\ndados_treino=np.vstack((pontosA, pontosB))\nnp.random.shuffle(dados_treino)\n\n# Figura para mostrar os dados de treino\nfig, ax1 = plt.subplots()\nax1.plot(xA,yA,'.b')\nax1.plot(xB,yB,'.r')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.grid(axis='x', color='0.5')\nplt.grid(axis='y', color='0.5')\n\n\n\n\n\n\n\n\n\n\nFigura 3: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Dados de treinamento (\\(N_t=1000\\)).\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 4: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Algoritmo LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)): saída do algoritmo, erro quadrático em dB e pesos e bias ao longo das iterações. As retas vermelhas tracejadas representam os valores dos pesos e bias da solução de Wiener.\n\n\n\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Dados de teste (\\(N_{\\rm teste}=2000\\)) e separação das duas regiões obtida com o LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)). Os dados da Região B classificados erroneamente são os pontos vermelhos sob o fundo azul (próximos de \\(x_2=1\\)), que neste caso leva a uma taxa de erro de 0,6%.\n\n\n\nAo diminuir o passo do algoritmo LMS para \\(\\eta=5\\times 10^{-5}\\), é possível observar na Figura 6 que o algoritmo LMS tem uma convergência mais lenta: os pesos levam cerca de \\(n=10^3\\) iterações para convergir enquanto o bias leva cerca de \\(n=10^5\\) iterações. Neste caso, foram considerados \\(N_t=10^5\\) dados de treinamento para que o algoritmo atingisse o regime permanente. Em contrapartida, a solução do algoritmo se torna mais próxima da de Wiener, como esperado. Assim, o projetista deve sempre ter em mente o compromisso entre passo de adaptação e precisão da solução. Apesar de mais precisa, a solução atingida ainda leva a erros na classificação como ocorre na Figura 5.\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Algoritmo LMS (\\(M=2\\), \\(\\eta=10^{-5}\\)): pesos ao longo das iterações. As retas vermelhas tracejadas representam os valores dos pesos da solução de Wiener.\n\n\n\nConsiderando agora \\(r_2=-4\\), as meias-luas se tornam mais próximas, o que faz com que o algoritmo LMS chegue a uma solução que leva a mais erros: pontos da Região A são classificados erroneamente como pertencentes à Região B e vice-versa, como é possível observar na Figura 7. Neste caso, a taxa de erro aumenta para aproximadamente 11,5%. Para se obter uma solução sem erros para \\(r_2=-4\\), é necessário considerar um classificador não linear.\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Dados de teste (\\(N_{\\rm teste}=2000\\)) e separação das duas regiões obtida com o LMS (\\(M=2\\), \\(\\eta=10^{-3}\\)). Os dados classificados erroneamente são os pontos azuis sob o fundo vermelho e os pontos vermelhos sob o fundo azul, que neste caso leva a uma taxa de erro de 11,5%."
  },
  {
    "objectID": "t_lms.html#época-batch-mini-batch-e-iteração",
    "href": "t_lms.html#época-batch-mini-batch-e-iteração",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Em diversas aplicações, o banco de dados é limitado. Esse é o caso, por exemplo, do problema de classificação de arritmias cardíacas utilizando sinais de eletrocardiograma (ECG). A aquisição de novos sinais deve seguir o padrão do banco de dados existente: os sinais precisam ser amostrados com a mesma frequência, os sensores devem ser os mesmos, o exame deve seguir o mesmo protocolo etc. Vamos supor que seja possível garantir o mesmo padrão de aquisição dos sinais. Depois de serem adquiridos, os novos sinais de ECG precisam ser classificados por especialistas. Para manter o padrão, é ideal ter os mesmos especialistas que trabalharam na classificação dos sinais do banco de dados existente. Dá para notar que o aumento de alguns bancos de dados é complexo. Deve ser por isso que o banco de dados de ECG do MIT-BIH (Massachusetts Institute of Technology - Boston’s Beth Israel Hospital Arrhythmia Database) não recebe novos sinais desde 1980.\nO que fazer quando a quantidade de dados é limitada e insuficiente para possibilitar a convergência dos algoritmos no treinamento? A solução é utilizar os dados de treinamento mais de uma vez. O treinamento realizado com o conjunto completo dos dados é chamado de época. Os algoritmos podem levar várias épocas até convergir. Como os dados utilizados em cada época são os mesmos, para gerar diversidade entre épocas, os dados de treinamento são misturados antes de se iniciar uma nova época (Haykin 2009).\nO ajuste dos pesos do algoritmo LMS, descrito no Algoritmo 1, ocorre de maneira estocástica. O gradiente da função custo é estimado de maneira instantânea, a cada dado de treinamento. Assim, considerando uma época, haverá \\(N_t\\) atualizações dos pesos do LMS e o algoritmo minimiza o erro quadrático instantâneo, ou seja, \\(\\widehat{J}_{MSE}(\\mathbf{w}(n-1))=e^2(n)\\). Cabe definir aqui o conceito de iteração. A iteração do algoritmo ocorre toda vez que os pesos são atualizados. No caso estocástico, temos \\(N_t\\) iterações por época. Note que neste caso, o índice \\(n\\) coincide com iteração, pois o vetor de pesos é atualizado a cada \\(n\\), ou seja, a cada dado de treinamento. Essa forma de atualização estocástica é útil em problemas de tempo real, uma vez que a cada dado de entrada se deseja ter o dado de saída correspondente com o menor atraso possível. Em cancelamento de eco acústico, por exemplo, é essencial que isso ocorra para não gerar atrasos indesejados no sinal de voz. Neste tipo de aplicação, o treinamento ocorre junto com a inferência, ou seja, a saída e o erro calculados no treinamento são utilizados para atualizar os pesos e ao mesmo tempo para se obter a estimativa ou classificação desejada. No entanto, problemas de tempo real não são a maioria entre os problemas de aprendizado de máquina.\nEm aprendizado de máquina, geralmente não estamos interessados em fazer a inferência durante o treinamento. A saída e o erro são utilizados no treinamento apenas para atualizar os pesos do algoritmo. Depois do treinamento, fixam-se os pesos para então se fazer a inferência e testar o classificador ou regressor. Por isso, vamos agora analisar outro caso extremo, em que todos os dados de treinamento são utilizados para estimar o vetor gradiente. Neste caso, o vetor de pesos será atualizado apenas uma vez a cada época. Portanto, teremos apenas uma iteração por época. Vamos supor que o vetor de pesos do LMS acabou de ser atualizado no final da época \\(k-1\\), ou seja, dispomos de \\(\\mathbf{w}(k-1)\\). Assim, ele será atualizado novamente apenas no final da época \\(k\\). Durante a época \\(k\\), estima-se o vetor gradiente como\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1))=-\\frac{2}{N_t}\\sum_{n=1}^{N_t}\\left[d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)\\right]\\mathbf{x}(n).\n\\]\nEsse gradiente deve ser então utilizado no final da época \\(k\\) para atualizar \\(\\mathbf{w}(k-1)\\), ou seja,\n\\[\n\\mathbf{w}(k)=\\mathbf{w}(k-1)-\\frac{\\eta}{2}\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1)).\n\\]\nNa sequência, o vetor \\(\\mathbf{w}(k)\\) é utilizado para estimar o gradiente na época \\(k+1\\) e assim sucessivamente. Essa forma de atualização do vetor de pesos é chamada de modo batch. Neste caso, o algoritmo LMS busca minimizar em cada época a seguinte aproximação da função custo:\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(k-1))=\\frac{1}{N_t}\\sum_{n=1}^{N_t}{e_{k-1}^2(n)}=\\frac{1}{N_t}\\sum_{n=1}^{N_t}[d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)]^2,\n\\]\nem que \\(k=1,2,\\cdots,N_e\\), sendo \\(N_e\\) o número de épocas.\nCabem aqui algumas observações:\n\nO treinamento em modo batch não é utilizado em aplicações de tempo real, pois gera um atraso inaceitável em aplicações desse tipo.\nO índice \\(n\\) neste modo de treinamento não representa iteração e sim a posição do dado no conjunto de treinamento. Dessa forma, para \\(n=5\\) temos \\(\\mathbf{x}(5)\\), que representa o quinto dado do conjunto de treinamento, que por sua vez, contém ao todo \\(N_t\\) dados.\nComo mencionado, quando utilizado o modo estocástico de treinamento, é importante misturar os dados de uma época para outra. Dessa forma o vetor \\(\\mathbf{x}(5)\\) da época \\(k\\) pode ser o vetor \\(\\mathbf{x}(200)\\) da época \\(k-1\\). No entanto, no modo batch, a mistura dos dados de uma época para a outra não tem efeito, já que o valor da função custo e o de seu gradiente são calculados utilizando todo o conjunto de dados.\nNa formulação anterior, a iteração foi representada por \\(k\\), que coincide com as épocas do treinamento.\nOs índices \\(k-1\\) e \\(n\\) no erro \\(e_{k-1}(n)\\) foram utilizados para indicar que ele é calculado com o vetor de pesos \\(\\mathbf{w}(k-1)\\) e com os dados de treinamento \\(\\mathbf{x}(n)\\) e \\(d(n)\\) da posição \\(n\\), respectivamente.\n\nDadas essas observações, na formulação do modo de treinamento batch, é mais conveniente usar a notação matricial, similar à da regressão linear multivariada. Assim, definindo-se na iteração (ou época) \\(k\\) os vetores\n\\[\n\\mathbf{w}(k-1)=\\left[\n  \\begin{array}{c}\n    b(k-1) \\\\\n    w_1(k-1) \\\\\n    \\vdots \\\\\n    w_M(k-1) \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{d}(k)\n=\\left[\n  \\begin{array}{c}\n    d(1) \\\\\n    d(2) \\\\\n    \\vdots \\\\\n    d(N_t) \\\\\n  \\end{array}\n\\right],\n\\;\\;\\;\\;\n\\mathbf{e}(k)\n=\\left[\n  \\begin{array}{c}\n    e_{k-1}(1) \\\\\n    e_{k-1}(2) \\\\\n    \\vdots \\\\\n    e_{k-1}(N_t) \\\\\n  \\end{array}\n\\right]\n\\]\ne a matriz\n\\[\n\\mathbf{X}(k)=\\left[\\begin{array}{c}\n                   \\mathbf{x}^{{\\rm T}}(1) \\\\\n                   \\mathbf{x}^{{\\rm T}}(2) \\\\\n                   \\vdots \\\\\n                   \\mathbf{x}^{{\\rm T}}(N_t)\n                 \\end{array}\n\\right]=\n\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N_t} & x_{2N_t} & \\cdots & x_{MN_t} \\\\\n  \\end{array}\n\\right],\n\\]\npode-se calcular o vetor de erros \\(\\mathbf{e}(k)\\) como\n\\[\n\\mathbf{e}(k)=\\mathbf{d}(k)-\\mathbf{X}(k)\\mathbf{w}(k-1),\n\\]\ne a estimativa do vetor gradiente como\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(k-1))=-\\frac{2}{N_t}\\mathbf{X}^{{\\rm T}}(k)\\mathbf{e}(k).\n\\]\nEssa estimativa do gradiente leva à seguinte atualização dos pesos:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(k)=\\mathbf{w}(k-1)+\\frac{\\eta}{N_t}\\mathbf{X}^{{\\rm T}}(k)\\mathbf{e}(k).\n$}\n\\end{equation*}\\]\nCom essa notação, a aproximação da função custo que o LMS busca minimizar a cada época neste modo pode ser reescrita como\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(k-1))=\\frac{1}{N_t}\\|\\mathbf{e}(k)\\|^2.\n\\]\nComo o treinamento em modo batch não é utilizado em aplicações de tempo real e todos os dados de treinamento estão disponíveis, é mais eficiente atualizar os pesos de forma matricial, o que permite que as contas sejam feitas em paralelo. Na formulação não matricial, o erro \\[e_{k-1}(n)=d(n)-\\mathbf{x}^{{\\rm T}}(n)\\mathbf{w}(k-1)\\]\né calculado para cada dado de treinamento e utilizado no cálculo \\(e_{k-1}(n)\\mathbf{x}(n)\\) para estimar o gradiente em um loop, o que torna o cálculo ineficiente.\nAinda é possível encontrar uma solução intermediária. Considere que, em toda época, os dados de treinamento sejam divididos em conjuntos de tamanho \\(N_b&lt;N_t\\), que é chamado na literatura de tamanho do mini-batch. Neste caso, teremos \\(N_{mb}\\triangleq \\lfloor N_t/N_b \\rfloor\\) conjuntos de dados a cada época2. Considere que o algoritmo utilize cada um desses conjuntos para estimar o vetor gradiente e com essa estimativa atualize os pesos. Dessa forma, os pesos serão atualizados \\(N_{mb}\\) vezes por época, a cada \\(N_b\\) dados de treinamento. Em outras palavras, o algoritmo terá \\(N_{mb}\\) iterações por época. Apesar dos pesos serem atualizados mais vezes por época que no modo de treinamento batch, o modo mini-batch também não é usado em aplicações de tempo real, o que faz com que a formulação matricial seja mais eficiente. Assim, na iteração \\(m\\), vamos definir os vetores\n\\[\n\\mathbf{w}(m-1)=\\left[\n  \\begin{array}{c}\n    b(m-1) \\\\\n    w_1(m-1) \\\\\n    \\vdots \\\\\n    w_M(m-1) \\\\\n  \\end{array}\n\\right],\\;\\;\\;\\;\n\\mathbf{d}(\\ell)\n=\\left[\n  \\begin{array}{c}\n    d({\\ell N_b+1}) \\\\\n    d({\\ell N_b+2}) \\\\\n    \\vdots \\\\\n    d({\\ell N_b+ N_b}) \\\\\n  \\end{array}\n\\right],\n\\;\\;\\;\\;\n\\mathbf{e}_{m-1}(\\ell)\n=\\left[\n  \\begin{array}{c}\n    e_{m-1}({\\ell N_b+1}) \\\\\n    e_{m-1}({\\ell N_b+2}) \\\\\n    \\vdots \\\\\n    e_{m-1}({\\ell N_b+N_b}) \\\\\n  \\end{array}\n\\right]\n\\]\ne a matriz\n\\[\n\\mathbf{X}(\\ell)=\\left[\\begin{array}{c}\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+1) \\\\\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+2) \\\\\n                   \\vdots \\\\\n                   \\mathbf{x}^{{\\rm T}}(\\ell N_b+N_b)\n                 \\end{array}\n\\right]=\n\\left[\n  \\begin{array}{ccccc}\n    1      & x_{1(\\ell N_b+1)} & x_{2(\\ell N_b+1)} & \\cdots & x_{M(\\ell N_b+1)} \\\\\n    1      & x_{1(\\ell N_b+2)} & x_{2(\\ell N_b+2)} & \\cdots & x_{M(\\ell N_b+2)} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1(\\ell N_b+N_b)} & x_{2(\\ell N_b+N_b)} & \\cdots & x_{M(\\ell N_b+N_b)} \\\\\n  \\end{array}\n\\right],\n\\]\nem que \\(m=1, 2, \\cdots, N_eN_{mb}\\) e \\(\\ell=0, 1, 2, \\cdots, N_{mb}-1\\). Diferente do modo de treinamento batch, iteração no modo mini-batch não coincide com época. Em cada época, temos \\(N_{mb}\\) iterações. Portanto, considerando \\(N_e\\) épocas, teremos \\(N_eN_{mb}\\) iterações no total3.\nUtilizando essas definições, o vetor de erros é dado por\n\\[\n\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\n\\]\ne a estimativa do vetor gradiente por\n\\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\rm MSE}(\\mathbf{w}(m-1))=-\\frac{2}{N_b}\\mathbf{X}^{{\\rm T}}(\\ell)\\mathbf{e}_{m-1}(\\ell).\n\\]\nEssa estimativa do gradiente leva à seguinte atualização dos pesos:\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\frac{\\eta}{N_b}\\mathbf{X}^{{\\rm T}}(\\ell)\\mathbf{e}_{m-1}(\\ell).\n$}\n\\end{equation*}\\]\nPor fim, a aproximação da função custo que o LMS busca minimizar a cada mini-batch pode ser escrita como\n\\[\n\\widehat{J}_{MSE}(\\mathbf{w}(m-1))=\\frac{1}{N_b}\\|\\mathbf{e}_{m-1}(\\ell)\\|^2.\n\\]\nÉ muito comum na literatura usar o modo mini-batch, já que se obtém uma melhor estimativa do gradiente e consequentemente uma melhor precisão para se alcançar o mínimo da função custo em comparação com o caso estocástico e um menor custo computacional em comparação com o modo batch. O pseudocódigo do algoritmo LMS no modo mini-batch está no Algoritmo 2. Observe que \\(N_b=1\\) leva ao modo de treinamento estocástico e \\(N_b=N_t\\) ao modo batch.\n\n\nExemplo 2 Sumário do algoritmo LMS com mini-batch. \\(N_e\\) é o número de épocas, \\(N_b\\) o tamanho do mini-batch, \\(N_t\\) o número de dados de treinamento e \\(N_{mb}= \\lfloor N_t/N_b \\rfloor\\) o número de mini-batches por época.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(k=1,2,\\ldots, N_e\\), calcule:      Misture os dados de treinamento      Organize os dados na matriz \\(\\mathbf{X}(\\ell)\\) e no vetor \\(\\mathbf{d}(\\ell)\\) para \\(\\ell=0, 1,2,\\ldots, N_{mb}-1\\)      Para \\(\\ell=0, 1,2,\\ldots, N_{mb} - 1\\) calcule:       \\(m=(k-1)N_{mb}+\\ell+1\\)       \\(\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\\)       \\(\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\displaystyle\\frac{\\eta}{N_b}\\mathbf{X}^{\\rm T}(\\ell)\\mathbf{e}_{m-1}(\\ell)\\)      Fim    Fim"
  },
  {
    "objectID": "t_lms.html#exemplo-do-lms-nos-três-modos-de-treinamento",
    "href": "t_lms.html#exemplo-do-lms-nos-três-modos-de-treinamento",
    "title": "O algoritmo LMS",
    "section": "",
    "text": "Para exemplificar os três modos de treinamento do LMS, vamos considerar novamente o problema de classificação das meias-luas. No modo de treinamento estocástico com \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=1\\) e \\(N_b=1\\). Os pesos e bias ao longo das iterações estão mostrados na Figura 8. Pode-se observar que esses parâmetros se aproximam dos valores obtidos com a solução de Wiener, mas como a estimativa do gradiente é instantânea, ocorrem variações em torno desses valores ótimos. No caso, como consideramos apenas uma época, temos \\(5000\\) iterações, valor que coincide com o número de dados de treinamento.\n\n\n\n\n\n\nFigura 8: Pesos e bias do algoritmo LMS no modo de treinamento estocástico (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=1\\) e \\(N_b=1\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nVamos agora considerar o algoritmo LMS no modo de treinamento mini-batch com \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=100\\) e \\(N_b=20\\). Os pesos e bias ao longo das iterações estão mostrados na Figura 9. Pode-se observar que os pesos variam menos em torno dos valores ótimos em comparação com o caso estocástico. Isso ocorre, pois a estimativa do gradiente é feita a cada \\(N_b=20\\) dados do conjunto de treinamento. Como foram consideradas \\(100\\) épocas, temos \\(N_e\\lfloor N_t/N_b\\rfloor=25000\\) iterações.\n\n\n\n\n\n\nFigura 9: Pesos e bias do algoritmo LMS no modo de treinamento mini-batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_e=100\\) e \\(N_b=20\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nConsiderando agora o algoritmo LMS no modo de treinamento batch com \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_e=1000\\) e \\(N_b=5000\\), os pesos e bias ao longo das iterações estão mostrados na Figura 10. Como o gradiente é estimado a cada época com todos os dados de treinamento, os pesos convergem exatamente para os valores ótimos. Como foram consideradas \\(1000\\) épocas, temos \\(1000\\) iterações.\n\n\n\n\n\n\nFigura 10: Pesos e bias do algoritmo LMS no modo de treinamento batch (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_e=1000\\) e \\(N_b=N_t=5000\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)).\n\n\n\nAs trajetórias dos pesos do algoritmo LMS nesses três modos de treinamento estão mostradas na Figura 11. Pelas trajetórias, é possível ver que o caminho do batch é mais direto e atinge exatamente a solução ótima. Já o caminho do mini-batch é menos direto e varia mais em torno da solução ótima. Por fim, o estocástico é o que mais varia ao longo do caminho e também quando se aproxima da solução ótima. Comparando esses três modos de treinamento, o modo mini-batch é o que apresenta o melhor compromisso entre custo computacional e precisão da resposta e por isso é o mais utilizado em aplicações de aprendizado de máquina.\n\n\n\n\n\n\nFigura 11: Trajetória dos pesos do algoritmo LMS (\\(M=2\\), \\(\\eta\\) epecificado na Figura 8, Figura 9 e Figura 10) nos três modos de treinamento (\\(N_t=5000\\)): estocástico (\\(N_e=1\\) e \\(N_b=1\\)), mini-batch (\\(N_e=100\\) e \\(N_b=20\\)) e batch (\\(N_e=1000\\) e \\(N_b=5000\\)). Problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\))."
  },
  {
    "objectID": "t_lms.html#footnotes",
    "href": "t_lms.html#footnotes",
    "title": "O algoritmo LMS",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nInfinito aqui significa valor acima do maior valor representável em um software numérico ou hardware.↩︎\nConsidera-se o arredondamento para baixo para que o número de conjuntos de dados por época seja sempre inteiro. Assim, por exemplo, se \\(N_t=1233\\) e \\(N_b=50\\), consideram-se \\(N_{mb}=24\\) conjuntos com \\(50\\) dados por época. Como os dados são misturados a dada época, os \\(33\\) dados desprezados em uma determinada época aparecerão em outras.↩︎\nDiferente também do caso batch, inserimos aqui o índice \\(m-1\\) ao vetor de erros \\(\\mathbf{e}_{m-1}(\\ell)\\). Isso foi feito porque o índice \\(\\ell\\) se repete em cada época e não coincide com a iteração. Por exemplo, podemos ter \\(\\mathbf{e}_{200}(5)\\) e \\(\\mathbf{e}_{50}(5)\\). No cálculo do primeiro vetor se utiliza \\(\\mathbf{w}(200)\\), enquanto no do segundo se utiliza \\(\\mathbf{w}(50)\\). Ambos os vetores são calculados com dados da posição \\(5N_b+1\\) à posição \\(5N_b+N_b\\). Apesar disso, os dados não são necessariamente os mesmos porque eles são misturados antes do início de cada época.↩︎"
  },
  {
    "objectID": "ex_aula_lms_modos.html",
    "href": "ex_aula_lms_modos.html",
    "title": "Exercício - Algoritmo LMS e modos de treinamento",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_rl.html",
    "href": "ex_aula_rl.html",
    "title": "Exercício - Regressão Linear",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html",
    "href": "t_pytorch_exemplo_mlp.html",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "Segue a implementação comentada em PyTorch da MLP para solução do problemas das meias luas.\n\n\nIniciando com a importação das bibliotecas:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\n# Fixando seeds para poder reproduzir os resultados\nnp.random.seed(111)\ntorch.manual_seed(111)\ntorch.cuda.manual_seed(111)\n\nDefinindo a função para gerar dados para treinamento:\n\n# Função para garar dados de treinamento\ndef meias_luas(NA, NB, r1, r2, r3):\n    \"\"\"\n    dados = meias_luas(NA,NB,r1,r2,r3)\n    NA: número de pontos da região A\n    NB: número de pontos da região B\n    r1, r2 e r3: dados das meias-luas\n    \"\"\"\n\n    # total de dados de treinamento\n    Nt = NA + NB\n\n    # dados das meia luas\n    rmin = r1 - r3 / 2\n    rmax = r1 + r3 / 2\n\n    # Pontos da Região A\n    a = np.pi * np.random.rand(NA, 1)\n    rxy = np.random.uniform(rmin, rmax, (NA, 1))\n    x1A = rxy * np.cos(a)\n    x2A = rxy * np.sin(a)\n    dA = np.ones((NA, 1))\n    pontosA = np.hstack((x1A, x2A, dA))\n\n    # Pontos da Região B\n    a = np.pi * np.random.rand(NB, 1)\n    rxy = np.random.uniform(rmin, rmax, (NB, 1))\n    x1B = rxy * np.cos(a) + r1\n    x2B = -rxy * np.sin(a) - r2\n    dB = -np.ones((NB, 1))    \n    pontosB = np.hstack((x1B, x2B, dB))\n\n    # Concatenando e embaralhando os dados\n    dados = np.vstack((pontosA, pontosB))\n    np.random.shuffle(dados)\n\n    # Figura para mostrar os dados de treino\n    fig, ax1 = plt.subplots()\n    ax1.plot(x1A, x2A, \".b\")\n    ax1.plot(x1B, x2B, \".r\")\n    plt.xlabel(\"x_1\")\n    plt.ylabel(\"x_2\")\n    plt.grid(axis=\"x\", color=\"0.5\")\n    plt.grid(axis=\"y\", color=\"0.5\")\n\n    return dados\n\nEm seguida, criando os dados para treinamento do modelo:\n\n# Gerando dados de treinamento\n# Note o uso do sufixo `_np` para facilitar a identificação\n# de arrays do NumPy e não confundi-los com tensores do PyTorch\n\n# número de pontos de treinamento da Região A\nNA = 500\n\n# número de pontos de treinamento da Região B\nNB = 500\n\n# número total de dados de treinamento\nNt = NA + NB\n\nr1 = 10\nr3 = 6\nr2 = -4\n\ndados_treino_np = meias_luas(NA, NB, r1, r2, r3)\n\n\n\n\n\n\n\n\nAjustando os valores dos hiperparâmetros:\n\n# Ajuste de hiperparâmetros\n\n# passo de adaptação da rede MLP\neta = 0.5\n\n# Tamanho do mini-batch\nNb = 100\n\n# Número de épocas\nNe = 10000\n\nO PyTorch utiliza um elemento chamado de DataLoader para facilitar o carregamento dos dados, embaralhamento e geração dos mini batches. Ele é criado a partir de um iterador, que contém pares de dados no formato (entrada, saída):\n\ndados_treino = torch.tensor(dados_treino_np, dtype=torch.float32)\ntrain_set = [\n    (dados_treino[i, [0, 1]], dados_treino[i, [2]])\n    for i in range(dados_treino.shape[0])\n]\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)\n\nVale notar alguns detalhes do código anterior:\n\nOs dados de treinamento são armazenados em um tensor chamado dados_treino, criado a partir do array NumPy chamado dados_treino_np. Por padrão, o PyTorch trabalha com precisão de 32 bits e o NumPy, com precisão de 64 bits. Dessa forma, pensando em trabalhar na precisão numérica padrão do PyTorch, é necessário especificar dtype=torch.float32 para criar um tensor com precisão de 32 bits a partir de um array com precisão de 64 bits;\nNesse exemplo, o iterador usado para criar o DataLoader é chamado de train_set;\nAo criar o DataLoader, é fornecido o tamanho do mini batch por meio do argumento batch_size e, nesse caso, o DataLoader será responsável por embaralhar os dados a cada época, de acordo com o argumento shuffle, configurado como True;\nCom essa implementação, o DataLoader fornece dois tensores a cada iteração:\n\n\nexample_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)\n\n\nX, d = next(iter(example_loader))\n\n\nX.shape\n\ntorch.Size([100, 2])\n\n\n\nd.shape\n\ntorch.Size([100, 1])\n\n\nNote que o número de linhas dos tensores corresponde ao número de elementos do mini batch. Por padrão, os modelos do PyTorch esperam tensores de entrada neste formato.\nO modelo é definido por meio de uma classe que herda de nn.Module:\n\nclass Model(nn.Module):\n\n    # Geralmente, os blocos da rede são definidos no método __init__()\n    def __init__(self):\n        # Necessário chamar __init__() da classe mãe\n        super().__init__()\n        \n        # Uma das formas de se definir um modelo é a sequencial\n        self.model = nn.Sequential(\n            # Entrada com 2 elementos, conectada a 3 neurônios\n            nn.Linear(2, 3),\n            # Função de ativação Tanh\n            nn.Tanh(),\n            \n            # Saídas de 3 neurônios conectadas a 5 neurônios\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 2),\n            nn.Tanh(),\n            \n            nn.Linear(2, 1),\n            nn.Tanh(),            \n        )\n\n    # O método forward() define como é feito o cálculo progressivo\n    # para obter a saída da rede, a partir da entrada x.\n    # Nesse caso, como foi definido um modelo sequencial em\n    # self.model, basta chamar self.model(x)\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\nUma das vantagens de se usar o PyTorch para desenvolver aplicações de aprendizado de máquina é a possibilidade do uso de GPUs para acelerar o processo computação de forma simples\nGeralmente é criado um objeto chamado device que aponta para a GPU, caso ela exista ou para a CPU, caso contrário, como mostrado a seguir:\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n\nCom esse objeto criado, basta chamar o método .to(device=device) de tensores ou modelos, para enviá-los à GPU, caso ela exista.\nTendo a classe do modelo definida, é necessário instanciar um objeto para representá-lo. Na linha a seguir, o modelo é instanciado e enviado à GPU com o método .to(), caso ela exista:\n\nmodel = Model().to(device=device)\n\nDefinindo a função custo e o otimizador:\n\nloss_function = nn.MSELoss()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\n\nNesse caso, utiliza-se a função custo do erro quadrático médio MSELoss e o otimizador baseado no gradiente descendente estocástico SGD. O PyTorch conta com uma série de outras fuções custo e otimizadores que podem ser utilizados. Para referência, consulte:\n\nFunções custo: https://pytorch.org/docs/stable/nn.html#loss-functions\nOtimizadores: https://pytorch.org/docs/stable/optim.html#algorithms\n\nUm diferencial do PyTorch é que o treinamento deve ser feito explicitamente, com um loop para as épocas e outro para os mini batches:\n\n# Lista usada para guardar o valor da função custo ao longo das iterações\nlosses = []\n\n# Loop das épocas\nfor epoch in range(Ne):\n    # Loop dos mini batches - note que é usado o DataLoader para obter\n    # os sinais de entrada e desejado, X e d\n    for n, (X, d) in enumerate(train_loader):\n\n        # Envia os dados para a GPU, caso ela exista\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        # Coloca o modelo em modo treinamento. Isso não é necessário nesse\n        # caso, pois não estamos fazendo validação. Mas é interessante manter\n        # a linha para lembrar desse detalhe\n        model.train()\n\n        # Zera informações de gradientes: por padrão o PyTorch acumula os \n        # gradientes a cada chamada de loss.backward(). Na maioria dos casos,\n        # estamos interessados apenas no último valor dos gradientes\n        model.zero_grad()\n        \n        # Calcula a saída\n        y = model(X)\n\n        # Calcula o valor da função custo\n        loss = loss_function(y, d)\n        \n        # Calcula os gradientes\n        loss.backward()\n        \n        # Atualiza os pesos do modelo, de acordo com as regras\n        # do otimizador escolhido\n        optimizer.step()\n        \n        # Armazena o valor da função custo\n        losses.append(loss.item())\n        \n        # Mostra o valor da função custo a cada 500 épocas        \n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Época: {epoch} Loss: {loss}\")\n\nplt.figure()\nplt.plot(losses)\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\n\nÉpoca: 0 Loss: 0.5397309064865112\nÉpoca: 500 Loss: 4.2221665353281423e-05\nÉpoca: 1000 Loss: 9.045926162798423e-06\nÉpoca: 1500 Loss: 5.263632374408189e-06\nÉpoca: 2000 Loss: 3.6803758121095598e-06\nÉpoca: 2500 Loss: 2.8507993192761205e-06\nÉpoca: 3000 Loss: 2.273268592034583e-06\nÉpoca: 3500 Loss: 1.953314040292753e-06\nÉpoca: 4000 Loss: 1.7074105471692747e-06\nÉpoca: 4500 Loss: 1.4421326568481163e-06\nÉpoca: 5000 Loss: 1.320619730904582e-06\nÉpoca: 5500 Loss: 1.2237804867254454e-06\nÉpoca: 6000 Loss: 1.0987090490743867e-06\nÉpoca: 6500 Loss: 1.0110974244526005e-06\nÉpoca: 7000 Loss: 9.265424409932166e-07\nÉpoca: 7500 Loss: 8.716854722479184e-07\nÉpoca: 8000 Loss: 7.869209639466135e-07\nÉpoca: 8500 Loss: 8.086310003818653e-07\nÉpoca: 9000 Loss: 7.067936280691356e-07\nÉpoca: 9500 Loss: 6.752062517989543e-07\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nPara testar o modelo, geramos dados de teste:\n\n# Dados de teste\nNAt = 1000\nNBt = 1000\nNteste = NAt + NBt\n\ndados_teste = meias_luas(NAt, NBt, r1, r2, r3)\n\n\n\n\n\n\n\n\nConvertemos os arrays do NumPy para tensores do PyTorch e enviamos os dados para a GPU, caso ela exista:\n\nxteste = torch.tensor(dados_teste[:,[0,1]], dtype=torch.float32).to(device=device)\ndteste = torch.tensor(dados_teste[:,[2]], dtype=torch.float32).to(device=device)\n\nCalculamos a saída do modelo considerando os dados de teste como entrada e convertemos a saída para um array do NumPy:\n\nyteste = model(xteste)\nyteste_np = yteste.cpu().detach().numpy()\n\nNote que, para obter o array do NumPy, é necessário:\n\nChamar o método .cpu() para trazer de volta os dados da GPU, caso ela exista;\nChamar o método .detach() para tirar o tensor do grafo computacional. Isso é necessário para que não sejam calculados os gradientes referentes às operações que eventualmente sejam feitas com yteste. Na prática, quase sempre que seja necessário converter um tensor PyTorch para um array NumPy, será necessário chamar o método .detach() antes;\nChamar o método .numpy() para converter os dados para um array do NumPy.\n\nTambém é possível plotar a fronteira de separação, de forma semelhante à utilizada anteriormente. A diferença é a necessidade da conversão dos dados para tensores do PyTorch para utilizar o modelo e a conversão de volta para arrays do NumPy para plotar o gráfico com o Matplotlib:\n\n# Gera a curva de separação das duas regiões\n# Dados da curva de separação\nNsep = 100\nx1S = np.linspace(-15, 25, Nsep).reshape(-1, 1)\nx2S = np.linspace(-10, 15, Nsep).reshape(-1, 1)\n\n# Gera pontos da grade\nxx1S, xx2S = np.meshgrid(x1S, x2S)\nxx1S = xx1S.reshape(-1, 1)\nxx2S = xx2S.reshape(-1, 1)\n\n# Gera array x\nNgrid = len(xx1S)\nxgrid_np = np.hstack((xx1S, xx2S))\n\n# Calcula saída para cada ponto da grade\nxgrid = torch.tensor(xgrid_np, dtype=torch.float32).to(device=device)\nygrid = model(xgrid)\nygrid_dec = torch.sign(ygrid)\n\nygrid_np = ygrid.cpu().detach().numpy()\nygrid_dec_np = ygrid_dec.cpu().detach().numpy()\n\nxteste_np = xteste.cpu().detach().numpy()\ndteste_np = dteste.cpu().detach().numpy()\n\n# Plota os pontos principais\nfig, ax2 = plt.subplots()\nfor i in range(Nteste):\n    if dteste_np[i] == 1:\n        ax2.plot(xteste_np[i, 0], xteste_np[i, 1], \".b\")\n    else:\n        ax2.plot(xteste_np[i, 0], xteste_np[i, 1], \".r\")\n\n# Plota pontos da grade com saída 0 (usa transparência alpha)        \nl0 = np.where(ygrid_dec_np == -1)[0]\nax2.plot(xgrid_np[l0, 0], xgrid_np[l0, 1], \"r.\", alpha=0.1)\n\n# Plota pontos da grade com saída 1 (usa transparência alpha)\nl1 = np.where(ygrid_dec_np == 1)[0]\nax2.plot(xgrid_np[l1, 0], xgrid_np[l1, 1], \"b.\", alpha=0.1)\n\n\n\n\n\n\n\n\nCalculando a taxa de erros:\n\nyteste_np_dec = np.sign(yteste_np)\n\nTaxa_de_erro = np.sum(np.absolute(dteste_np - yteste_np_dec)) * 100 / (2 * Nteste)\n\nprint(f\"Taxa de erro: {Taxa_de_erro}\")\n\nTaxa de erro: 0.05000000074505806\n\n\n\n\n\nÉ interessante usar um conjunto de dados de validação durante o treinamento para observar se não está ocorrendo overfitting do modelo:\n\nNAv = 50\nNBv = 50\ndados_val = meias_luas(NAv, NBv, r1, r2, r3)\n\nX_val = torch.tensor(dados_val[:,[0,1]], dtype=torch.float32).to(device=device)\nd_val = torch.tensor(dados_val[:,[2]], dtype=torch.float32).to(device=device)\n\n\n\n\n\n\n\n\nSegue um exemplo da rotina de treinamento considerando a etapa de validação. As diferenças em relação à rotina mostrado anteriormente estão destacadas. Vale notar alguns detalhes:\n\nA necessidade de colocar o modelo em modo treinamento (train) para atualizar os pesos e inferência (eval) para calcular o valor da função custo de validação;\nPara o cálculo da saída e do valor da função custo, não é necessário calcular gradientes.\n\n\n# Reiniciando o modelo e otimizador\nmodel = Model().to(device=device)\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\n\n# Listas para guardar o valor da função custo\n# no treinamento e validação ao longo das iterações\nlosses = []\nval_losses = []\n\nfor epoch in range(Ne):\n    for n, (X, d) in enumerate(train_loader):\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        # Necessário colocar o modelo em modo treinamento\n        # na etapa de treinamento\n        model.train()\n        model.zero_grad()\n        y = model(X)\n        loss = loss_function(y, d)\n        loss.backward()\n        optimizer.step()\n\n        # Validação\n        # Necessário colocar o modelo em modo de inferência (eval)\n        # pois algumas camadas têm comportamento diferente para inferência,\n        # por exemplo, o Dropout.\n        model.eval()\n\n        # Cálculo da saída e valor da função custo com os dados de validação\n        # Nesse caso, não é necessário calcular gradientes, por isso é utilizado\n        # o bloco with torch.no_grad():\n        with torch.no_grad():\n            y_val = model(X_val)\n            val_loss = loss_function(y_val, d_val)\n\n        # Armazena o valor da função custo de treinamento e validação\n        losses.append(loss.item())\n        val_losses.append(val_loss.item())\n        \n        # Mostra os valores da função custo de treinamento e validação\n        # a cada 500 épocas        \n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Epoch: {epoch} Loss: {loss} Val. Loss: {val_loss}\")\n\nplt.figure()\nplt.plot(losses)\nplt.plot(val_losses, alpha=0.8)\nplt.legend([\"Loss\", \"Val. Loss\"])\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\n\nEpoch: 0 Loss: 0.5027859210968018 Val. Loss: 0.4702392518520355\nEpoch: 500 Loss: 0.014364180155098438 Val. Loss: 0.00017858465434983373\nEpoch: 1000 Loss: 9.866827895166352e-06 Val. Loss: 9.640273674449418e-06\nEpoch: 1500 Loss: 5.4211923270486295e-06 Val. Loss: 5.332761247700546e-06\nEpoch: 2000 Loss: 3.7570675885945093e-06 Val. Loss: 3.6991368688177317e-06\nEpoch: 2500 Loss: 2.902035930674174e-06 Val. Loss: 2.8353019843052607e-06\nEpoch: 3000 Loss: 2.484635842847638e-06 Val. Loss: 2.299876314282301e-06\nEpoch: 3500 Loss: 1.918974021464237e-06 Val. Loss: 1.9352103208802873e-06\nEpoch: 4000 Loss: 1.6821684312162688e-06 Val. Loss: 1.670778601692291e-06\nEpoch: 4500 Loss: 1.5319814110625884e-06 Val. Loss: 1.4700692645419622e-06\nEpoch: 5000 Loss: 1.2996845271118218e-06 Val. Loss: 1.312647668783029e-06\nEpoch: 5500 Loss: 1.260786916645884e-06 Val. Loss: 1.1858140851472854e-06\nEpoch: 6000 Loss: 1.1408079672037275e-06 Val. Loss: 1.081413188330771e-06\nEpoch: 6500 Loss: 1.0122498679265846e-06 Val. Loss: 9.93700609797088e-07\nEpoch: 7000 Loss: 9.345478133582219e-07 Val. Loss: 9.192294214699359e-07\nEpoch: 7500 Loss: 8.631181458440551e-07 Val. Loss: 8.556194188713562e-07\nEpoch: 8000 Loss: 8.076505082499352e-07 Val. Loss: 7.99747624569136e-07\nEpoch: 8500 Loss: 7.589032406940532e-07 Val. Loss: 7.511117701142211e-07\nEpoch: 9000 Loss: 7.423016654684034e-07 Val. Loss: 7.082934985191969e-07\nEpoch: 9500 Loss: 6.734330213475914e-07 Val. Loss: 6.692786769235681e-07\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\n\n\n\nO PyTorch disponibiliza uma lista grande de funções de ativação que podem ser utilizadas. Para referência, acesse https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity.\nPara usar ReLU, por exemplo, utiliza-se a função nn.ReLU:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.ReLU(),\n            nn.Linear(3, 5),\n            nn.ReLU(),\n            nn.Linear(5, 2),\n            nn.ReLU(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\nOutra função muito usada é a sigmoide, nos casos em que é necessário limitar uma saída entre 0 e 1. A implementação é feita com nn.Sigmoid():\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Sigmoid(),\n            nn.Linear(3, 5),\n            nn.Sigmoid(),\n            nn.Linear(5, 2),\n            nn.Sigmoid(),\n            nn.Linear(2, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n\n\n\nPara utilizar Dropout, basta adicionar camadas do tipo nn.Dropout(n), em que n representa a proporção de neurônios que deve ser desativada. No exemplo a seguir é utilizado Dropout de 10% para os neurônios da primeira camada:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Tanh(),\n            nn.Dropout(0.1),\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            nn.Linear(5, 2),\n            nn.Tanh(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n\n\n\nPara utilizar o Adam, basta criar o otimizador usando torch.optim.Adam:\n\noptimizer = torch.optim.Adam(model.parameters(), lr=eta, betas=(0.9, 0.999))\n\n\n\n\nA função custo da entropia cruzada binária é implementada pela classe torch.nn.BCELoss. Para utilizá-la, basta definir:\n\nloss_function = nn.BCELoss()\n\nNo caso de classificação multiclasse, pode-se utilizar a classe torch.nn.CrossEntropyLoss. Vale notar que essa função custo espera comparar um vetor de \\(C\\) posições com um número de \\(0\\) a \\(C-1\\). Além disso, é esperado que os elementos do vetor representem a evidência, ou seja os valores chamados de logits, que não são normalizados e podem valer de \\(-\\infty\\) a \\(\\infty\\). Por isso, na saída da rede, não deve ser usada a função Softmax.\n\n\n\nUma forma de inicializar os pesos da rede é definir uma função para inicializar os parâmetros de um elemento do modelo e usar o método .apply() para aplicar essa função à todos os elementos do modelo:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Tanh(),\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            nn.Linear(5, 2),\n            nn.Tanh(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n\nmodel = Model().to(device=device)    \n    \ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        torch.nn.init.xavier_normal_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n\nmodel.apply(weights_init)\n\nModel(\n  (model): Sequential(\n    (0): Linear(in_features=2, out_features=3, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=3, out_features=5, bias=True)\n    (3): Tanh()\n    (4): Linear(in_features=5, out_features=2, bias=True)\n    (5): Tanh()\n    (6): Linear(in_features=2, out_features=1, bias=True)\n    (7): Tanh()\n  )\n)\n\n\nNesse exemplo, é usada a inicialização de Xavier, com distribuição Gaussiana (torch.nn.init.xavier_normal_). Além dela, há diversas outras alternativas, listadas em https://pytorch.org/docs/stable/nn.init.html.\n\n\n\nO Pytorch oferece algumas classes que permitem o uso de passo variável de acordo com diferentes regras. Essas classes são agupadas em torch.optim.lr_scheduler. Seguem alguns exemplos:\n\ntorch.optim.lr_scheduler.StepLR\ntorch.optim.lr_scheduler.ExponentialLR\n\nPara implementar, é necessário criar um objeto que controla o ajuste do passo variável, geralmente chamado de scheduler, indicando o otimizador escolhido. Além disso, no loop de treinamento, deve-se chamar o método .step() do objeto scheduler, após chamar o do otimizador. Segue um exemplo:\n# (...)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\nlosses = []\n\n# Loop das épocas\n\nfor epoch in range(Ne):\n    for n, (X, d) in enumerate(train_loader):\n\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        model.train()\n        model.zero_grad()\n        y = model(X)\n        loss = loss_function(y, d)\n        loss.backward()\n       \n        optimizer.step()\n        scheduler.step()\n        \n        losses.append(loss.item())\n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Época: {epoch} Loss: {loss}\")\n\n\n\nO Pytorch disponibiliza a classe BatchNorm1d para implementar uma camada de normalização em lote em redes MLP. Geralmente as camadas de normalização em lote são inseridas depois de uma camada Linear e antes da função de ativação. Quando utilizada a normalização em lote, é uma prática comum desabilitar o bias da camada Linear anterior tendo em vista que a camada de normalização em lote adapta um parâmetro que funciona como bias.\nSegue um exemplo:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3, bias=False),\n            nn.BatchNorm1d(3),\n            nn.Tanh(),\n            \n            nn.Linear(3, 5, bias=False),\n            nn.BatchNorm1d(5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 5, bias=False),\n            nn.BatchNorm1d(5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 2, bias=False),\n            nn.BatchNorm1d(2),\n            nn.Tanh(),\n            \n            nn.Linear(2, 1),            \n            nn.Tanh(),            \n        )"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#treinamento-sem-a-etapa-de-validação",
    "href": "t_pytorch_exemplo_mlp.html#treinamento-sem-a-etapa-de-validação",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "Iniciando com a importação das bibliotecas:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\n# Fixando seeds para poder reproduzir os resultados\nnp.random.seed(111)\ntorch.manual_seed(111)\ntorch.cuda.manual_seed(111)\n\nDefinindo a função para gerar dados para treinamento:\n\n# Função para garar dados de treinamento\ndef meias_luas(NA, NB, r1, r2, r3):\n    \"\"\"\n    dados = meias_luas(NA,NB,r1,r2,r3)\n    NA: número de pontos da região A\n    NB: número de pontos da região B\n    r1, r2 e r3: dados das meias-luas\n    \"\"\"\n\n    # total de dados de treinamento\n    Nt = NA + NB\n\n    # dados das meia luas\n    rmin = r1 - r3 / 2\n    rmax = r1 + r3 / 2\n\n    # Pontos da Região A\n    a = np.pi * np.random.rand(NA, 1)\n    rxy = np.random.uniform(rmin, rmax, (NA, 1))\n    x1A = rxy * np.cos(a)\n    x2A = rxy * np.sin(a)\n    dA = np.ones((NA, 1))\n    pontosA = np.hstack((x1A, x2A, dA))\n\n    # Pontos da Região B\n    a = np.pi * np.random.rand(NB, 1)\n    rxy = np.random.uniform(rmin, rmax, (NB, 1))\n    x1B = rxy * np.cos(a) + r1\n    x2B = -rxy * np.sin(a) - r2\n    dB = -np.ones((NB, 1))    \n    pontosB = np.hstack((x1B, x2B, dB))\n\n    # Concatenando e embaralhando os dados\n    dados = np.vstack((pontosA, pontosB))\n    np.random.shuffle(dados)\n\n    # Figura para mostrar os dados de treino\n    fig, ax1 = plt.subplots()\n    ax1.plot(x1A, x2A, \".b\")\n    ax1.plot(x1B, x2B, \".r\")\n    plt.xlabel(\"x_1\")\n    plt.ylabel(\"x_2\")\n    plt.grid(axis=\"x\", color=\"0.5\")\n    plt.grid(axis=\"y\", color=\"0.5\")\n\n    return dados\n\nEm seguida, criando os dados para treinamento do modelo:\n\n# Gerando dados de treinamento\n# Note o uso do sufixo `_np` para facilitar a identificação\n# de arrays do NumPy e não confundi-los com tensores do PyTorch\n\n# número de pontos de treinamento da Região A\nNA = 500\n\n# número de pontos de treinamento da Região B\nNB = 500\n\n# número total de dados de treinamento\nNt = NA + NB\n\nr1 = 10\nr3 = 6\nr2 = -4\n\ndados_treino_np = meias_luas(NA, NB, r1, r2, r3)\n\n\n\n\n\n\n\n\nAjustando os valores dos hiperparâmetros:\n\n# Ajuste de hiperparâmetros\n\n# passo de adaptação da rede MLP\neta = 0.5\n\n# Tamanho do mini-batch\nNb = 100\n\n# Número de épocas\nNe = 10000\n\nO PyTorch utiliza um elemento chamado de DataLoader para facilitar o carregamento dos dados, embaralhamento e geração dos mini batches. Ele é criado a partir de um iterador, que contém pares de dados no formato (entrada, saída):\n\ndados_treino = torch.tensor(dados_treino_np, dtype=torch.float32)\ntrain_set = [\n    (dados_treino[i, [0, 1]], dados_treino[i, [2]])\n    for i in range(dados_treino.shape[0])\n]\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)\n\nVale notar alguns detalhes do código anterior:\n\nOs dados de treinamento são armazenados em um tensor chamado dados_treino, criado a partir do array NumPy chamado dados_treino_np. Por padrão, o PyTorch trabalha com precisão de 32 bits e o NumPy, com precisão de 64 bits. Dessa forma, pensando em trabalhar na precisão numérica padrão do PyTorch, é necessário especificar dtype=torch.float32 para criar um tensor com precisão de 32 bits a partir de um array com precisão de 64 bits;\nNesse exemplo, o iterador usado para criar o DataLoader é chamado de train_set;\nAo criar o DataLoader, é fornecido o tamanho do mini batch por meio do argumento batch_size e, nesse caso, o DataLoader será responsável por embaralhar os dados a cada época, de acordo com o argumento shuffle, configurado como True;\nCom essa implementação, o DataLoader fornece dois tensores a cada iteração:\n\n\nexample_loader = torch.utils.data.DataLoader(train_set, batch_size=Nb, shuffle=True)\n\n\nX, d = next(iter(example_loader))\n\n\nX.shape\n\ntorch.Size([100, 2])\n\n\n\nd.shape\n\ntorch.Size([100, 1])\n\n\nNote que o número de linhas dos tensores corresponde ao número de elementos do mini batch. Por padrão, os modelos do PyTorch esperam tensores de entrada neste formato.\nO modelo é definido por meio de uma classe que herda de nn.Module:\n\nclass Model(nn.Module):\n\n    # Geralmente, os blocos da rede são definidos no método __init__()\n    def __init__(self):\n        # Necessário chamar __init__() da classe mãe\n        super().__init__()\n        \n        # Uma das formas de se definir um modelo é a sequencial\n        self.model = nn.Sequential(\n            # Entrada com 2 elementos, conectada a 3 neurônios\n            nn.Linear(2, 3),\n            # Função de ativação Tanh\n            nn.Tanh(),\n            \n            # Saídas de 3 neurônios conectadas a 5 neurônios\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 2),\n            nn.Tanh(),\n            \n            nn.Linear(2, 1),\n            nn.Tanh(),            \n        )\n\n    # O método forward() define como é feito o cálculo progressivo\n    # para obter a saída da rede, a partir da entrada x.\n    # Nesse caso, como foi definido um modelo sequencial em\n    # self.model, basta chamar self.model(x)\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\nUma das vantagens de se usar o PyTorch para desenvolver aplicações de aprendizado de máquina é a possibilidade do uso de GPUs para acelerar o processo computação de forma simples\nGeralmente é criado um objeto chamado device que aponta para a GPU, caso ela exista ou para a CPU, caso contrário, como mostrado a seguir:\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n\nCom esse objeto criado, basta chamar o método .to(device=device) de tensores ou modelos, para enviá-los à GPU, caso ela exista.\nTendo a classe do modelo definida, é necessário instanciar um objeto para representá-lo. Na linha a seguir, o modelo é instanciado e enviado à GPU com o método .to(), caso ela exista:\n\nmodel = Model().to(device=device)\n\nDefinindo a função custo e o otimizador:\n\nloss_function = nn.MSELoss()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\n\nNesse caso, utiliza-se a função custo do erro quadrático médio MSELoss e o otimizador baseado no gradiente descendente estocástico SGD. O PyTorch conta com uma série de outras fuções custo e otimizadores que podem ser utilizados. Para referência, consulte:\n\nFunções custo: https://pytorch.org/docs/stable/nn.html#loss-functions\nOtimizadores: https://pytorch.org/docs/stable/optim.html#algorithms\n\nUm diferencial do PyTorch é que o treinamento deve ser feito explicitamente, com um loop para as épocas e outro para os mini batches:\n\n# Lista usada para guardar o valor da função custo ao longo das iterações\nlosses = []\n\n# Loop das épocas\nfor epoch in range(Ne):\n    # Loop dos mini batches - note que é usado o DataLoader para obter\n    # os sinais de entrada e desejado, X e d\n    for n, (X, d) in enumerate(train_loader):\n\n        # Envia os dados para a GPU, caso ela exista\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        # Coloca o modelo em modo treinamento. Isso não é necessário nesse\n        # caso, pois não estamos fazendo validação. Mas é interessante manter\n        # a linha para lembrar desse detalhe\n        model.train()\n\n        # Zera informações de gradientes: por padrão o PyTorch acumula os \n        # gradientes a cada chamada de loss.backward(). Na maioria dos casos,\n        # estamos interessados apenas no último valor dos gradientes\n        model.zero_grad()\n        \n        # Calcula a saída\n        y = model(X)\n\n        # Calcula o valor da função custo\n        loss = loss_function(y, d)\n        \n        # Calcula os gradientes\n        loss.backward()\n        \n        # Atualiza os pesos do modelo, de acordo com as regras\n        # do otimizador escolhido\n        optimizer.step()\n        \n        # Armazena o valor da função custo\n        losses.append(loss.item())\n        \n        # Mostra o valor da função custo a cada 500 épocas        \n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Época: {epoch} Loss: {loss}\")\n\nplt.figure()\nplt.plot(losses)\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\n\nÉpoca: 0 Loss: 0.5397309064865112\nÉpoca: 500 Loss: 4.2221665353281423e-05\nÉpoca: 1000 Loss: 9.045926162798423e-06\nÉpoca: 1500 Loss: 5.263632374408189e-06\nÉpoca: 2000 Loss: 3.6803758121095598e-06\nÉpoca: 2500 Loss: 2.8507993192761205e-06\nÉpoca: 3000 Loss: 2.273268592034583e-06\nÉpoca: 3500 Loss: 1.953314040292753e-06\nÉpoca: 4000 Loss: 1.7074105471692747e-06\nÉpoca: 4500 Loss: 1.4421326568481163e-06\nÉpoca: 5000 Loss: 1.320619730904582e-06\nÉpoca: 5500 Loss: 1.2237804867254454e-06\nÉpoca: 6000 Loss: 1.0987090490743867e-06\nÉpoca: 6500 Loss: 1.0110974244526005e-06\nÉpoca: 7000 Loss: 9.265424409932166e-07\nÉpoca: 7500 Loss: 8.716854722479184e-07\nÉpoca: 8000 Loss: 7.869209639466135e-07\nÉpoca: 8500 Loss: 8.086310003818653e-07\nÉpoca: 9000 Loss: 7.067936280691356e-07\nÉpoca: 9500 Loss: 6.752062517989543e-07\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nPara testar o modelo, geramos dados de teste:\n\n# Dados de teste\nNAt = 1000\nNBt = 1000\nNteste = NAt + NBt\n\ndados_teste = meias_luas(NAt, NBt, r1, r2, r3)\n\n\n\n\n\n\n\n\nConvertemos os arrays do NumPy para tensores do PyTorch e enviamos os dados para a GPU, caso ela exista:\n\nxteste = torch.tensor(dados_teste[:,[0,1]], dtype=torch.float32).to(device=device)\ndteste = torch.tensor(dados_teste[:,[2]], dtype=torch.float32).to(device=device)\n\nCalculamos a saída do modelo considerando os dados de teste como entrada e convertemos a saída para um array do NumPy:\n\nyteste = model(xteste)\nyteste_np = yteste.cpu().detach().numpy()\n\nNote que, para obter o array do NumPy, é necessário:\n\nChamar o método .cpu() para trazer de volta os dados da GPU, caso ela exista;\nChamar o método .detach() para tirar o tensor do grafo computacional. Isso é necessário para que não sejam calculados os gradientes referentes às operações que eventualmente sejam feitas com yteste. Na prática, quase sempre que seja necessário converter um tensor PyTorch para um array NumPy, será necessário chamar o método .detach() antes;\nChamar o método .numpy() para converter os dados para um array do NumPy.\n\nTambém é possível plotar a fronteira de separação, de forma semelhante à utilizada anteriormente. A diferença é a necessidade da conversão dos dados para tensores do PyTorch para utilizar o modelo e a conversão de volta para arrays do NumPy para plotar o gráfico com o Matplotlib:\n\n# Gera a curva de separação das duas regiões\n# Dados da curva de separação\nNsep = 100\nx1S = np.linspace(-15, 25, Nsep).reshape(-1, 1)\nx2S = np.linspace(-10, 15, Nsep).reshape(-1, 1)\n\n# Gera pontos da grade\nxx1S, xx2S = np.meshgrid(x1S, x2S)\nxx1S = xx1S.reshape(-1, 1)\nxx2S = xx2S.reshape(-1, 1)\n\n# Gera array x\nNgrid = len(xx1S)\nxgrid_np = np.hstack((xx1S, xx2S))\n\n# Calcula saída para cada ponto da grade\nxgrid = torch.tensor(xgrid_np, dtype=torch.float32).to(device=device)\nygrid = model(xgrid)\nygrid_dec = torch.sign(ygrid)\n\nygrid_np = ygrid.cpu().detach().numpy()\nygrid_dec_np = ygrid_dec.cpu().detach().numpy()\n\nxteste_np = xteste.cpu().detach().numpy()\ndteste_np = dteste.cpu().detach().numpy()\n\n# Plota os pontos principais\nfig, ax2 = plt.subplots()\nfor i in range(Nteste):\n    if dteste_np[i] == 1:\n        ax2.plot(xteste_np[i, 0], xteste_np[i, 1], \".b\")\n    else:\n        ax2.plot(xteste_np[i, 0], xteste_np[i, 1], \".r\")\n\n# Plota pontos da grade com saída 0 (usa transparência alpha)        \nl0 = np.where(ygrid_dec_np == -1)[0]\nax2.plot(xgrid_np[l0, 0], xgrid_np[l0, 1], \"r.\", alpha=0.1)\n\n# Plota pontos da grade com saída 1 (usa transparência alpha)\nl1 = np.where(ygrid_dec_np == 1)[0]\nax2.plot(xgrid_np[l1, 0], xgrid_np[l1, 1], \"b.\", alpha=0.1)\n\n\n\n\n\n\n\n\nCalculando a taxa de erros:\n\nyteste_np_dec = np.sign(yteste_np)\n\nTaxa_de_erro = np.sum(np.absolute(dteste_np - yteste_np_dec)) * 100 / (2 * Nteste)\n\nprint(f\"Taxa de erro: {Taxa_de_erro}\")\n\nTaxa de erro: 0.05000000074505806"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#validação-cruzada-hold-out",
    "href": "t_pytorch_exemplo_mlp.html#validação-cruzada-hold-out",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "É interessante usar um conjunto de dados de validação durante o treinamento para observar se não está ocorrendo overfitting do modelo:\n\nNAv = 50\nNBv = 50\ndados_val = meias_luas(NAv, NBv, r1, r2, r3)\n\nX_val = torch.tensor(dados_val[:,[0,1]], dtype=torch.float32).to(device=device)\nd_val = torch.tensor(dados_val[:,[2]], dtype=torch.float32).to(device=device)\n\n\n\n\n\n\n\n\nSegue um exemplo da rotina de treinamento considerando a etapa de validação. As diferenças em relação à rotina mostrado anteriormente estão destacadas. Vale notar alguns detalhes:\n\nA necessidade de colocar o modelo em modo treinamento (train) para atualizar os pesos e inferência (eval) para calcular o valor da função custo de validação;\nPara o cálculo da saída e do valor da função custo, não é necessário calcular gradientes.\n\n\n# Reiniciando o modelo e otimizador\nmodel = Model().to(device=device)\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\n\n# Listas para guardar o valor da função custo\n# no treinamento e validação ao longo das iterações\nlosses = []\nval_losses = []\n\nfor epoch in range(Ne):\n    for n, (X, d) in enumerate(train_loader):\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        # Necessário colocar o modelo em modo treinamento\n        # na etapa de treinamento\n        model.train()\n        model.zero_grad()\n        y = model(X)\n        loss = loss_function(y, d)\n        loss.backward()\n        optimizer.step()\n\n        # Validação\n        # Necessário colocar o modelo em modo de inferência (eval)\n        # pois algumas camadas têm comportamento diferente para inferência,\n        # por exemplo, o Dropout.\n        model.eval()\n\n        # Cálculo da saída e valor da função custo com os dados de validação\n        # Nesse caso, não é necessário calcular gradientes, por isso é utilizado\n        # o bloco with torch.no_grad():\n        with torch.no_grad():\n            y_val = model(X_val)\n            val_loss = loss_function(y_val, d_val)\n\n        # Armazena o valor da função custo de treinamento e validação\n        losses.append(loss.item())\n        val_losses.append(val_loss.item())\n        \n        # Mostra os valores da função custo de treinamento e validação\n        # a cada 500 épocas        \n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Epoch: {epoch} Loss: {loss} Val. Loss: {val_loss}\")\n\nplt.figure()\nplt.plot(losses)\nplt.plot(val_losses, alpha=0.8)\nplt.legend([\"Loss\", \"Val. Loss\"])\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\n\nEpoch: 0 Loss: 0.5027859210968018 Val. Loss: 0.4702392518520355\nEpoch: 500 Loss: 0.014364180155098438 Val. Loss: 0.00017858465434983373\nEpoch: 1000 Loss: 9.866827895166352e-06 Val. Loss: 9.640273674449418e-06\nEpoch: 1500 Loss: 5.4211923270486295e-06 Val. Loss: 5.332761247700546e-06\nEpoch: 2000 Loss: 3.7570675885945093e-06 Val. Loss: 3.6991368688177317e-06\nEpoch: 2500 Loss: 2.902035930674174e-06 Val. Loss: 2.8353019843052607e-06\nEpoch: 3000 Loss: 2.484635842847638e-06 Val. Loss: 2.299876314282301e-06\nEpoch: 3500 Loss: 1.918974021464237e-06 Val. Loss: 1.9352103208802873e-06\nEpoch: 4000 Loss: 1.6821684312162688e-06 Val. Loss: 1.670778601692291e-06\nEpoch: 4500 Loss: 1.5319814110625884e-06 Val. Loss: 1.4700692645419622e-06\nEpoch: 5000 Loss: 1.2996845271118218e-06 Val. Loss: 1.312647668783029e-06\nEpoch: 5500 Loss: 1.260786916645884e-06 Val. Loss: 1.1858140851472854e-06\nEpoch: 6000 Loss: 1.1408079672037275e-06 Val. Loss: 1.081413188330771e-06\nEpoch: 6500 Loss: 1.0122498679265846e-06 Val. Loss: 9.93700609797088e-07\nEpoch: 7000 Loss: 9.345478133582219e-07 Val. Loss: 9.192294214699359e-07\nEpoch: 7500 Loss: 8.631181458440551e-07 Val. Loss: 8.556194188713562e-07\nEpoch: 8000 Loss: 8.076505082499352e-07 Val. Loss: 7.99747624569136e-07\nEpoch: 8500 Loss: 7.589032406940532e-07 Val. Loss: 7.511117701142211e-07\nEpoch: 9000 Loss: 7.423016654684034e-07 Val. Loss: 7.082934985191969e-07\nEpoch: 9500 Loss: 6.734330213475914e-07 Val. Loss: 6.692786769235681e-07\n\n\nText(0, 0.5, 'Loss')"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-outras-funções-de-ativação",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-outras-funções-de-ativação",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "O PyTorch disponibiliza uma lista grande de funções de ativação que podem ser utilizadas. Para referência, acesse https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity.\nPara usar ReLU, por exemplo, utiliza-se a função nn.ReLU:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.ReLU(),\n            nn.Linear(3, 5),\n            nn.ReLU(),\n            nn.Linear(5, 2),\n            nn.ReLU(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\nOutra função muito usada é a sigmoide, nos casos em que é necessário limitar uma saída entre 0 e 1. A implementação é feita com nn.Sigmoid():\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Sigmoid(),\n            nn.Linear(3, 5),\n            nn.Sigmoid(),\n            nn.Linear(5, 2),\n            nn.Sigmoid(),\n            nn.Linear(2, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-dropout",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-dropout",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "Para utilizar Dropout, basta adicionar camadas do tipo nn.Dropout(n), em que n representa a proporção de neurônios que deve ser desativada. No exemplo a seguir é utilizado Dropout de 10% para os neurônios da primeira camada:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Tanh(),\n            nn.Dropout(0.1),\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            nn.Linear(5, 2),\n            nn.Tanh(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-o-otimizador-adam",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-o-otimizador-adam",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "Para utilizar o Adam, basta criar o otimizador usando torch.optim.Adam:\n\noptimizer = torch.optim.Adam(model.parameters(), lr=eta, betas=(0.9, 0.999))"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-a-função-custo-da-entropia-cruzada",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-a-função-custo-da-entropia-cruzada",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "A função custo da entropia cruzada binária é implementada pela classe torch.nn.BCELoss. Para utilizá-la, basta definir:\n\nloss_function = nn.BCELoss()\n\nNo caso de classificação multiclasse, pode-se utilizar a classe torch.nn.CrossEntropyLoss. Vale notar que essa função custo espera comparar um vetor de \\(C\\) posições com um número de \\(0\\) a \\(C-1\\). Além disso, é esperado que os elementos do vetor representem a evidência, ou seja os valores chamados de logits, que não são normalizados e podem valer de \\(-\\infty\\) a \\(\\infty\\). Por isso, na saída da rede, não deve ser usada a função Softmax."
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#inicialização-de-pesos",
    "href": "t_pytorch_exemplo_mlp.html#inicialização-de-pesos",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "Uma forma de inicializar os pesos da rede é definir uma função para inicializar os parâmetros de um elemento do modelo e usar o método .apply() para aplicar essa função à todos os elementos do modelo:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3),\n            nn.Tanh(),\n            nn.Linear(3, 5),\n            nn.Tanh(),\n            nn.Linear(5, 2),\n            nn.Tanh(),\n            nn.Linear(2, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n\nmodel = Model().to(device=device)    \n    \ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        torch.nn.init.xavier_normal_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n\nmodel.apply(weights_init)\n\nModel(\n  (model): Sequential(\n    (0): Linear(in_features=2, out_features=3, bias=True)\n    (1): Tanh()\n    (2): Linear(in_features=3, out_features=5, bias=True)\n    (3): Tanh()\n    (4): Linear(in_features=5, out_features=2, bias=True)\n    (5): Tanh()\n    (6): Linear(in_features=2, out_features=1, bias=True)\n    (7): Tanh()\n  )\n)\n\n\nNesse exemplo, é usada a inicialização de Xavier, com distribuição Gaussiana (torch.nn.init.xavier_normal_). Além dela, há diversas outras alternativas, listadas em https://pytorch.org/docs/stable/nn.init.html."
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-passo-de-adaptação-variável-annealing",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-passo-de-adaptação-variável-annealing",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "O Pytorch oferece algumas classes que permitem o uso de passo variável de acordo com diferentes regras. Essas classes são agupadas em torch.optim.lr_scheduler. Seguem alguns exemplos:\n\ntorch.optim.lr_scheduler.StepLR\ntorch.optim.lr_scheduler.ExponentialLR\n\nPara implementar, é necessário criar um objeto que controla o ajuste do passo variável, geralmente chamado de scheduler, indicando o otimizador escolhido. Além disso, no loop de treinamento, deve-se chamar o método .step() do objeto scheduler, após chamar o do otimizador. Segue um exemplo:\n# (...)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=eta)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\nlosses = []\n\n# Loop das épocas\n\nfor epoch in range(Ne):\n    for n, (X, d) in enumerate(train_loader):\n\n        X = X.to(device=device)\n        d = d.to(device=device)\n        \n        model.train()\n        model.zero_grad()\n        y = model(X)\n        loss = loss_function(y, d)\n        loss.backward()\n       \n        optimizer.step()\n        scheduler.step()\n        \n        losses.append(loss.item())\n        if epoch % 500 == 0 and n == dados_treino.shape[0]//Nb - 1:\n            print(f\"Época: {epoch} Loss: {loss}\")"
  },
  {
    "objectID": "t_pytorch_exemplo_mlp.html#utilizando-normalização-em-lote-batch-normalization",
    "href": "t_pytorch_exemplo_mlp.html#utilizando-normalização-em-lote-batch-normalization",
    "title": "Implementação da rede MLP com PyTorch",
    "section": "",
    "text": "O Pytorch disponibiliza a classe BatchNorm1d para implementar uma camada de normalização em lote em redes MLP. Geralmente as camadas de normalização em lote são inseridas depois de uma camada Linear e antes da função de ativação. Quando utilizada a normalização em lote, é uma prática comum desabilitar o bias da camada Linear anterior tendo em vista que a camada de normalização em lote adapta um parâmetro que funciona como bias.\nSegue um exemplo:\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.model = nn.Sequential(\n            nn.Linear(2, 3, bias=False),\n            nn.BatchNorm1d(3),\n            nn.Tanh(),\n            \n            nn.Linear(3, 5, bias=False),\n            nn.BatchNorm1d(5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 5, bias=False),\n            nn.BatchNorm1d(5),\n            nn.Tanh(),\n            \n            nn.Linear(5, 2, bias=False),\n            nn.BatchNorm1d(2),\n            nn.Tanh(),\n            \n            nn.Linear(2, 1),            \n            nn.Tanh(),            \n        )"
  },
  {
    "objectID": "ex_aula_autodiff.html",
    "href": "ex_aula_autodiff.html",
    "title": "Exercício - Autodiff",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_lms_eco.html",
    "href": "ex_aula_lms_eco.html",
    "title": "Exercício - Cancelamento de eco acústico com o algoritmo NLMS",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_introducao.html",
    "href": "t_introducao.html",
    "title": "Introdução",
    "section": "",
    "text": "O Aprendizado de Máquina (Machine Learning) surgiu dentro de um campo da Ciência da Computação conhecido como Inteligência Artificial (IA). O objetivo da IA é tornar máquinas inteligentes, capazes de “pensar racionalmente como humanos” e resolver problemas. Já o aprendizado de máquina busca criar sistemas computacionais e algoritmos para possibilitar máquinas a “aprender” a partir de uma experiência prévia. Como a inteligência e o aprendizado andam juntos, o aprendizado de máquina tem um papel dominante em IA. Uma máquina aprende quando ela é capaz de acumular experiência (por meio de dados, programas, etc.) e desenvolver novo conhecimento de modo a fazer com que seu desempenho em tarefas específicas melhore com o tempo. A ideia de aprender a partir da experiência é central em vários problemas de aprendizado de máquina como os de classificação, cujo objetivo é encontrar uma forma sistemática de classificar um exemplo novo.\nAs redes neurais, também chamadas de redes neurais artificiais, fazem parte de um subconjunto de técnicas de aprendizado de máquina. Na literatura, o termo “aprendizado de máquina” é muitas vezes confundido com o termo “aprendizado profundo” (deep learning). No entanto, uma rede neural só é considerada profunda se tiver duas ou mais camadas ocultas, conceito que será abordado posteriormente. Pode-se dizer que a maioria das redes neurais usadas na prática considera atualmente o aprendizado profundo. Na Figura 1, fica claro que as redes neurais profundas são um subconjunto das redes neurais, que por sua vez são um subconjunto de técnicas de aprendizado de máquina. Todas as técnicas de aprendizado de máquina fazem parte do campo de IA.\n\n\n\n\n\n\nFigura 1: Diagrama indicando o subcampo das redes neurais profundas dentro da IA.\n\n\n\nDesde o surgimento dos computadores, os cientistas têm buscado maneiras de permitir que as máquinas produzam uma saída desejada a partir de entradas para tarefas como classificação e regressão. As redes neurais são sistemas não lineares que podem ser usados para essas tarefas. Elas surgiram na década de 1940, mas o algoritmo utilizado no seu treinamento, chamado de algoritmo de retropropagação (backpropagation), foi proposto apenas em 1986. Nas décadas de 1990 e 2000, muitos problemas foram observados no treinamento das redes neurais, que dificultavam sua capacidade de generalização e sua utilização em problemas práticos. No entanto, na década de 2010 diferentes abordagens foram propostas para melhorar seu treinamento, incluindo o treinamento das redes profundas. Essas abordagens fizeram com que as redes passassem a ganhar competições em diferentes problemas de classificação e regressão. Desde então, as redes neurais estão em ascensão devido à sua habilidade de resolver problemas anteriormente considerados insolúveis. Atualmente, elas têm sido consideradas em diferentes áreas como carros autônomos, cálculo de risco, detecção de fraude, detecção precoce de câncer, classificação de arritmias cardíacas, classificação de mosquitos, entre muitos outros.\nO aprendizado de máquina pode ser classificado em dois tipos: supervisionado e não supervisionado. No caso supervisionado, existem dados rotulados que são usados no treinamento. Por exemplo, suponha que desejamos classificar o tipo de arritmia cardíaca. Nesse tipo de aplicação, é comum considerar o aprendizado supervisionado: existem bancos de dados com sinais de eletrocardiograma (ECG), cujas arritmias foram analisadas e classificadas a priori por um grupo de especialistas. Parte desses sinais é utilizada no treinamento: para cada batimento cardíaco a saída da máquina é comparada com a classificação conhecida. A comparação entre a saída e a classificação desejada é usada para ajustar os parâmetros da máquina a fim de minimizar uma função dessa diferença. Parte dos dados que não foi utilizada no treinamento é reservada para o teste. Neste caso, os parâmetros da máquina são mantidos fixos para verificar se a máquina tem uma boa capacidade de generalização. Em outras palavras, verifica-se se a máquina treinada é capaz de classificar dados que não foram utilizados no treinamento, possibilitando medir a acurácia que se obtém nessa classificação. No aprendizado não supervisionado, a máquina deve ser capaz de ajustar seus parâmetros sem utilizar dados com a classificação ou regressão desejada, ou seja, os rótulos não são conhecidos. Esse tipo de aprendizado é adequado para problemas que exigem que a máquina identifique e extraia semelhanças entre as entradas para que entradas semelhantes possam ser categorizadas juntas. Os dois tipos fundamentais de métodos de aprendizado não supervisionado são o agrupamento e a estimativa de densidade. O primeiro, que é o mais utilizado na prática, envolve problemas em que se necessita agrupar os dados em categorias específicas conhecidas como clusters, enquanto o último envolve estimar a distribuição estatística dos dados. Alguns exemplos de algoritmos de aprendizado de máquina não supervisionado incluem \\(k\\)-means, análise de componentes principais e clusterização hierárquica.\nA seguir vamos abordar os dois tipos de problema que as redes neurais são capazes de resolver: classificação e regressão.\n\n\nA modelagem preditiva consiste em desenvolver um modelo usando dados históricos para fazer uma previsão sobre novos dados para os quais não temos resposta. Ela pode ser descrita pelo problema matemático de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída, que é chamado de aproximação de funções. O trabalho do algoritmo de modelagem é encontrar a melhor função de mapeamento possível, considerando o tempo e os recursos disponíveis. Podemos dividir a aproximação de funções em problemas de classificação ou de regressão.\n\n\nA modelagem preditiva de classificação é a tarefa de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída discretas. As variáveis de saída são frequentemente chamadas de rótulos ou categorias. A função de mapeamento prevê a classe ou categoria para uma determinada observação. Por exemplo, um e-mail de texto pode ser classificado como pertencente a uma das duas classes: “spam” e “não spam”.\nAlgumas observações sobre o problema de classificação:\n\nUm problema de classificação requer que os exemplos sejam classificados em uma de duas ou mais classes;\nUma classificação pode ter variáveis de entrada discretas ou contínuas. Variáveis contínuas são aquelas que assumem qualquer valor em um intervalo da reta real, e.g. qualquer valor real do intervalo \\([-1,\\; 1]\\);\nUm problema com duas classes é chamado de classificação binária;\nUm problema com mais de duas classes é chamado de classificação multiclasse;\nUm problema em que um exemplo é atribuído a várias classes é chamado de classificação multirrótulo.\n\nÉ comum que os modelos de classificação prevejam um valor contínuo como a probabilidade de um dado exemplo pertencer a cada classe de saída. Uma probabilidade prevista pode ser convertida em um valor de classe selecionando o rótulo da classe que tem a probabilidade mais alta. Por exemplo, um e-mail específico de texto pode receber as probabilidades de \\(0,1\\) de ser “spam” e \\(0,9\\) de ser “não spam”. Podemos converter essas probabilidades em um rótulo de classe selecionando o rótulo “não spam”, pois ele tem a maior probabilidade prevista.\nExistem muitas maneiras de estimar a habilidade do classificador, sendo a mais comum calcular a acurácia da classificação. A acurácia da classificação é a porcentagem de exemplos classificados corretamente de todas as previsões feitas. Por exemplo, se um modelo preditivo de classificação fizer 5 previsões e 3 delas estiverem corretas e 2 incorretas, a acurácia da classificação do modelo com base apenas nessas previsões seria \\((3/5)\\times 100= 60\\%\\).\nComo exemplo de problema de classificação, considere as 20 imagens da Figura 2. Observe que cada imagem contém a foto de um gato ou de um cachorro. Pode-se usar uma técnica de aprendizado de máquina para classificar cada uma dessas imagens entre as duas classes possíveis: gato ou cachorro.\n\n\n\n\n\n\nFigura 2: Imagens de gatos e cachorros.\n\n\n\n\n\n\nA modelagem preditiva de regressão é a tarefa de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída contínuas. Por exemplo, pode-se prever que um apartamento seja vendido por um valor específico em reais, talvez na faixa de R$\\(500.000\\).\nAlgumas observações sobre o problema de regressão:\n\nUm problema de regressão requer a previsão de uma quantidade;\nUma regressão pode ter variáveis de entrada discretas ou contínuas;\nUm problema com múltiplas variáveis de entrada é chamado regressão multivariada;\nUm problema de regressão em que as variáveis de entrada são ordenadas por tempo é chamado de previsão de séries temporais.\n\nComo o modelo preditivo de regressão prevê uma quantidade, a habilidade do modelo deve ser descrita pelo erro de predição calculado como a diferença entre a previsão do regressor e o valor desejado. Existem muitas maneiras de estimar a habilidade do regressor, sendo a mais comum calcular a raiz quadrada do erro quadrático médio (root mean squared error - RMSE), que tem a mesma unidade do valor predito. Por exemplo, se um modelo preditivo de regressão fez 2 previsões, uma de 1,5 em que o valor esperado é 1,0 e outra de 3,3 em que o valor esperado é 3,0, então o RMSE seria\n\\[\n{\\rm RMSE}=\\sqrt{\\frac{(1,0-1,5)^2+(3,0-3,3)^2}{2}}=0,412.\n\\]\nComo exemplo de problema de regressão, considere a evolução do índice Bovespa (Ibovespa) de dezembro de 1967 a dezembro de 2017 mostrado na Figura 3. O Ibovespa é o indicador de desempenho mais importante das ações negociadas na B3 e reúne as empresas mais importantes do mercado de capitais brasileiro. Suponha que se deseja usar uma técnica de aprendizado de máquina para prever o valor desse índice no ano seguinte, ou seja, de janeiro a dezembro de 2018. Para isso, pode-se considerar os dados sequenciais desde dezembro de 1967 e usar os dados do ano seguinte como resposta desejada. Assim, no treinamento pode-se considerar, por exemplo, os dados de 1968 para prever os dados de 1969 e assim por diante. O valor do índice a ser previsto é qualquer valor contínuo no intervalo \\([0,\\; 131.000]\\) já que em 2018 esse índice não tinha atingido seu recorde nominal histórico que ocorreu em 07/06/2021, fechando em 130.776 pontos.\n\n\n\n\n\n\nFigura 3: Evolução do Ibovespa de dezembro de 1967 a dezembro de 2017.\n\n\n\n\n[Haykin (2009)](Kinsley e Kukie​la 2020)[Brownlee (2017)](Izenman 2008)(Murphy 2012)"
  },
  {
    "objectID": "t_introducao.html#aproximação-de-funções",
    "href": "t_introducao.html#aproximação-de-funções",
    "title": "Introdução",
    "section": "",
    "text": "A modelagem preditiva consiste em desenvolver um modelo usando dados históricos para fazer uma previsão sobre novos dados para os quais não temos resposta. Ela pode ser descrita pelo problema matemático de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída, que é chamado de aproximação de funções. O trabalho do algoritmo de modelagem é encontrar a melhor função de mapeamento possível, considerando o tempo e os recursos disponíveis. Podemos dividir a aproximação de funções em problemas de classificação ou de regressão.\n\n\nA modelagem preditiva de classificação é a tarefa de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída discretas. As variáveis de saída são frequentemente chamadas de rótulos ou categorias. A função de mapeamento prevê a classe ou categoria para uma determinada observação. Por exemplo, um e-mail de texto pode ser classificado como pertencente a uma das duas classes: “spam” e “não spam”.\nAlgumas observações sobre o problema de classificação:\n\nUm problema de classificação requer que os exemplos sejam classificados em uma de duas ou mais classes;\nUma classificação pode ter variáveis de entrada discretas ou contínuas. Variáveis contínuas são aquelas que assumem qualquer valor em um intervalo da reta real, e.g. qualquer valor real do intervalo \\([-1,\\; 1]\\);\nUm problema com duas classes é chamado de classificação binária;\nUm problema com mais de duas classes é chamado de classificação multiclasse;\nUm problema em que um exemplo é atribuído a várias classes é chamado de classificação multirrótulo.\n\nÉ comum que os modelos de classificação prevejam um valor contínuo como a probabilidade de um dado exemplo pertencer a cada classe de saída. Uma probabilidade prevista pode ser convertida em um valor de classe selecionando o rótulo da classe que tem a probabilidade mais alta. Por exemplo, um e-mail específico de texto pode receber as probabilidades de \\(0,1\\) de ser “spam” e \\(0,9\\) de ser “não spam”. Podemos converter essas probabilidades em um rótulo de classe selecionando o rótulo “não spam”, pois ele tem a maior probabilidade prevista.\nExistem muitas maneiras de estimar a habilidade do classificador, sendo a mais comum calcular a acurácia da classificação. A acurácia da classificação é a porcentagem de exemplos classificados corretamente de todas as previsões feitas. Por exemplo, se um modelo preditivo de classificação fizer 5 previsões e 3 delas estiverem corretas e 2 incorretas, a acurácia da classificação do modelo com base apenas nessas previsões seria \\((3/5)\\times 100= 60\\%\\).\nComo exemplo de problema de classificação, considere as 20 imagens da Figura 2. Observe que cada imagem contém a foto de um gato ou de um cachorro. Pode-se usar uma técnica de aprendizado de máquina para classificar cada uma dessas imagens entre as duas classes possíveis: gato ou cachorro.\n\n\n\n\n\n\nFigura 2: Imagens de gatos e cachorros.\n\n\n\n\n\n\nA modelagem preditiva de regressão é a tarefa de aproximar uma função de mapeamento de variáveis de entrada para variáveis de saída contínuas. Por exemplo, pode-se prever que um apartamento seja vendido por um valor específico em reais, talvez na faixa de R$\\(500.000\\).\nAlgumas observações sobre o problema de regressão:\n\nUm problema de regressão requer a previsão de uma quantidade;\nUma regressão pode ter variáveis de entrada discretas ou contínuas;\nUm problema com múltiplas variáveis de entrada é chamado regressão multivariada;\nUm problema de regressão em que as variáveis de entrada são ordenadas por tempo é chamado de previsão de séries temporais.\n\nComo o modelo preditivo de regressão prevê uma quantidade, a habilidade do modelo deve ser descrita pelo erro de predição calculado como a diferença entre a previsão do regressor e o valor desejado. Existem muitas maneiras de estimar a habilidade do regressor, sendo a mais comum calcular a raiz quadrada do erro quadrático médio (root mean squared error - RMSE), que tem a mesma unidade do valor predito. Por exemplo, se um modelo preditivo de regressão fez 2 previsões, uma de 1,5 em que o valor esperado é 1,0 e outra de 3,3 em que o valor esperado é 3,0, então o RMSE seria\n\\[\n{\\rm RMSE}=\\sqrt{\\frac{(1,0-1,5)^2+(3,0-3,3)^2}{2}}=0,412.\n\\]\nComo exemplo de problema de regressão, considere a evolução do índice Bovespa (Ibovespa) de dezembro de 1967 a dezembro de 2017 mostrado na Figura 3. O Ibovespa é o indicador de desempenho mais importante das ações negociadas na B3 e reúne as empresas mais importantes do mercado de capitais brasileiro. Suponha que se deseja usar uma técnica de aprendizado de máquina para prever o valor desse índice no ano seguinte, ou seja, de janeiro a dezembro de 2018. Para isso, pode-se considerar os dados sequenciais desde dezembro de 1967 e usar os dados do ano seguinte como resposta desejada. Assim, no treinamento pode-se considerar, por exemplo, os dados de 1968 para prever os dados de 1969 e assim por diante. O valor do índice a ser previsto é qualquer valor contínuo no intervalo \\([0,\\; 131.000]\\) já que em 2018 esse índice não tinha atingido seu recorde nominal histórico que ocorreu em 07/06/2021, fechando em 130.776 pontos.\n\n\n\n\n\n\nFigura 3: Evolução do Ibovespa de dezembro de 1967 a dezembro de 2017.\n\n\n\n\n[Haykin (2009)](Kinsley e Kukie​la 2020)[Brownlee (2017)](Izenman 2008)(Murphy 2012)"
  },
  {
    "objectID": "t_autodiff.html",
    "href": "t_autodiff.html",
    "title": "Introdução à diferenciação automática",
    "section": "",
    "text": "Download dos slides"
  },
  {
    "objectID": "ex_aula_mlp.html",
    "href": "ex_aula_mlp.html",
    "title": "Exercício - MLP",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_perceptron_rosenblatt_e_reg_logistica_2.html",
    "href": "ex_aula_perceptron_rosenblatt_e_reg_logistica_2.html",
    "title": "Exercício - Perceptron de Rosenblatt e Regressão Logística 2",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ap_python_topicos/python_01.html",
    "href": "ap_python_topicos/python_01.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n3:26 Instalação da distribuição Python Miniconda\n6:44 Abrindo o shell do Anaconda\n8:26 Listando e criando ambientes conda\n10:06 Instalando pacote conda do Jupyter\n12:26 Iniciando o Jupyter Notebook\n17:36 Instalando pacotes em um ambiente, com pip\n20:26 Iniciando o Jupyter Lab\n23:18 Desativando e removendo um ambiente conda\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_01.html#configurando-o-ambiente",
    "href": "ap_python_topicos/python_01.html#configurando-o-ambiente",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n3:26 Instalação da distribuição Python Miniconda\n6:44 Abrindo o shell do Anaconda\n8:26 Listando e criando ambientes conda\n10:06 Instalando pacote conda do Jupyter\n12:26 Iniciando o Jupyter Notebook\n17:36 Instalando pacotes em um ambiente, com pip\n20:26 Iniciando o Jupyter Lab\n23:18 Desativando e removendo um ambiente conda\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_06.html",
    "href": "ap_python_topicos/python_06.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n1:50 Comparando o desempenho do Python Puro com o do NumPy\n12:25 Criando arrays do NumPy\n14:05 Entendendo os tipos de dados em arrays do NumPy\n16:40 Inspecionando as dimensões (shape) de um array do NumPy\n16:56 Obtendo a transposta de uma matriz\n18:10 Evitando problemas com arrays de rank 1\n24:20 Testando as dimensões de arrays com a função assert\n25:20 Convertendo as dimensões de um array do NumPy (reshape)\n30:50 Fatiamento em arrays do NumPy (slicing)\n37:00 Usando funções de conveniência para criar arrays do NumPy\n40:30 Fazendo operações aritméticas elemento a elemento com arrays do NumPy\n44:00 Fazendo operações matriciais com arrays do NumPy\n46:00 Entendendo o broadcasting\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_06.html#numpy",
    "href": "ap_python_topicos/python_06.html#numpy",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n1:50 Comparando o desempenho do Python Puro com o do NumPy\n12:25 Criando arrays do NumPy\n14:05 Entendendo os tipos de dados em arrays do NumPy\n16:40 Inspecionando as dimensões (shape) de um array do NumPy\n16:56 Obtendo a transposta de uma matriz\n18:10 Evitando problemas com arrays de rank 1\n24:20 Testando as dimensões de arrays com a função assert\n25:20 Convertendo as dimensões de um array do NumPy (reshape)\n30:50 Fatiamento em arrays do NumPy (slicing)\n37:00 Usando funções de conveniência para criar arrays do NumPy\n40:30 Fazendo operações aritméticas elemento a elemento com arrays do NumPy\n44:00 Fazendo operações matriciais com arrays do NumPy\n46:00 Entendendo o broadcasting\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_05.html",
    "href": "ap_python_topicos/python_05.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n0:40 Entendendo a tipagem dinâmica\n2:27 Representando números complexos\n3:00 Usando os operadores de divisão\n5:30 Usando o operador módulo (resto da divisão)\n7:05 Entendendo como as variáveis são identificadas\n10:02 Usando a função id() para identificar objetos na memória\n11:10 Copiando objetos com o método .copy()\n12:35 Usando a biblioteca copy\n14:25 Fazendo uma cópia rasa (shallow)\n18:05 Fazendo uma cópia profunda (deep)\n20:40 Usando a indexação em coleções (indexing)\n22:50 Fazendo fatiamento de coleções (slicing)\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_05.html#alguns-detalhes-do-python",
    "href": "ap_python_topicos/python_05.html#alguns-detalhes-do-python",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n0:40 Entendendo a tipagem dinâmica\n2:27 Representando números complexos\n3:00 Usando os operadores de divisão\n5:30 Usando o operador módulo (resto da divisão)\n7:05 Entendendo como as variáveis são identificadas\n10:02 Usando a função id() para identificar objetos na memória\n11:10 Copiando objetos com o método .copy()\n12:35 Usando a biblioteca copy\n14:25 Fazendo uma cópia rasa (shallow)\n18:05 Fazendo uma cópia profunda (deep)\n20:40 Usando a indexação em coleções (indexing)\n22:50 Fazendo fatiamento de coleções (slicing)\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "t_pytorch_topicos.html",
    "href": "t_pytorch_topicos.html",
    "title": "Tópicos sobre o framework PyTorch",
    "section": "",
    "text": "No Google Colab, é necessário habilitar o suporte à GPU acessando “Change Runtime Type” e selecionando uma opção com GPU como, por exemplo, a “T4 GPU”."
  },
  {
    "objectID": "t_pytorch_topicos.html#tensores-e-operações-básicas",
    "href": "t_pytorch_topicos.html#tensores-e-operações-básicas",
    "title": "Tópicos sobre o framework PyTorch",
    "section": "Tensores e operações básicas",
    "text": "Tensores e operações básicas\n\nObjetos do tipo torch.Tensor, semelhantes à arrays do NumPy, mas com algumas funções adicionais:\n\nPodem ser alocados facilmente na GPU;\nPossibilidade de cálculo automático de gradientes.\n\nRef.: https://pytorch.org/docs/stable/tensors.html\nPodem ser criados a partir de listas do Python:\n\n\ntorch.tensor([2, 3, 4])\n\ntensor([2, 3, 4])\n\n\n\nAtributos de shape e rank semelhantes aos arrays do NumPy:\n\n\nx = torch.tensor([2, 3, 4])\nx.shape\n\ntorch.Size([3])\n\n\n\nx = torch.tensor([[2, 3, 4]])\nx.shape\n\ntorch.Size([1, 3])\n\n\n\nTipo padrão para representação de ponto flutuante é o float32:\n\n\nx = torch.tensor([[2., 3., 4.]])\nx.dtype\n\ntorch.float32\n\n\n\nFunções auxiliares para criação de tensores com zeros, uns, aleatórios e matrizes identidade:\n\n\ntorch.zeros((2, 3))\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones((3, 2))\n\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\n\ntorch.rand(2, 2)\n\ntensor([[0.1710, 0.2434],\n        [0.2614, 0.6550]])\n\n\n\ntorch.randn(3, 3)\n\ntensor([[ 2.2044, -0.7878, -1.0922],\n        [ 0.0309, -0.4513,  1.5449],\n        [ 0.3358,  0.2322,  0.2016]])\n\n\n\ntorch.eye(3)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n\n\nFunções auxiliares para criação de sequências:\n\n\ntorch.linspace(0, 1, 10)\n\ntensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n        1.0000])\n\n\n\ntorch.arange(10)\n\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\n\nreshapee view: são semelhantes, mas\n\nview usa os mesmos dados do tensor original. Não funciona para o caso de dados não contíguos;\nreshape tenta fazer o mesmo que view, mas no caso de dados não contíguos, retorna um tensor com uma cópia dos dados originais.\n\nA sugestão é usar sempre view;\nRef.: https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n\n\nx = torch.rand(4,3)\nx\n\ntensor([[0.1625, 0.3299, 0.0073],\n        [0.2945, 0.8182, 0.0982],\n        [0.6897, 0.5476, 0.5530],\n        [0.8207, 0.0292, 0.2640]])\n\n\n\nx.view(12, -1)\n\ntensor([[0.1625],\n        [0.3299],\n        [0.0073],\n        [0.2945],\n        [0.8182],\n        [0.0982],\n        [0.6897],\n        [0.5476],\n        [0.5530],\n        [0.8207],\n        [0.0292],\n        [0.2640]])\n\n\n\nx.reshape(12, -1)\n\ntensor([[0.1625],\n        [0.3299],\n        [0.0073],\n        [0.2945],\n        [0.8182],\n        [0.0982],\n        [0.6897],\n        [0.5476],\n        [0.5530],\n        [0.8207],\n        [0.0292],\n        [0.2640]])\n\n\n\nOperações\n\nOperações aritméticas e matriciais semelhantes às do NumPy;\nPyTorch disponibiliza diversas operações ponto a ponto como torch.abs() e torch.cos() e diversas operações de redução como torch.sum() e torch.mean();\nÉ importante utilizar as operações do PyTorch para processar os tensores, para que seja possível calcular o gradiente automaticamente com o autograd;\nAlém disso, há diversas operações de comparação, espectrais e outras.\nReferência: https://pytorch.org/docs/stable/torch.html#math-operations\n\n\n\nAlocação em CPU e em GPU\n\nAtributo is_cuda permite ver se o tensor está alocado na GPU:\n\n\nx = torch.Tensor([1, 2, 3])\n\n\nx.is_cuda\n\nFalse\n\n\n\nPara alocar na GPU, é necessário criar um objeto device e usar o método .to():\n\n\ndevice = torch.device(\"cuda\")\nx = x.to(device)\n\n\nx.is_cuda\n\nTrue\n\n\n\nÉ possível ver se há GPU disponível com o método torch.cuda.is_available():\n\n\ntorch.cuda.is_available() \n\nTrue\n\n\n\nÉ usual usar a seguinte estrutura para alocação automática de tensores na GPU, quando disponível:\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nx = x.to(device)\n\n\nPara trazer realocar um tensor de volta à CPU, pode-se usar .cpu():\n\n\nx.is_cuda\n\nTrue\n\n\n\nx = x.cpu()\n\n\nx.is_cuda\n\nFalse\n\n\n\nO método .to() também é usado para fazer casting de tensores:\n\n\nx.dtype\n\ntorch.float32\n\n\n\ny = x.to(torch.float64)\ny.dtype\n\ntorch.float64\n\n\n\n\nConversão de dados para NumPy e vice-versa\n\nUm tensor do PyTorch pode ser convertido para um array do NumPy usando o método .numpy():\n\n\nx = torch.randn(5,5)\nx\n\ntensor([[ 0.0226,  1.1913,  0.9024,  0.1993,  1.4604],\n        [ 0.4746, -0.0730, -0.3775, -0.3115,  0.0461],\n        [-0.1660,  0.2899,  1.5823, -0.0143,  1.0010],\n        [ 1.3880, -0.7835,  1.2934, -1.7216, -0.3019],\n        [ 1.1091,  0.1235, -0.8572,  0.1827, -1.4729]])\n\n\n\nx.dtype\n\ntorch.float32\n\n\n\nx_np = x.numpy()\nx_np\n\narray([[ 0.02260382,  1.1912687 ,  0.902434  ,  0.19927278,  1.4604093 ],\n       [ 0.4746041 , -0.0730461 , -0.3775207 , -0.31152746,  0.04614168],\n       [-0.16600847,  0.28993207,  1.5822891 , -0.01425551,  1.0010148 ],\n       [ 1.3880016 , -0.7835027 ,  1.2933816 , -1.7216129 , -0.30192617],\n       [ 1.1091497 ,  0.12347655, -0.8571574 ,  0.18268453, -1.4728575 ]],\n      dtype=float32)\n\n\n\nx_np.dtype\n\ndtype('float32')\n\n\n\ntorch.Tensor pode criar um tensor PyTorch a partir de um array do NumPy, mas é necessário atenção à precisão numérica:\n\n\nx_np = np.random.randn(5,5)\nx_np\n\narray([[-1.15333332, -1.68247501,  0.26655873,  1.01178785, -0.87462355],\n       [-1.6621385 ,  0.21882814,  0.12260001,  0.50508408,  0.44652293],\n       [-1.14593503, -0.42022754, -0.63663901, -0.32429263,  1.12014158],\n       [ 1.07981783,  0.69727713,  0.08246118, -1.24697262,  1.4056202 ],\n       [ 0.56203902,  0.69106794,  0.31189597,  0.24461639, -0.62747763]])\n\n\n\nx_np.dtype\n\ndtype('float64')\n\n\n\nx = torch.Tensor(x_np,)\nx\n\ntensor([[-1.1533, -1.6825,  0.2666,  1.0118, -0.8746],\n        [-1.6621,  0.2188,  0.1226,  0.5051,  0.4465],\n        [-1.1459, -0.4202, -0.6366, -0.3243,  1.1201],\n        [ 1.0798,  0.6973,  0.0825, -1.2470,  1.4056],\n        [ 0.5620,  0.6911,  0.3119,  0.2446, -0.6275]])\n\n\n\nx.dtype\n\ntorch.float32\n\n\n\nA função torch.from_numpy preserva o tipo do array NumPy:\n\n\nx2 = torch.from_numpy(x_np)\nx2.dtype\n\ntorch.float64"
  },
  {
    "objectID": "t_pytorch_topicos.html#autograd",
    "href": "t_pytorch_topicos.html#autograd",
    "title": "Tópicos sobre o framework PyTorch",
    "section": "Autograd",
    "text": "Autograd\n\nTensores com o atributo requires_grad=True têm o gradiente calculado automaticamente;\nSó vetores do tipo float ou complex podem usar requires_grad=True.\n\n\nx0 = torch.tensor([1., 2., 3.], requires_grad=True)\nx0\n\ntensor([1., 2., 3.], requires_grad=True)\n\n\n\nx1 = torch.tensor([4., 5., 6.], requires_grad=True)\nx1\n\ntensor([4., 5., 6.], requires_grad=True)\n\n\n\nTensores criados a partir de outros com requires_grad=True também têm requires_grad=True:\n\n\nf = torch.sum(x0**2 + x1)\n\n\nf.requires_grad\n\nTrue\n\n\n\nTensores criados pelo usuário são leaf nodes no grafo, identificados pelo atributo is_leaf:\n\n\nx0.is_leaf\n\nTrue\n\n\n\nx1.is_leaf\n\nTrue\n\n\n\nf.is_leaf\n\nFalse\n\n\n\nGradiente de f em relação x0 e x1:\n\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} =\n\\left[\n\\begin{matrix}\n\\frac{\\partial f}{\\partial x_0}\\\\\n\\frac{\\partial f}{\\partial x_1}\\\\\n\\end{matrix}\n\\right] =\n\\left[\n\\begin{matrix}\n2x_0\\\\\n1\\\\\n\\end{matrix}\n\\right]\n\\]\n\nOs gradientes são armazenados no atributo grad, inicialmente igual a None:\n\n\nx0.grad is None\n\nTrue\n\n\n\nx1.grad is None\n\nTrue\n\n\n\nPara que os gradientes sejam calculados, é necessário executar o método .backward() do nó em relação ao qual se deseja calculá-los:\n\n\nf.backward()\n\n\nx0.grad\n\ntensor([2., 4., 6.])\n\n\n\nx0\n\ntensor([1., 2., 3.], requires_grad=True)\n\n\n\nx1.grad\n\ntensor([1., 1., 1.])\n\n\n\nx1\n\ntensor([4., 5., 6.], requires_grad=True)"
  },
  {
    "objectID": "t_pytorch_topicos.html#do-numpy-ao-pytorch",
    "href": "t_pytorch_topicos.html#do-numpy-ao-pytorch",
    "title": "Tópicos sobre o framework PyTorch",
    "section": "Do NumPy ao PyTorch",
    "text": "Do NumPy ao PyTorch\nA seguir, serão apresentados os principais elementos do PyTorch, partindo de um exemplo de treinamento de um modelo com o LMS implementado com o NumPy. Serão abordados:\n\nUso de tensores PyTorch;\nUso do autograd;\nBlocos para função custo;\nBlocos para otimizadores;\nUso de objetos representando modelos PyTorch;\nUso de blocos PyTorch para a composição de modelos.\n\nAs mudanças no código serão indicadas por comentários.\n\nComeçando pelo NumPy\n\nCódigo para treinamento de um modelo com o LMS para identificação de sistemas:\n\n\nN = 500\nM = 3\nsigmav2 = 0.01\neta = 0.1\n\nx = np.random.randn(N, 1)\nwo = np.array([[0.1, 0.2, 0.3]])\nd = signal.lfilter(wo.squeeze(), 1, x.squeeze()) + np.sqrt(sigmav2) * np.random.randn(N)\nd = d.reshape(-1, 1)\n\n\ndef lms(x, d, eta, M):\n    N = len(x)\n    xM = np.zeros((M, 1))\n    wi = np.zeros((1, M))    \n    y = np.zeros((N, 1))\n    e = np.zeros((N, 1))\n    w = np.zeros((N + 1, M))\n    \n    for i in range(N):\n        xM = np.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        yi = wi @ xM\n        ei = d[i] - yi\n        wi = wi + eta/2 * 2 * ei * xM.T\n        y[i] = yi\n        e[i] = ei\n        w[i + 1, :] = wi\n    return y, e, w\n\n\n(y_lms, e_lms, w_lms) = lms(x, d, eta, M)\n\n\nVamos comparar a evolução dos pesos, usando a seguinte função:\n\n\ndef plot_ws(w, w_lms):\n    plt.figure()\n    plt.plot(w, \"b\")\n    plt.plot(w_lms, \"k\", linewidth=5, alpha=0.5)\n    plt.xlabel(\"iterações\")\n    plt.ylabel(\"coeficientes\")\n    plt.grid()\n\n\nplot_ws(w_lms, w_lms)\n\n\n\n\n\n\n\n\n\n\nUso de tensores PyTorch\n\nPara manter a precisão numérica padrão do NumPy, vamos configurar o PyTorch para usar tensores do tipo float64:\n\n\ntorch.set_default_dtype(torch.float64)\n\n\nA forma de uso dos tensores PyTorch é bem semelhante à dos arrays NumPy. Na maioria dos casos, basta trocar a chamada np. por torch.:\n\n\nx = torch.tensor(x)\nwo = torch.tensor(wo)\nd = torch.tensor(d)\n\n\ndef lms_torch(x, d, eta, M):\n    N = len(x)\n    \n    # xM = np.zeros((M, 1))\n    # wi = np.zeros((1, M))\n    # y = np.zeros((N, 1))\n    # e = np.zeros((N, 1))\n    # w = np.zeros((N + 1, M))\n    xM = torch.zeros((M, 1))\n    wi = torch.zeros((1, M))\n    y = torch.zeros((N, 1))\n    e = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n    \n    for i in range(N):\n        \n        # xM = np.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        \n        yi = wi @ xM\n        ei = d[i] - yi\n        wi = wi + eta/2 * 2 * ei * xM.T\n        y[i] = yi\n        e[i] = ei\n        w[i + 1, :] = wi\n    return y, e, w\n\n\n(y_torch, e_torch, w_torch) = lms_torch(x, d, eta, M)\n\n\nplot_ws(w_torch.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\n\nUso do autograd\n\nTensores que necessitam do cálculo do gradiente, devem ter o atributo requires_grad=True;\nGradientes são calculados utilizando o método .backward() chamado no objeto que representa o nó em relação ao qual desejamos calcular os gradientes;\nÉ necessário tomar cuidado com operações para as quais não queremos calcular o gradiente. Nesses casos, utilizamos o bloco de contexto with torch.no_grad():;\nNote que é importante não sobrescrever o objeto wi para que os gradientes sejam computados corretamente (uso de wi[:] = (...));\nA cada chamada de .backward(), os valores dos gradientes são acumulados nos atributos .grad de cada parâmetro do modelo;\n\nDessa forma, é necessário zerar os gradientes a cada iteração, usando o método .grad.zero_().\n\n\n\ndef lms_torch_autograd(x, d, eta, M):\n    N = len(x)\n    xM = torch.zeros((M, 1))\n\n    # wi = torch.zeros((1, M))\n    wi = torch.zeros((1, M), requires_grad=True)\n\n    y = torch.zeros((N, 1))\n    e = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        yi = wi @ xM\n        ei = d[i] - yi        \n\n        # Novo\n        mse = ei**2\n        mse.backward()\n\n        # wi = wi + eta/2 * 2 * ei * xM.T\n        # y[i] = yi\n        # e[i] = ei\n        # w[i + 1, :] = wi\n        with torch.no_grad():            \n            wi[:] = wi[:] - eta/2 * wi.grad\n            y[i] = yi\n            e[i] = ei\n            w[i + 1, :] = wi\n\n        # Novo    \n        wi.grad.zero_()\n        \n    return y, e, w\n\n\n(y_torch_autograd, e_torch_autograd, w_torch_autograd) = lms_torch_autograd(x, d, eta, M)\n\n\nplot_ws(w_torch_autograd.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\n\nBlocos para função custo\n\nO PyTorch disponibiliza diversos blocos para a representação de funções custo: https://pytorch.org/docs/stable/nn.html#loss-functions;\nPara o caso da função custo MSE, utilizamos o bloco nn.MSELoss.\n\n\ndef lms_torch_loss(x, d, eta, M):\n    N = len(x)\n    xM = torch.zeros((M, 1))\n    wi = torch.zeros((1, M), requires_grad=True)\n    y = torch.zeros((N, 1))\n    \n    #e = torch.zeros((N, 1))\n    losses = torch.zeros((N, 1))\n\n    w = torch.zeros(N + 1, M)\n\n    # Novo\n    loss_function = nn.MSELoss()\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        yi = wi @ xM\n\n        #ei = d[i] - yi\n        #mse = ei**2\n        #mse.backward()\n        loss = loss_function(yi.squeeze(), d[i].squeeze())\n        loss.backward()\n    \n        with torch.no_grad():            \n            wi[:] = wi[:] - eta/2 * wi.grad\n            y[i] = yi\n            losses[i] = loss\n            w[i + 1, :] = wi\n        wi.grad.zero_()\n    return y, loss, w\n\n\n(y_torch_loss, e_torch_loss, w_torch_loss) = lms_torch_loss(x, d, eta, M)\n\n\nplot_ws(w_torch_loss.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\n\nBlocos para otimizadores\n\nO PyTorch disponibiliza diversos blocos para a representação de otimizadores: https://pytorch.org/docs/stable/optim.html#algorithms;\nPara o caso do otimizador com o algoritmo backpropagation tradicional, chamado de Stochastic Gradient Descent, utilizamos o bloco nn.SGD;\nA atualização dos pesos do otimizador é feita chamando o método .step().\n\n\ndef lms_torch_optim(x, d, eta, M):\n    N = len(x)\n    xM = torch.zeros((M, 1))\n    wi = torch.zeros((1, M), requires_grad=True)\n    y = torch.zeros((N, 1))\n    losses = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n\n    loss_function = nn.MSELoss()\n\n    # Novo\n    optimizer = torch.optim.SGD([wi], lr=eta/2)\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        yi = wi @ xM\n        loss = loss_function(yi.squeeze(), d[i].squeeze())\n        loss.backward()\n\n        # Novo\n        optimizer.step()\n    \n        with torch.no_grad():            \n            \n            # wi[:] = wi[:] - eta/2 * wi.grad\n            \n            y[i] = yi\n            losses[i] = loss\n            w[i + 1, :] = wi\n        wi.grad.zero_()\n    return y, loss, w\n\n\n(y_torch_optim, e_torch_optim, w_torch_optim) = lms_torch_optim(x, d, eta, M)\n\n\nplot_ws(w_torch_optim.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\n\nUso de objetos representando modelos PyTorch\n\nOs modelos PyTorch são construídos com a definição de classes que herdam de nn.module;\nNo método __init__(), devem ser criados os elementos que compõem o modelo. Nesse caso, vamos utilizar apenas um objeto representando parâmetros genéricos;\nO método forward() define como é calculada a saída a partir da entrada, nesse caso chamada de xM;\nApós a criação da classe, instancia-se um objeto para representar o modelo;\n\nA saída do modelo é calculada utilizando este objeto.\n\nAssim como feito anteriormente, é necessário zerar os gradientes do modelo a cada iteração;\n\nÉ usual fazer isso no início do loop de treinamento.\n\n\n\n# Novo\nclass LMS(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wi = nn.Parameter(torch.zeros((1, M), requires_grad=True))\n        \n    def forward(self, xM):\n        output = self.wi @ xM\n        return output\n\n# Novo\nmodel = LMS()\n\n# def lms_torch_optim(x, d, eta, M):\ndef lms_torch_model(x, d, eta, M, model):\n\n    N = len(x)\n    xM = torch.zeros((M, 1))\n    \n    # wi = torch.zeros((1, M), requires_grad=True)\n    \n    y = torch.zeros((N, 1))\n    losses = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n\n    loss_function = nn.MSELoss()\n\n    #optimizer = torch.optim.SGD([wi], lr=eta/2)\n    optimizer = torch.optim.SGD(model.parameters(), lr=eta/2)\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n\n        # yi = wi @ xM\n        model.zero_grad()\n        yi = model(xM)\n\n        loss = loss_function(yi.squeeze(), d[i].squeeze())\n        loss.backward()\n        optimizer.step()\n    \n        with torch.no_grad():            \n            y[i] = yi\n            losses[i] = loss\n            \n            # w[i + 1, :] = wi\n            w[i + 1, :] = model.wi.clone()\n\n        # wi.grad.zero_()        \n    \n    return y, loss, w\n\n\n(y_torch_model, e_torch_model, w_torch_model) = lms_torch_model(x, d, eta, M, model)\n\n\nplot_ws(w_torch_model.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\n\nUso de blocos PyTorch para a composição de modelos\n\nNo último exemplo, construímos um modelo PyTorch baseado em um conjunto de parâmetros configurados com nn.Parameter.\n\nNo entanto, o PyTorch conta com inúmeros blocos para a composição de modelos como blocos lineares e funções de ativação para a composição de camadas de redes MLP;\nRef.: https://pytorch.org/docs/stable/nn.html.\n\nNo caso do LMS, podemos utilizar o bloco nn.Linear.\n\n\n# class LMS(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.wi = nn.Parameter(torch.zeros((1, M), requires_grad=True))\n        \n#     def forward(self, xM):\n#         output = self.wi @ xM\n#         return output\n\nclass LMS(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(M, 1, bias=False)        \n        \n    def forward(self, x):\n        output = self.l1(x.squeeze())\n        return output\n\nmodel = LMS()\n\ndef lms_torch_model_2(x, d, eta, M, model):\n    N = len(x)\n    xM = torch.zeros((M, 1))\n    y = torch.zeros((N, 1))\n    losses = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n\n    loss_function = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=eta/2)\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        model.zero_grad()\n        yi = model(xM)\n        loss = loss_function(yi.squeeze(), d[i].squeeze())\n        loss.backward()\n        optimizer.step()\n    \n        with torch.no_grad():            \n            y[i] = yi\n            losses[i] = loss\n\n            # w[i + 1, :] = model.wi.clone()\n            w[i + 1, :] = model.l1.weight.clone()\n\n    return y, loss, w\n\n\n(y_torch_model_2, e_torch_model_2, w_torch_model_2) = lms_torch_model_2(x, d, eta, M, model)\n\n\nplot_ws(w_torch_model_2.numpy(), w_lms)\n\n\n\n\n\n\n\n\n\nAs curvas de evolução dos pesos não coincidem com as anteriores, neste caso.\n\nIsso ocorre por conta da inicialialização dos pesos utilizada pelo bloco nn.Linearque não são inicializados com zeros, conforme descrito na documentação.\n\nPara obter o mesmo comportamento, é necessário inicializar os pesos com zeros:\n\n\nclass LMS(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(M, 1, bias=False)\n\n    def forward(self, x):\n        output = self.l1(x.squeeze())\n        return output\n\n# Novo\ndef weights_init(m):\n    classname = m.__class__.__name__    \n    if classname.find('Linear') != -1:\n        if m.weight is not None:\n            #torch.nn.init.xavier_normal_(m.weight)\n            torch.nn.init.zeros_(m.weight)\n        if m.bias is not None:\n            torch.nn.init.zeros_(m.bias)\n\nmodel = LMS()\n\n# Novo\nmodel.apply(weights_init)\n\n\ndef lms_torch_model_3(x, d, eta, M, model):\n    N = len(x)\n    xM = torch.zeros((M, 1))\n    y = torch.zeros((N, 1))\n    losses = torch.zeros((N, 1))\n    w = torch.zeros(N + 1, M)\n\n    loss_function = nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=eta/2)\n    \n    for i in range(N):\n        xM = torch.vstack((x[i : i + 1, [0]], xM[0 : M - 1, [0]]))\n        model.zero_grad()\n        yi = model(xM)\n        loss = loss_function(yi.squeeze(), d[i].squeeze())\n        loss.backward()\n        optimizer.step()\n    \n        with torch.no_grad():            \n            y[i] = yi\n            losses[i] = loss            \n            w[i + 1, :] = model.l1.weight.clone()\n    return y, loss, w\n\n\n(y_torch_model_3, e_torch_model_3, w_torch_model_3) = lms_torch_model_3(x, d, eta, M, model)\n\n\nplot_ws(w_torch_model_3.numpy(), w_lms)"
  },
  {
    "objectID": "ex_aula_mlp_5.html",
    "href": "ex_aula_mlp_5.html",
    "title": "Exercício - MLP 5",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_mlp.html",
    "href": "t_mlp.html",
    "title": "A rede perceptron multicamada",
    "section": "",
    "text": "Apesar do perceptron de Rosenblatt ter sido proposto no final da década de 1950, o algoritmo de retropropagação (backpropagation), utilizado no treinamento das redes neurais, surgiu apenas em 1986 no artigo (Rumelhart, Hinton, e Williams 1986). Desde então, ele tem sido abordado em diferentes livros de redes neurais e aprendizado de máquina como, por exemplo, (Haykin 2009), (Bishop 2006), (Goodfellow, Bengio, e Courville 2016), (Theodoridis e Koutroubas 2003), (Witten et al. 2017), (Izenman 2008), (Duda, Hart, e Stork 2001), (Kinsley e Kukie​la 2020), (Kröse e Smagt 1996), entre muitos outros. A seguir, vamos tratar da rede percepton multicamada (multilayer perceptron - MLP) e do algoritmo backpropagation em detalhes.\n\n\nO modelo do neurônio proposto por Rosenblatt é usado até hoje nas redes neurais. A diferença está apenas na função não linear, chamada de função de ativação. No modelo de Rosenblatt, utilizou-se a função sinal que tem derivada nula em todos os pontos, exceto na origem onde não é definida. Em vez dessa função, é comum utilizar funções de ativação com derivada não nula e definida em todos os pontos, como a função sigmoidal, a tangente hiperbólica, entre outras. Essas funções permitem chegar ao algoritmo de retropropagação (backpropagation) como veremos mais adiante.\nConsiderando o \\(n\\)-ésimo vetor dos dados de treinamento\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}\n\\]\ne o vetor de pesos e bias com dimensão \\(M+1\\)\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T},\n\\]\na saída do combinador linear pode ser escrita como\n\\[\nv(n) = \\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\n\\]\ne a saída do neurônio é dada por\n\\[\ny(n)=\\varphi(v(n)),\n\\]\nem que \\(\\varphi(\\cdot)\\) é a função de ativação. Cabe observar que nesta formulação, o bias aparece na primeira posição do vetor \\(\\mathbf{w}\\) e por isso, o vetor de entrada \\(\\mathbf{x}\\) tem sempre o valor \\(1\\) em sua primeira posição. O diagrama de fluxo de sinal do neurônio está mostrado na Figura 1.\n\n\n\n\n\n\nFigura 1: Fluxo de sinal do modelo de neurônio.\n\n\n\nO neurônio da Figura 1 é a unidade básica da rede perceptron multicamada (multilayer perceptron - MLP). Na maior parte das aplicações, um único neurônio não é suficiente para se obter bons resultados. Dessa forma, a rede MLP organiza os neurônios em camadas, havendo uma ou mais camadas ocultas (hidden layers) e a camada de saída1. As camadas ocultas são camadas intermediárias entre a entrada e a saída da rede neural. Cada neurônio pertencente a uma camada oculta possui sinapses associadas a pesos, ligando-o a todos os neurônios da camada anterior. No caso da primeira camada oculta, em cada neurônio, as \\(N_0\\triangleq M\\) entradas da rede são ponderadas pelos pesos, que junto com o bias, permitem o cálculo da combinação linear, representada na Figura 1 por \\(v(n)\\). Na Figura 2, esquematizamos o fluxo de sinal de uma rede MLP com \\(L=3\\) camadas, em que cada círculo representa um neurônio. Denotando o número de neurônios da camada \\(j\\) por \\(N_{j}\\), é conveniente introduzir uma notação para a configuração da rede. Assim, uma rede MLP de 3 camadas apresenta configuração \\(N_0\\)-\\(N_1(\\varphi_1)\\)-\\(N_2(\\varphi_2)\\)-\\(N_3(\\varphi_3)\\). Apesar de ser possível utilizar diferentes funções de ativação em uma mesma camada, é comum considerar a mesma função para todos os neurônios de uma dada camada, o que é contemplado pela notação que acabamos de introduzir. Especificamente, a rede da Figura 2 tem configuração \\(3\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(2(\\text{sgm})\\), ou seja, \\(N_0=3\\) entradas, \\(N_1=3\\) neurônios na primeira camada oculta, \\(N_2=2\\) neurônios na segunda camada oculta e \\(N_3=2\\) neurônios na camada de saída. Os neurônios das camadas ocultas têm como função de ativação a tangente hiperbólica (tanh) e os de saída, a função sigmoidal (sgm), ou seja, \\(\\varphi_1=\\varphi_2=\\text{tanh}\\) e \\(\\varphi_3=\\text{sgm}\\). Também vamos denotar o número de neurônios da camada de saída por \\(N_L\\). Assim, no exemplo, \\(N_L=N_3=2\\).\n\n\n\n\n\n\nFigura 2: Representação de uma rede MLP com \\(L=3\\) camadas (\\(L-1=2\\) camadas ocultas) em que cada círculo representa um neurônio.\n\n\n\nVamos usar o exemplo da rede MLP da Figura 2 para calcular a saída da camada oculta 2, cujos pesos estão indicados. É importante atentar para a notação: \\(\\omega_{k\\ell}^{(j)}\\) representa o peso que liga o neurônio \\(\\ell\\) da camada \\(j-1\\) ao neurônio \\(k\\) da camada \\(j\\). Se o neurônio \\(k\\) pertencer à primeira camada oculta, devemos trocar “neurônio \\(\\ell\\)” na frase anterior por “entrada \\(\\ell\\)”. Voltando à Figura 2, observe que a entrada da rede é dada pelo vetor\n\\[\n[x_{1n}\\;\\; x_{2n}\\;\\;\\cdots\\;\\; x_{N_0n}]^{\\rm T},\n\\]\nque corresponde ao \\(n\\)-ésimo dado do conjunto de treinamento, em que \\(N_0\\) é o número de entradas (\\(N_0=3\\) no exemplo). É conveniente usar a notação \\(y_k^{(0)}\\triangleq x_{kn}\\), \\(k=1,2,\\ldots, N_0\\) para denotar os elementos do vetor de entrada. Dessa forma, podemos definir o vetor de entrada da rede como\n\\[\n\\mathbf{y}^{(0)} \\triangleq \\begin{bmatrix}\n                        y^{(0)}_1 \\\\\n                        y^{(0)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(0)}_{N_0}\n                        \\end{bmatrix}.\n\\]\nPara simplificar a notação, o índice \\(n\\) relacionado ao \\(n\\)-ésimo dado de treinamento não será levado em conta nesta seção. Em cada neurônio da primeira camada oculta, esse vetor é ponderado e somado ao bias. A saída de cada neurônio é então calculada aplicando-se a função de ativação \\(\\varphi_1(\\cdot)\\) ao resultado dessa combinação linear. Reunindo as saídas dos neurônios da primeira camada oculta em um vetor, temos\n\\[\n\\mathbf{y}^{(1)} = \\begin{bmatrix}\n                        y^{(1)}_1 \\\\\n                        y^{(1)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(1)}_{N_{1}}\n                        \\end{bmatrix},\n\\]\nem que \\(N_1\\) representa o número de neurônios dessa camada (\\(N_1=3\\) no exemplo). Para levar em conta os biases e obter uma formulação matricial, é conveniente definir\n\\[\n\\mathbf{x}^{(2)}=\\left[\\begin{array}{c}\n                           1 \\\\\n                           \\mathbf{y}^{(1)}\n                         \\end{array}\n\\right]\n\\]\npara denotar o vetor de entrada da camada 2. A seguir, detalhamos o cálculo do vetor de saída da camada 2, ou seja, \\(\\mathbf{y}^{(2)}\\).\nVamos reunir os biases e os pesos da camada 2 na seguinte matriz\n\\[\\begin{align*}\n\\mathbf{W}^{(2)} = \\begin{bmatrix}\n    {\\color{blue}b^{(2)}_1} & {\\color{blue}w_{11}^{(2)}}     & {\\color{blue}w_{12}^{(2)}}              & {\\color{blue}w_{13}^{(2)}}\\\\\n    {\\color{red}b^{(2)}_2} & {\\color{red}w_{21}^{(2)}}     & {\\color{red}w_{22}^{(2)}}             &  {\\color{red}w_{23}^{(2)}}\\\\\n    \\end{bmatrix}_{2\\times 4}\n\\triangleq \\begin{bmatrix}\n    {\\color{blue}\\mathbf{w}_{1}^{(2)}} \\\\ {\\color{red}\\mathbf{w}_{2}^{(2)}}\\\\\n    \\end{bmatrix}.\n\\end{align*}\\]\nO bias e os pesos em azul da primeira linha da matriz \\(\\mathbf{W}^{(2)}\\), que correspondem aos elementos do vetor linha \\({\\mathbf{w}_{1}^{(2)}}\\), são utilizados no cálculo da saída do neurônio 1 dessa camada. Já o bias e os pesos em vermelho da segunda linha são utilizados no cálculo da saída do neurônio 2. Observe que, diferente da formulação do neurônio utilizada até o momento, definimos os vetores contendo o bias e os pesos de cada neurônio da camada \\(j\\), ou seja, \\(\\mathbf{w}_k^{(j)}\\), \\(k=1, 2, \\ldots, N_j\\), como vetores linha e não como vetores coluna. Dessa forma, podemos escrever\n\\[\\begin{align*}\n\\mathbf{v}^{(2)} = \\begin{bmatrix}\n                        {\\color{blue}v^{(2)}_1} \\\\\n                        {\\color{red}v^{(2)}_2}\\\\\n                        \\end{bmatrix} =\n                        \\begin{bmatrix}\n                        {\\color{blue}\\mathbf{w}_1^{(2)}}\\mathbf{x}^{(2)} \\\\\n                        {\\color{red}\\mathbf{w}_2^{(2)}}\\mathbf{x}^{(2)}\\\\\n                        \\end{bmatrix}\n                         =\\mathbf{W}^{(2)}\\mathbf{x}^{(2)}\n                                        \\;\\;\\textnormal{e}\\;\\;\n\\mathbf{y}^{(2)} = \\varphi_2(\\mathbf{v}^{(2)})=\\begin{bmatrix}\n                        {\\color{blue}\\varphi_2(v^{(2)}_1)} \\\\\n                        {\\color{red}\\varphi_2(v^{(2)}_2)}\\\\\n                        \\end{bmatrix}= \\begin{bmatrix}\n                        {\\color{blue}y^{(2)}_1} \\\\\n                        {\\color{red}y^{(2)}_2} \\\\\n                        \\end{bmatrix}.\n\\end{align*}\\]\nGeneralizando, temos\n\\[\\begin{align*}\n\\mathbf{v}^{(j)} \\triangleq \\begin{bmatrix}\n                        v^{(j)}_1 \\\\\n                        v^{(j)}_2 \\\\\n                        \\vdots \\\\\n                        v^{(j)}_{N_{j}}\n                        \\end{bmatrix},\\;\\;\\;\n\\mathbf{y}^{(j)} \\triangleq  \\begin{bmatrix}\n                        y^{(j)}_1 \\\\\n                        y^{(j)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(j)}_{N_{j}}\n                        \\end{bmatrix}\n                        \\;\\; \\text{e} \\;\\;\n\\mathbf{x}^{(j)}\\triangleq \\begin{bmatrix}\n                        1 \\\\\n                        \\mathbf{y}^{(j-1)}\n                        \\end{bmatrix},             \n\\end{align*}\\]\nem que \\(j=1, 2, \\ldots, L\\), \\(L\\) é o número de camadas da rede e \\(\\mathbf{y}^{(0)}\\) o vetor de entradas da rede. Definindo agora a matriz de pesos associados à camada \\(j\\) como\n\\[\\begin{align*}\n\\mathbf{W}^{(j)} \\triangleq \\begin{bmatrix}\n    b^{(j)}_1 & w_{11}^{(j)}     & w_{12}^{(j)}             & \\dots  & w_{1N_{j - 1}}^{(j)}\\\\\n    b^{(j)}_2 & w_{21}^{(j)}     & w_{22}^{(j)}             & \\dots  & w_{2N_{j - 1}}^{(j)}\\\\\n    \\vdots           &  \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n    b_{N_j}^{(j)} & w_{N_{j}1}^{(j)} & w_{N_{j}2}^{(j)}         & \\dots  & w_{N_{j}N_{j - 1}}^{(j)}\n    \\end{bmatrix}_{(N_{j})\\times (N_{j - 1}+1)}\n= \\begin{bmatrix}\n    \\mathbf{w}_{1}^{(j)} \\\\ \\mathbf{w}_{2}^{(j)} \\\\ \\vdots \\\\ \\mathbf{w}_{N_{j}}^{(j)}\\\\\n    \\end{bmatrix},\n\\end{align*}\\]\no vetor de saída dos combinadores lineares da camada \\(j\\) é dado por\n\\[\\begin{equation*}\n    \\mathbf{v}^{(j)} = \\left[\\begin{array}{c}\n                               \\mathbf{w}_1^{(j)}\\mathbf{x}^{(j)} \\\\\n                               \\mathbf{w}_2^{(j)}\\mathbf{x}^{(j)} \\\\\n                               \\vdots \\\\\n                               \\mathbf{w}_{N_j}^{(j)}\\mathbf{x}^{(j)}\n                             \\end{array}\n    \\right]= \\mathbf{W}^{(j)}\\mathbf{x}^{(j)}.\n\\end{equation*}\\]\nPor fim, o vetor de saída dessa camada é calculado como\n\\[\\begin{equation*}\n    \\mathbf{y}^{(j)} = \\varphi_j\\left(\\mathbf{v}^{(j)}\\right),\n\\end{equation*}\\]\nem que a função de ativação \\(\\varphi_j\\left(\\cdot\\right)\\) é aplicada a cada elemento do vetor \\(\\mathbf{v}^{(j)}\\).\nO cálculo apresentado até aqui é utilizado para obter as saídas dos neurônios de cada camada da rede. Como o cálculo das saídas da camada \\(j\\) depende das saídas da camada \\((j-1)\\), dizemos que o cálculo é progressivo. Uma vez calculada a saída da rede, podemos compará-la com o sinal desejado e utilizar esse resultado para atualizar os pesos e os biases a fim de minimizar uma função custo, como veremos a seguir.\n\n\n\nO algoritmo de retropropagação (backpropagation) é o mais utilizado no processo de aprendizado supervisionado das redes neurais. Ele é dividido em duas etapas, descritas a seguir.\n\n\nNessa etapa, os pesos e biases são mantidos fixos e o cálculo é realizado progressivamente até se obter o vetor de saída \\(\\textbf{y}^{(L)}\\). Nesse cálculo, a entrada é propagada ao longo da rede, camada por camada, como detalhado na seção anterior.\n\n\n\nNessa etapa, os pesos e biases são atualizados com o objetivo de minimizar uma função custo. Apesar de existirem diferentes funções custo, vamos nos concentrar por ora apenas no erro quadrático médio (MSE - mean square error), definido como\n\\[\nJ_{\\rm MSE}=\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}e^2_\\ell(n)\n\\]\nem que\n\\[\ne_\\ell(n)=d_\\ell(n)-y_\\ell^{(L)}(n)\n\\]\nsão os erros dos neurônios da camada de saída da rede. Para simplificar a dedução, vamos considerar o modo de treinamento estocástico em que os pesos e biases são atualizados a cada dado de treinamento \\(n=1, 2, \\ldots, N_t.\\)\nUtilizando o método do gradiente estocástico, a matriz de pesos da camada \\(j\\) pode ser atualizada como\n\\[\\begin{equation*}\n  \\textbf{W}^{(j)}(n) = \\textbf{W}^{(j)}(n-1) - \\eta \\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n-1)},\n\\end{equation*}\\]\nem que \\(\\eta\\) é um passo de adaptação e\n\\[\n\\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n\\!-\\!1)}= \\begin{bmatrix}\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_1(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{11}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{12}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{1N_{j - 1}}^{(j)}(n\\!-\\!1)}\\\\\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_2(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{21}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{22}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial  w_{2N_{j - 1}}^{(j)}(n\\!-\\!1)}\\\\\n    \\vdots           &  \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n    \\frac{\\partial J_{MSE}}{\\partial  b_{N_j}^{(j)}(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial  w_{N_{j}1}^{(j)}(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{N_{j}2}^{(j)}(n\\!-\\!1)}         & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{N_{j}N_{j - 1}}^{(j)}(n\\!-\\!1)}\n    \\end{bmatrix}.\n\\]\nObserve que na \\(k\\)-ésima linha dessa matriz, temos o vetor gradiente2\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(j)}}J_{\\rm MSE}=\n\\frac{\\partial J_{\\rm MSE}}{\\partial \\mathbf{w}_k^{(j)}(n-1)}=\n\\begin{bmatrix}\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_k(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{k1}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{k2}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{kN_{j - 1}}^{(j)}(n\\!-\\!1)}\\end{bmatrix}.\n\\]\nAssim, podemos escrever\n\\[\n\\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n\\!-\\!1)}=\\begin{bmatrix} \\boldsymbol{\\nabla}_{\\mathbf{w}_1^{(j)}}J_{MSE} \\\\ \\boldsymbol{\\nabla}_{\\mathbf{w}_2^{(j)}}J_{MSE} \\\\ \\vdots \\\\ \\boldsymbol{\\nabla}_{\\mathbf{w}_{N_j}^{(j)}}J_{MSE} \\end{bmatrix}.\n\\]\nVamos calcular os vetores gradientes considerando os neurônios da camada de saída, ou seja, \\(j=L\\). Usando a regra da cadeia sucessivas vezes, obtemos\n\\[\\begin{equation*}\n        \\begin{aligned}[b]\n            \\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L)}}J_{MSE}& = \\frac{\\partial J_{\\rm MSE}}{\\partial \\textbf{w}_k^{(L)}(n-1)}=\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}\\frac{\\partial e_{\\ell}^2(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)}=\\frac{1}{N_L}\\frac{\\partial e_{k}^2(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)}\\\\\n             &= \\frac{1}{N_L}\\;\\frac{\\partial e_k^2(n)}{\\partial y_k^{(L)}(n)}\\; \\frac{\\partial y_k^{(L)}(n)}{\\partial v^{(L)}_k(n)}\\; \\frac{\\partial v^{(L)}_k(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)} \\\\\n            & = \\frac{1}{N_L}\\; 2 e_k(n)\\; \\frac{\\partial [d_k(n)-y_k^{(L)}(n)]}{\\partial y_k^{(L)}(n)}\\; \\frac{\\partial \\varphi_L\\left(v^{(L)}_k(n)\\right)}{\\partial v^{(L)}_k(n)}\\; \\frac{\\partial \\textbf{w}^{(L)}_k(n-1)\\,\\textbf{x}^{(L)}(n)}{\\partial \\textbf{w}^{(L)}_k(n-1)} \\\\\n            & = -\\frac{2}{N_L}\\; e_k(n)\\; \\varphi_L'\\!\\left(v^{(L)}_k(n)\\right) \\; [\\textbf{x}^{(L)}(n)]^{\\rm T},\n        \\end{aligned}\\nonumber\n        \\label{chain_rule_w}\n  \\end{equation*}\\]\nem que \\(\\varphi_L'(\\cdot)\\) representa a derivada da função de ativação \\(\\varphi_L(\\cdot)\\), o que justifica a importância dessa função ser derivável em todos os pontos.\nDefinindo o gradiente local da camada \\(L\\) como\n\\[\n  \\delta_{k}^{(L)}(n)\\triangleq \\varphi_L'(v_{k}^{(L)}(n))e_{k}(n),\n  \\]\no vetor gradiente pode ser reescrito como\n\\[\n  \\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L)}}J_{MSE}= -\\frac{2}{N_L}\\; \\delta_{k}^{(L)}(n) \\; [\\textbf{x}^{(L)}(n)]^{\\rm T}.\n\\]\nUma vez calculados os gradientes da camada de saída \\(L\\), podemos calcular os gradientes da última camada oculta, ou seja, para \\(j=L-1\\). Assim,\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} = \\frac{\\partial J_{\\rm MSE}}{\\partial \\mathbf{w}_k^{(L-1)}(n-1)}= \\frac{1}{N_L}\\;\\sum_{\\ell=1}^{N_L}\\frac{\\partial  e_\\ell^2(n)}{\\partial \\mathbf{w}_k^{(L-1)}(n-1)}.\n\\]\nNovamente, usando a regra da cadeia sucessivas vezes, obtemos\n\\[\\begin{align*}\n\\frac{\\partial  e_\\ell^2(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}&=2 e_{\\ell}(n)\\frac{\\partial  [d_{\\ell}(n)-y_{\\ell}^{(L)}(n)]}{\\partial y_{\\ell}^{(L)}(n)}\n\\frac{\\partial y_{\\ell}^{(L)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\frac{\\partial \\varphi_L(v_{\\ell}^{(L)}(n))}{\\partial v_{\\ell}^{(L)}(n)}\\frac{\\partial v_{\\ell}^{(L)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}.\n\\end{align*}\\]\nObserve que\n\\[\nv_{\\ell}^{(L)}(n)=\\mathbf{w}_{\\ell}^{(L)}(n-1)\\mathbf{x}^{(L)}(n)=b_{\\ell}^{(L)}(n-1)+\\sum_{m=1}^{N_{L-1}}w_{\\ell m}^{(L)}(n-1)y_m^{(L-1)}(n).\n\\]\nNo cálculo de \\(v_{\\ell}^{(L)}(n)\\), o único termo que depende de \\(\\textbf{w}_k^{(L-1)}(n-1)\\) é \\(w_{\\ell k}^{(L)}(n-1)y_k^{(L-1)}(n)\\), já que a saída \\(y_k^{(L-1)}(n)\\) é calculada utilizando os pesos \\(\\textbf{w}_k^{(L-1)}(n-1)\\). Assim, obtemos\n\\[\\begin{align*}\n\\frac{\\partial  e_\\ell^2(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\frac{\\partial y_{k}^{(L-1)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\frac{\\partial \\varphi_{L-1}(v_{k}^{(L-1)}(n))}{\\partial v_{k}^{(L-1)}(n)}\\frac{\\partial v_{k}^{(L-1)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\frac{\\partial \\mathbf{w}_{k}^{(L-1)}(n-1)\\mathbf{x}^{(L-1)}(n)} {\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\varphi_{L-1}'(v_{k}^{(L-1)}(n))[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\end{align*}\\]\nIdentificando \\(\\delta^{(L)}_{\\ell}(n)\\) na expressão anterior e substituindo o resultado na expressão do gradiente, obtém-se\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} =  -\\frac{2}{N_L}\\;\\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\;\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1)[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\]\nDefinindo agora o gradiente local da camada \\(L-1\\) como\n\\[\n\\delta_{k}^{(L-1)}(n)\\triangleq \\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\;\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1),\n\\]\no vetor gradiente pode ser reescrito como\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} =  -\\frac{2}{N_L}\\delta_{k}^{(L-1)}(n)[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\]\nComparando a expressão do gradiente local \\(\\delta_{k}^{(L-1)}(n)\\) da camada \\(L-1\\) com a expressão do gradiente local \\(\\delta_{k}^{(L)}(n)\\) da camada \\(L\\), o somatório\n\\[\n\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1)\n\\]\nfaz o papel de erro do neurônio \\(k\\) da camada \\(L-1\\). Essa retropropagação dos erros deve continuar até a primeira camada oculta. O fluxo do sinal na retropropagação para as camadas \\(L\\) e \\(L-1\\) está esquematizado na Figura 3. O erro do neurônio \\(k\\) da camada \\(L-1\\) é o sinal obtido no ponto indicado pelo círculo azul na figura.\n\n\n\n\n\n\nFigura 3: Fluxo do sinal na retropropagação considerando as camadas \\(L\\) e \\(L-1\\).\n\n\n\nGeneralizando, define-se o gradiente local para qualquer camada oculta \\(j\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\delta_{k}^{(j)}(n)\\triangleq \\varphi_j'(v_{k}^{(j)}(n))\\;\\sum_{\\ell=1}^{N_{j+1}}\\delta_{\\ell}^{(j+1)}(n)w_{\\ell k}^{(j+1)}(n-1)\n$}\n\\end{equation*}\\]\ne para a camada de saída \\(L\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\delta_{k}^{(L)}(n)\\triangleq \\varphi_L'(v_{k}^{(L)}(n))e_k(n).\n$}\n\\end{equation*}\\]\nDefinindo os vetores\n\\[\n\\boldsymbol{\\delta}^{(j)}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    \\delta_{1}^{(j)}(n) \\\\\n                                    \\delta_{2}^{(j)}(n) \\\\\n                                    \\vdots \\\\\n                                    \\delta_{N_j}^{(j)}(n)\n                                  \\end{array}\n\\right],\\;\\;\\;\n\\mathbf{e}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    e_{1}(n) \\\\\n                                    e_{2}(n) \\\\\n                                    \\vdots \\\\\n                                    e_{N_L}(n)\n                                  \\end{array}\\right],\\;\\;\\;\n\\mathbf{d}_{\\varphi}^{(j)}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    \\varphi_j'(v_1^{(j)}(n)) \\\\\n                                    \\varphi_j'(v_2^{(j)}(n)) \\\\\n                                    \\vdots \\\\\n                                    \\varphi_j'(v_{N_j}^{(j)}(n))\n                                  \\end{array}\\right]\n\\]\ne a matriz \\(\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\) excluíndo a coluna de biases da matriz \\({\\mathbf{W}}^{(j+1)}(n-1)\\), ou seja,\n\\[\n\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\triangleq \\begin{bmatrix}\n     w_{11}^{(j+1)}(n-1)     & w_{12}^{(j+1)}(n-1)            & \\dots  & w_{1N_{j}}^{(j+1)}(n-1)\\\\\n     w_{21}^{(j+1)}(n-1)     & w_{22}^{(j+1)}(n-1)            & \\dots  & w_{2N_{j}}^{(j+1)}(n-1)\\\\\n      \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n     w_{N_{j+1}1}^{(j+1)}(n-1) & w_{N_{j+1}2}^{(j+1)}(n-1)         & \\dots  & w_{N_{j+1}N_{j}}^{(j+1)}(n-1)\n    \\end{bmatrix}_{N_{j+1}\\times N_{j}},\n\\] podemos escrever para a camada de saída\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\boldsymbol{\\delta}^{(L)}(n)=\\nonumber\\mathbf{d}_{\\varphi}^{(L)}(n)\\odot\\mathbf{e}(n)\n$}\n\\end{equation*}\\]\ne para as camadas ocultas\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\boldsymbol{\\delta}^{(j)}(n)=\\nonumber\\mathbf{d}_{\\varphi}^{(j)}(n)\\odot\\left\\{\\left[\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\right]^{\\rm T}\\;\\boldsymbol{\\delta}^{(j+1)}(n)\\right\\}\\nonumber\n$}\n\\end{equation*}\\]\nem que \\(\\odot\\) representa a multiplicação elemento por elemento entre dois vetores. Essa forma de calcular os vetores de gradientes locais é mais eficiente, já que todos os elementos são calculados de uma vez em cada camada .\nÉ comum incorporar a constante \\(2/N_L\\) que aparece nos cálculos dos gradientes ao passo de adaptação \\(\\eta\\). Dessa forma, as equações de atualização dos vetores de pesos do neurônio \\(k\\) da camada \\(j\\) podem ser escritas como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}_{k}^{(j)}(n)=\\mathbf{w}_{k}^{(j)}(n-1)+\\eta\\delta_{k}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}\n$}\n\\end{equation*}\\]\n\\(k=1, 2, \\ldots, N_j\\;\\;,\\) \\(j=1, 2 \\ldots, L\\). Definindo a matriz\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}\n\\]\npodemos atualizar a matriz de pesos da camada \\(j\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n).\n$}\n\\end{equation*}\\]\nComo a implementação matricial é mais eficiente, essa forma de atualização é preferida. Os pesos e biases precisam ser inicializados. No LMS, esses parâmetros são geralmente inicializados com zero. No entanto, se fizermos o mesmo no algoritmo backpropagation, dependendo da função de ativação, esses parâmetros não serão atualizados. Para exemplificar, vamos considerar a tangente hiperbólica como função de ativação. Essa função é definida como\n\\[\n\\varphi_j(v)={\\rm tanh}(v)=\\frac{e^v-e^{-v}}{e^v+e^{-v}}.\n\\]\nNote que se inicializarmos os pesos e biases com zero, as saídas dos combinadores lineares dos neurônios também serão iguais zero e como \\({\\rm tanh}(0)=0\\), as saídas dos neurônios e consequentemente os gradientes serão nulos. Dessa forma, não ocorre atualização dos pesos e biases. Em geral, não temos nenhuma informação prévia sobre a solução “ótima” dos pesos e biases. Por isso, costuma-se inicializar esses parâmetros a partir de uma distribuição uniforme. O intervalo da distribuição depende do problema. Podemos considerar, por exemplo, valores uniformemente distribuídos no intervalo \\([-10^{-2}, 10^{-2}]\\).\nDiferente do que acontece no algoritmo LMS, a função custo minimizada pelo algoritmo backpropagation tem inúmeros mínimos locais devido às não linearidades inseridas pelas funções de ativação. Suponha hipoteticamente que temos apenas dois parâmetros \\(w_0\\) e \\(w_1\\) a serem ajustados. Na Figura 4 (a) temos o MSE a ser minimizado pelo LMS, que apresenta um único ponto de mínimo, que é a solução de Wiener como vimos anteriormente. Já na Figura 4 (b), temos uma função custo com inúmeros mínimos locais. Não temos mais uma função convexa e o gradiente é nulo nesses pontos de mínimo. Isso faz com que o algoritmo pare de atualizar e atinja uma solução subótima, que por sua vez, pode estar muito distante do mínimo global da função custo. Vários fatores fazem com que o algoritmo backpropagation pare em um mínimo local: o passo de adaptação, a inicialização, determinadas funções de ativação, etc. A solução subótima nem sempre é adequada. Por isso, várias soluções para fazer com que o algoritmo saia dos mínimos locais foram propostas na literatura como veremos posteriormente.\n\n\n\n\n\n\nFigura 4: a) Função custo do MSE a ser minimizada pelo LMS; b) Função custo do MSE, a ser minimizada pelo backpropagation [Fonte].\n\n\n\nNo Algoritmo 1, é apresentado o pseudocódigo do algoritmo backpropagation no modo de treinamento estocástico. Apesar da dedução ter sido feita neste modo, ele raramente é utilizado de forma estocástica. Em vez disso, é mais comum considerar os modos de treinamento batch e mini-batch. Como fizemos a formulação desses modos no algoritmo LMS, sua extensão para o backpropagation é direta e deixaremos a cargo do leitor.\n\n\nExemplo 1 Sumário do algoritmo backpropagation para treinamento da rede MLP no modo estocástico. \\(N_t\\) é o número de dados de treinamento.\nInicialização: as matrizes \\(\\mathbf{W}^{(j)}(0),\\;j=1,2,\\ldots, L\\) devem ser inicializadas com números aleatórios uniformemente distribuídos    Para \\(n=1,2,\\ldots,\\) calcule:      Cálculo progressivo      \\(\\mathbf{y}^{(0)} = [y_{1}\\; y_{2}\\;\\cdots\\; y_{N_0}]^{\\rm T} = [x_{1n}\\; x_{2n}\\;\\cdots\\; x_{N_0n}]^{\\rm T}\\)      Para \\(j=1,2,\\ldots, L,\\) calcule:        \\(\\mathbf{x}^{(j)}(n)=\\left[\\begin{array}{c}\n                                                      1 \\\\\n                                                      \\mathbf{y}^{(j-1)}(n)\\\\\n                                                    \\end{array}\n                                      \\right]\\)        \\(\\mathbf{v}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)\\mathbf{x}^{(j)}(n)\\)        \\(\\mathbf{y}^{(j)}(n)=\\varphi_j(\\mathbf{v}^{(j)}(n))\\)        \\(\\mathbf{d}_{\\varphi}^{(j)}(n)=\\varphi_j'(\\mathbf{v}^{(j)}(n))\\)      Fim      \\(\\mathbf{e}(n)=\\mathbf{d}(n)-\\mathbf{y}^{(L)}(n)\\)      Cálculo regressivo      Para \\(j=L,L-1,\\ldots, 1,\\) calcule:        Se \\(j=L\\)          \\(\\boldsymbol{\\delta}^{(j)}(n)=\\mathbf{d}_{\\varphi}^{(L)}(n)\\odot \\mathbf{e}(n)\\)        Caso contrário          \\(\\boldsymbol{\\delta}^{(j)}(n)=\\mathbf{d}_{\\varphi}^{(j)}(n)\\odot\\left\\{\\left[\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\right]^{\\rm T}\\;\\boldsymbol{\\delta}^{(j+1)}(n)\\right\\}\\)        Fim        \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n) [\\mathbf{x}^{(j)}(n)]^{T}\\)        \\(\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+ \\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\)      Fim    Fim\n\n\n\n\n\n\nVamos voltar ao exemplo de classificação das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\). Vimos que essa situação exigia uma separação não linear que tanto o algoritmo LMS quanto o perceptron de Rosenblatt não são capazes de fornecer. Vamos considerar agora a solução obtida por uma rede MLP com a seguinte configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) no modo de treinamento mini-batch (\\(N_0=2\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\) e o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!1\\). Considerou-se ainda a função custo do erro quadrático médio (MSE).\nNa Figura 5 são mostradas a função custo ao longo das épocas, a classificação dos dados de teste e a separação das regiões. Observa-se que a função custo não terminou de convergir apesar das \\(N_e=10^4\\) épocas. No entanto, o valor que ela atinge na última época é de aproximadamente \\(J_{\\rm MSE}\\approx 9,7 \\times 10^{-6}\\), o que corresponde a \\(-50,\\!1~\\text{dB}\\). Caso não tenha ocorrido overfitting, esse valor é baixo o suficiente para conseguir separar adequadamente as regiões. Para comprovar, foram gerados \\(N_{\\rm teste}=2000\\) dados de teste que foram classificados pela MLP considerando os pesos e os biases da última iteração. Podemos observar na figura que não há erros de classificação (taxa de erro igual a zero) e a separação obtida é não linear, como esperado.\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nPara ilustrar a evolução da fronteira de decisão ao longo das épocas, a classificação dos dados de teste e a separação das regiões são mostradas na Figura 6 para diferentes valores de \\(N_e\\). Para cada caso, os pesos obtidos na época em questão foram considerados fixos e usados para classificação dos dados de teste. As respectivas taxas de erro estão mostradas na Tabela 1. Nota-se que \\(N_e=5\\) épocas é muito pouco para que a MLP consiga classificar os dados. A medida que o número de épocas aumenta, a separação vai tomando uma forma mais adequada, o que faz com que a taxa de erros diminua, como esperado. Para \\(N_e=400\\), a taxa de erros é de 1,65% e para \\(N_e=500\\) não há mais erros de classificação. Diante disso, \\(500\\) épocas são suficientes para que a MLP consiga classificar corretamente os dados.\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões ao longo das épocas obtidas com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\n\n\n\nTabela 1: Taxas de erro das classificações da Figura 6.\n\n\n\n\n\n\\(N_e\\)\nTaxa de erro (%)\n\n\n\n\n5\n50,00\n\n\n9\n17,05\n\n\n50\n11,95\n\n\n200\n10,65\n\n\n250\n10,08\n\n\n300\n9,00\n\n\n400\n1,65\n\n\n500\n0,00\n\n\n\n\n\n\nApesar do excelente resultado da Figura 5, nenhuma técnica foi utilizada para fazer com que o algoritmo backpropagation não ficasse parado em mínimos locais. Ao inicializar os pesos e biases considerando a mesma distribuição mas sorteando valores diferentes, é possível que o algoritmo fique parado em mínimos locais que levam a soluções subótimas. Essa situação pode ser observada na Figura 7. Observa-se que a função custo atinge aproximadamente \\(J_{\\rm MSE}\\approx 0,18\\), o que corresponde a \\(-7,4\\) dB. Apesar da separação não linear, há vários pontos da Região A (azul) classificados erroneamente como pertencentes à Região B (vermelha), o que leva a uma taxa de erros de \\(7,\\!8\\%\\).\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nA conclusão desse exemplo é que a rede MLP é capaz de proporcionar soluções não lineares que podem ser adequadas a vários problemas de classificação e regressão. No entanto, é importante utilizar técnicas que façam com que o algoritmo backpropagation não fique parado em mínimos locais. Antes de discutirmos essas técnicas, vamos abordar a seguir o Teorema de Cybenko que diz que a MLP é um aproximador universal de funções.\n\n\n\nUma rede MLP treinada com o algoritmo backpropagation pode ser vista como um sistema capaz de realizar um mapeamento entrada-saída de forma não linear. Considere uma rede MLP com \\(N_0\\) entradas e \\(N_L\\) saídas. A relação entrada-saída da rede define um mapeamento de um espaço Euclidiano de entrada de dimensão \\(N_0\\) a um espaço Euclidiano de saída de dimensão \\(N_L\\), que é infinitamente e continuamente diferenciável desde que a função de ativação também seja. Neste contexto, cabe a seguinte pergunta: qual o número mínimo de camadas ocultas que a rede MLP precisa ter para fornecer uma aproximação de qualquer mapeamento contínuo? A resposta para essa pergunta envolve o Teorema da Aproximação Universal, enunciado a seguir.\n\n\n\n\n\n\nImportanteTeorema da Aproximação Universal\n\n\n\nSeja \\(\\varphi(\\cdot)\\) uma função contínua, não constante, limitada e monotônica crescente. Vamos utilizar \\(I_{N_0}\\) para denotar o hipercubo unitário \\([0,\\;1]^{N_0}\\) de dimensão \\(N_0\\). O espaço de funções contínuas em \\(I_{N_0}\\) é denotado por \\(C(I_{N_0})\\). Então, dada qualquer função \\(f \\in C(I_{N_0})\\) e \\(\\varepsilon&gt;0\\), existe um inteiro \\(N_1\\) e conjuntos de constantes reais \\(\\alpha_i\\), \\(b_i\\) e \\(w_{ij}\\), \\(i=1, 2, \\ldots, N_1\\) e \\(j=1, 2, \\ldots,N_0\\) tal que se pode definir\n\\[\nF(x_1, x_2, \\ldots, x_{N_0})=\\displaystyle\\sum_{i=1}^{N_1}\\alpha_i\\varphi\\left(\\displaystyle\\sum_{j=1}^{N_0}w_{ij}x_j+b_i\\right)\n\\]\ncomo uma aproximação da função \\(f(\\cdot)\\), ou seja,\n\\[\n|F(x_1, x_2, \\ldots, x_{N_0})-f(x_1, x_2, \\ldots, x_{N_0})|&lt;\\varepsilon\n\\]\npara todos \\(x_1, x_2, \\ldots, x_{N_0}\\) do espaço de entrada.\n\n\nO Teorema da Aproximação Universal é diretamente aplicável à rede MLP. Primeiramente, cabe observar que a tangente hiperbólica, comumente usada como função de ativação, é não constante, limitada e monotonicamente crescente. Portanto, ela satisfaz as condições impostas para a função \\(\\varphi(\\cdot)\\). Além disso,\n\\[\n\\displaystyle\\sum_{i=1}^{N_1}\\alpha_i\\varphi\\left(\\displaystyle\\sum_{j=1}^{N_0}w_{ij}x_j+b_i\\right)\n\\]\nrepresenta a saída de uma MLP descrita a seguir:\n\na rede possui \\(N_0\\) entradas, indicadas por \\(x_1, x_2, \\ldots, x_{N_0}\\), e uma única camada oculta composta por \\(N_1\\) neurônios;\no neurônio oculto \\(i\\) tem pesos \\(w_{i1}, w_{i2}, \\ldots, w_{iN_0}\\) e bias \\(b_i\\);\na saída da rede é uma combinação linear das saídas dos neurônios ocultos, com \\(\\alpha_1\\), \\(\\alpha_2, \\ldots,\\) \\(\\alpha_{N_1}\\) sendo os pesos da saída.\n\nA partir desse teorema, pode-se afirmar que uma única camada oculta é suficiente para que uma rede MLP obtenha uma aproximação uniforme para um determinado conjunto de treinamento, representado pelas entradas \\(x_1, x_2, \\ldots, x_{N_0}\\), e uma saída desejada \\(f(x_1, x_2, \\ldots, x_{N_0})\\). Por isso, as redes MLP são conhecidas como aproximadores universais de funções. Esse resultado foi demonstrado pela primeira vez por Cybenko em 1988 e por isso, também é chamado de Teorema de Cybenko na literatura de redes neurais. Apesar desse resultado interessante, o teorema não nos diz que uma única camada oculta é ótima no sentido de tempo de aprendizado, simplicidade de implementação ou capacidade de generalização. Mais detalhes podem ser encontrados, por exemplo, em (Haykin 2009)."
  },
  {
    "objectID": "t_mlp.html#o-modelo-da-rede-e-o-cálculo-progressivo",
    "href": "t_mlp.html#o-modelo-da-rede-e-o-cálculo-progressivo",
    "title": "A rede perceptron multicamada",
    "section": "",
    "text": "O modelo do neurônio proposto por Rosenblatt é usado até hoje nas redes neurais. A diferença está apenas na função não linear, chamada de função de ativação. No modelo de Rosenblatt, utilizou-se a função sinal que tem derivada nula em todos os pontos, exceto na origem onde não é definida. Em vez dessa função, é comum utilizar funções de ativação com derivada não nula e definida em todos os pontos, como a função sigmoidal, a tangente hiperbólica, entre outras. Essas funções permitem chegar ao algoritmo de retropropagação (backpropagation) como veremos mais adiante.\nConsiderando o \\(n\\)-ésimo vetor dos dados de treinamento\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}\n\\]\ne o vetor de pesos e bias com dimensão \\(M+1\\)\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T},\n\\]\na saída do combinador linear pode ser escrita como\n\\[\nv(n) = \\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\n\\]\ne a saída do neurônio é dada por\n\\[\ny(n)=\\varphi(v(n)),\n\\]\nem que \\(\\varphi(\\cdot)\\) é a função de ativação. Cabe observar que nesta formulação, o bias aparece na primeira posição do vetor \\(\\mathbf{w}\\) e por isso, o vetor de entrada \\(\\mathbf{x}\\) tem sempre o valor \\(1\\) em sua primeira posição. O diagrama de fluxo de sinal do neurônio está mostrado na Figura 1.\n\n\n\n\n\n\nFigura 1: Fluxo de sinal do modelo de neurônio.\n\n\n\nO neurônio da Figura 1 é a unidade básica da rede perceptron multicamada (multilayer perceptron - MLP). Na maior parte das aplicações, um único neurônio não é suficiente para se obter bons resultados. Dessa forma, a rede MLP organiza os neurônios em camadas, havendo uma ou mais camadas ocultas (hidden layers) e a camada de saída1. As camadas ocultas são camadas intermediárias entre a entrada e a saída da rede neural. Cada neurônio pertencente a uma camada oculta possui sinapses associadas a pesos, ligando-o a todos os neurônios da camada anterior. No caso da primeira camada oculta, em cada neurônio, as \\(N_0\\triangleq M\\) entradas da rede são ponderadas pelos pesos, que junto com o bias, permitem o cálculo da combinação linear, representada na Figura 1 por \\(v(n)\\). Na Figura 2, esquematizamos o fluxo de sinal de uma rede MLP com \\(L=3\\) camadas, em que cada círculo representa um neurônio. Denotando o número de neurônios da camada \\(j\\) por \\(N_{j}\\), é conveniente introduzir uma notação para a configuração da rede. Assim, uma rede MLP de 3 camadas apresenta configuração \\(N_0\\)-\\(N_1(\\varphi_1)\\)-\\(N_2(\\varphi_2)\\)-\\(N_3(\\varphi_3)\\). Apesar de ser possível utilizar diferentes funções de ativação em uma mesma camada, é comum considerar a mesma função para todos os neurônios de uma dada camada, o que é contemplado pela notação que acabamos de introduzir. Especificamente, a rede da Figura 2 tem configuração \\(3\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(2(\\text{sgm})\\), ou seja, \\(N_0=3\\) entradas, \\(N_1=3\\) neurônios na primeira camada oculta, \\(N_2=2\\) neurônios na segunda camada oculta e \\(N_3=2\\) neurônios na camada de saída. Os neurônios das camadas ocultas têm como função de ativação a tangente hiperbólica (tanh) e os de saída, a função sigmoidal (sgm), ou seja, \\(\\varphi_1=\\varphi_2=\\text{tanh}\\) e \\(\\varphi_3=\\text{sgm}\\). Também vamos denotar o número de neurônios da camada de saída por \\(N_L\\). Assim, no exemplo, \\(N_L=N_3=2\\).\n\n\n\n\n\n\nFigura 2: Representação de uma rede MLP com \\(L=3\\) camadas (\\(L-1=2\\) camadas ocultas) em que cada círculo representa um neurônio.\n\n\n\nVamos usar o exemplo da rede MLP da Figura 2 para calcular a saída da camada oculta 2, cujos pesos estão indicados. É importante atentar para a notação: \\(\\omega_{k\\ell}^{(j)}\\) representa o peso que liga o neurônio \\(\\ell\\) da camada \\(j-1\\) ao neurônio \\(k\\) da camada \\(j\\). Se o neurônio \\(k\\) pertencer à primeira camada oculta, devemos trocar “neurônio \\(\\ell\\)” na frase anterior por “entrada \\(\\ell\\)”. Voltando à Figura 2, observe que a entrada da rede é dada pelo vetor\n\\[\n[x_{1n}\\;\\; x_{2n}\\;\\;\\cdots\\;\\; x_{N_0n}]^{\\rm T},\n\\]\nque corresponde ao \\(n\\)-ésimo dado do conjunto de treinamento, em que \\(N_0\\) é o número de entradas (\\(N_0=3\\) no exemplo). É conveniente usar a notação \\(y_k^{(0)}\\triangleq x_{kn}\\), \\(k=1,2,\\ldots, N_0\\) para denotar os elementos do vetor de entrada. Dessa forma, podemos definir o vetor de entrada da rede como\n\\[\n\\mathbf{y}^{(0)} \\triangleq \\begin{bmatrix}\n                        y^{(0)}_1 \\\\\n                        y^{(0)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(0)}_{N_0}\n                        \\end{bmatrix}.\n\\]\nPara simplificar a notação, o índice \\(n\\) relacionado ao \\(n\\)-ésimo dado de treinamento não será levado em conta nesta seção. Em cada neurônio da primeira camada oculta, esse vetor é ponderado e somado ao bias. A saída de cada neurônio é então calculada aplicando-se a função de ativação \\(\\varphi_1(\\cdot)\\) ao resultado dessa combinação linear. Reunindo as saídas dos neurônios da primeira camada oculta em um vetor, temos\n\\[\n\\mathbf{y}^{(1)} = \\begin{bmatrix}\n                        y^{(1)}_1 \\\\\n                        y^{(1)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(1)}_{N_{1}}\n                        \\end{bmatrix},\n\\]\nem que \\(N_1\\) representa o número de neurônios dessa camada (\\(N_1=3\\) no exemplo). Para levar em conta os biases e obter uma formulação matricial, é conveniente definir\n\\[\n\\mathbf{x}^{(2)}=\\left[\\begin{array}{c}\n                           1 \\\\\n                           \\mathbf{y}^{(1)}\n                         \\end{array}\n\\right]\n\\]\npara denotar o vetor de entrada da camada 2. A seguir, detalhamos o cálculo do vetor de saída da camada 2, ou seja, \\(\\mathbf{y}^{(2)}\\).\nVamos reunir os biases e os pesos da camada 2 na seguinte matriz\n\\[\\begin{align*}\n\\mathbf{W}^{(2)} = \\begin{bmatrix}\n    {\\color{blue}b^{(2)}_1} & {\\color{blue}w_{11}^{(2)}}     & {\\color{blue}w_{12}^{(2)}}              & {\\color{blue}w_{13}^{(2)}}\\\\\n    {\\color{red}b^{(2)}_2} & {\\color{red}w_{21}^{(2)}}     & {\\color{red}w_{22}^{(2)}}             &  {\\color{red}w_{23}^{(2)}}\\\\\n    \\end{bmatrix}_{2\\times 4}\n\\triangleq \\begin{bmatrix}\n    {\\color{blue}\\mathbf{w}_{1}^{(2)}} \\\\ {\\color{red}\\mathbf{w}_{2}^{(2)}}\\\\\n    \\end{bmatrix}.\n\\end{align*}\\]\nO bias e os pesos em azul da primeira linha da matriz \\(\\mathbf{W}^{(2)}\\), que correspondem aos elementos do vetor linha \\({\\mathbf{w}_{1}^{(2)}}\\), são utilizados no cálculo da saída do neurônio 1 dessa camada. Já o bias e os pesos em vermelho da segunda linha são utilizados no cálculo da saída do neurônio 2. Observe que, diferente da formulação do neurônio utilizada até o momento, definimos os vetores contendo o bias e os pesos de cada neurônio da camada \\(j\\), ou seja, \\(\\mathbf{w}_k^{(j)}\\), \\(k=1, 2, \\ldots, N_j\\), como vetores linha e não como vetores coluna. Dessa forma, podemos escrever\n\\[\\begin{align*}\n\\mathbf{v}^{(2)} = \\begin{bmatrix}\n                        {\\color{blue}v^{(2)}_1} \\\\\n                        {\\color{red}v^{(2)}_2}\\\\\n                        \\end{bmatrix} =\n                        \\begin{bmatrix}\n                        {\\color{blue}\\mathbf{w}_1^{(2)}}\\mathbf{x}^{(2)} \\\\\n                        {\\color{red}\\mathbf{w}_2^{(2)}}\\mathbf{x}^{(2)}\\\\\n                        \\end{bmatrix}\n                         =\\mathbf{W}^{(2)}\\mathbf{x}^{(2)}\n                                        \\;\\;\\textnormal{e}\\;\\;\n\\mathbf{y}^{(2)} = \\varphi_2(\\mathbf{v}^{(2)})=\\begin{bmatrix}\n                        {\\color{blue}\\varphi_2(v^{(2)}_1)} \\\\\n                        {\\color{red}\\varphi_2(v^{(2)}_2)}\\\\\n                        \\end{bmatrix}= \\begin{bmatrix}\n                        {\\color{blue}y^{(2)}_1} \\\\\n                        {\\color{red}y^{(2)}_2} \\\\\n                        \\end{bmatrix}.\n\\end{align*}\\]\nGeneralizando, temos\n\\[\\begin{align*}\n\\mathbf{v}^{(j)} \\triangleq \\begin{bmatrix}\n                        v^{(j)}_1 \\\\\n                        v^{(j)}_2 \\\\\n                        \\vdots \\\\\n                        v^{(j)}_{N_{j}}\n                        \\end{bmatrix},\\;\\;\\;\n\\mathbf{y}^{(j)} \\triangleq  \\begin{bmatrix}\n                        y^{(j)}_1 \\\\\n                        y^{(j)}_2 \\\\\n                        \\vdots \\\\\n                        y^{(j)}_{N_{j}}\n                        \\end{bmatrix}\n                        \\;\\; \\text{e} \\;\\;\n\\mathbf{x}^{(j)}\\triangleq \\begin{bmatrix}\n                        1 \\\\\n                        \\mathbf{y}^{(j-1)}\n                        \\end{bmatrix},             \n\\end{align*}\\]\nem que \\(j=1, 2, \\ldots, L\\), \\(L\\) é o número de camadas da rede e \\(\\mathbf{y}^{(0)}\\) o vetor de entradas da rede. Definindo agora a matriz de pesos associados à camada \\(j\\) como\n\\[\\begin{align*}\n\\mathbf{W}^{(j)} \\triangleq \\begin{bmatrix}\n    b^{(j)}_1 & w_{11}^{(j)}     & w_{12}^{(j)}             & \\dots  & w_{1N_{j - 1}}^{(j)}\\\\\n    b^{(j)}_2 & w_{21}^{(j)}     & w_{22}^{(j)}             & \\dots  & w_{2N_{j - 1}}^{(j)}\\\\\n    \\vdots           &  \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n    b_{N_j}^{(j)} & w_{N_{j}1}^{(j)} & w_{N_{j}2}^{(j)}         & \\dots  & w_{N_{j}N_{j - 1}}^{(j)}\n    \\end{bmatrix}_{(N_{j})\\times (N_{j - 1}+1)}\n= \\begin{bmatrix}\n    \\mathbf{w}_{1}^{(j)} \\\\ \\mathbf{w}_{2}^{(j)} \\\\ \\vdots \\\\ \\mathbf{w}_{N_{j}}^{(j)}\\\\\n    \\end{bmatrix},\n\\end{align*}\\]\no vetor de saída dos combinadores lineares da camada \\(j\\) é dado por\n\\[\\begin{equation*}\n    \\mathbf{v}^{(j)} = \\left[\\begin{array}{c}\n                               \\mathbf{w}_1^{(j)}\\mathbf{x}^{(j)} \\\\\n                               \\mathbf{w}_2^{(j)}\\mathbf{x}^{(j)} \\\\\n                               \\vdots \\\\\n                               \\mathbf{w}_{N_j}^{(j)}\\mathbf{x}^{(j)}\n                             \\end{array}\n    \\right]= \\mathbf{W}^{(j)}\\mathbf{x}^{(j)}.\n\\end{equation*}\\]\nPor fim, o vetor de saída dessa camada é calculado como\n\\[\\begin{equation*}\n    \\mathbf{y}^{(j)} = \\varphi_j\\left(\\mathbf{v}^{(j)}\\right),\n\\end{equation*}\\]\nem que a função de ativação \\(\\varphi_j\\left(\\cdot\\right)\\) é aplicada a cada elemento do vetor \\(\\mathbf{v}^{(j)}\\).\nO cálculo apresentado até aqui é utilizado para obter as saídas dos neurônios de cada camada da rede. Como o cálculo das saídas da camada \\(j\\) depende das saídas da camada \\((j-1)\\), dizemos que o cálculo é progressivo. Uma vez calculada a saída da rede, podemos compará-la com o sinal desejado e utilizar esse resultado para atualizar os pesos e os biases a fim de minimizar uma função custo, como veremos a seguir."
  },
  {
    "objectID": "t_mlp.html#o-algoritmo-de-retropropagação",
    "href": "t_mlp.html#o-algoritmo-de-retropropagação",
    "title": "A rede perceptron multicamada",
    "section": "",
    "text": "O algoritmo de retropropagação (backpropagation) é o mais utilizado no processo de aprendizado supervisionado das redes neurais. Ele é dividido em duas etapas, descritas a seguir.\n\n\nNessa etapa, os pesos e biases são mantidos fixos e o cálculo é realizado progressivamente até se obter o vetor de saída \\(\\textbf{y}^{(L)}\\). Nesse cálculo, a entrada é propagada ao longo da rede, camada por camada, como detalhado na seção anterior.\n\n\n\nNessa etapa, os pesos e biases são atualizados com o objetivo de minimizar uma função custo. Apesar de existirem diferentes funções custo, vamos nos concentrar por ora apenas no erro quadrático médio (MSE - mean square error), definido como\n\\[\nJ_{\\rm MSE}=\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}e^2_\\ell(n)\n\\]\nem que\n\\[\ne_\\ell(n)=d_\\ell(n)-y_\\ell^{(L)}(n)\n\\]\nsão os erros dos neurônios da camada de saída da rede. Para simplificar a dedução, vamos considerar o modo de treinamento estocástico em que os pesos e biases são atualizados a cada dado de treinamento \\(n=1, 2, \\ldots, N_t.\\)\nUtilizando o método do gradiente estocástico, a matriz de pesos da camada \\(j\\) pode ser atualizada como\n\\[\\begin{equation*}\n  \\textbf{W}^{(j)}(n) = \\textbf{W}^{(j)}(n-1) - \\eta \\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n-1)},\n\\end{equation*}\\]\nem que \\(\\eta\\) é um passo de adaptação e\n\\[\n\\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n\\!-\\!1)}= \\begin{bmatrix}\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_1(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{11}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{12}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{1N_{j - 1}}^{(j)}(n\\!-\\!1)}\\\\\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_2(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{21}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{22}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial  w_{2N_{j - 1}}^{(j)}(n\\!-\\!1)}\\\\\n    \\vdots           &  \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n    \\frac{\\partial J_{MSE}}{\\partial  b_{N_j}^{(j)}(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial  w_{N_{j}1}^{(j)}(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{N_{j}2}^{(j)}(n\\!-\\!1)}         & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{N_{j}N_{j - 1}}^{(j)}(n\\!-\\!1)}\n    \\end{bmatrix}.\n\\]\nObserve que na \\(k\\)-ésima linha dessa matriz, temos o vetor gradiente2\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(j)}}J_{\\rm MSE}=\n\\frac{\\partial J_{\\rm MSE}}{\\partial \\mathbf{w}_k^{(j)}(n-1)}=\n\\begin{bmatrix}\n    \\frac{\\partial J_{MSE}}{\\partial b^{(j)}_k(n\\!-\\!1)} & \\frac{\\partial J_{MSE}}{\\partial w_{k1}^{(j)}(n\\!-\\!1)}     & \\frac{\\partial J_{MSE}}{\\partial w_{k2}^{(j)}(n\\!-\\!1)}             & \\dots  & \\frac{\\partial J_{MSE}}{\\partial w_{kN_{j - 1}}^{(j)}(n\\!-\\!1)}\\end{bmatrix}.\n\\]\nAssim, podemos escrever\n\\[\n\\frac{\\partial J_{MSE}}{\\partial \\textbf{W}^{(j)}(n\\!-\\!1)}=\\begin{bmatrix} \\boldsymbol{\\nabla}_{\\mathbf{w}_1^{(j)}}J_{MSE} \\\\ \\boldsymbol{\\nabla}_{\\mathbf{w}_2^{(j)}}J_{MSE} \\\\ \\vdots \\\\ \\boldsymbol{\\nabla}_{\\mathbf{w}_{N_j}^{(j)}}J_{MSE} \\end{bmatrix}.\n\\]\nVamos calcular os vetores gradientes considerando os neurônios da camada de saída, ou seja, \\(j=L\\). Usando a regra da cadeia sucessivas vezes, obtemos\n\\[\\begin{equation*}\n        \\begin{aligned}[b]\n            \\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L)}}J_{MSE}& = \\frac{\\partial J_{\\rm MSE}}{\\partial \\textbf{w}_k^{(L)}(n-1)}=\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}\\frac{\\partial e_{\\ell}^2(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)}=\\frac{1}{N_L}\\frac{\\partial e_{k}^2(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)}\\\\\n             &= \\frac{1}{N_L}\\;\\frac{\\partial e_k^2(n)}{\\partial y_k^{(L)}(n)}\\; \\frac{\\partial y_k^{(L)}(n)}{\\partial v^{(L)}_k(n)}\\; \\frac{\\partial v^{(L)}_k(n)}{\\partial \\textbf{w}_k^{(L)}(n-1)} \\\\\n            & = \\frac{1}{N_L}\\; 2 e_k(n)\\; \\frac{\\partial [d_k(n)-y_k^{(L)}(n)]}{\\partial y_k^{(L)}(n)}\\; \\frac{\\partial \\varphi_L\\left(v^{(L)}_k(n)\\right)}{\\partial v^{(L)}_k(n)}\\; \\frac{\\partial \\textbf{w}^{(L)}_k(n-1)\\,\\textbf{x}^{(L)}(n)}{\\partial \\textbf{w}^{(L)}_k(n-1)} \\\\\n            & = -\\frac{2}{N_L}\\; e_k(n)\\; \\varphi_L'\\!\\left(v^{(L)}_k(n)\\right) \\; [\\textbf{x}^{(L)}(n)]^{\\rm T},\n        \\end{aligned}\\nonumber\n        \\label{chain_rule_w}\n  \\end{equation*}\\]\nem que \\(\\varphi_L'(\\cdot)\\) representa a derivada da função de ativação \\(\\varphi_L(\\cdot)\\), o que justifica a importância dessa função ser derivável em todos os pontos.\nDefinindo o gradiente local da camada \\(L\\) como\n\\[\n  \\delta_{k}^{(L)}(n)\\triangleq \\varphi_L'(v_{k}^{(L)}(n))e_{k}(n),\n  \\]\no vetor gradiente pode ser reescrito como\n\\[\n  \\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L)}}J_{MSE}= -\\frac{2}{N_L}\\; \\delta_{k}^{(L)}(n) \\; [\\textbf{x}^{(L)}(n)]^{\\rm T}.\n\\]\nUma vez calculados os gradientes da camada de saída \\(L\\), podemos calcular os gradientes da última camada oculta, ou seja, para \\(j=L-1\\). Assim,\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} = \\frac{\\partial J_{\\rm MSE}}{\\partial \\mathbf{w}_k^{(L-1)}(n-1)}= \\frac{1}{N_L}\\;\\sum_{\\ell=1}^{N_L}\\frac{\\partial  e_\\ell^2(n)}{\\partial \\mathbf{w}_k^{(L-1)}(n-1)}.\n\\]\nNovamente, usando a regra da cadeia sucessivas vezes, obtemos\n\\[\\begin{align*}\n\\frac{\\partial  e_\\ell^2(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}&=2 e_{\\ell}(n)\\frac{\\partial  [d_{\\ell}(n)-y_{\\ell}^{(L)}(n)]}{\\partial y_{\\ell}^{(L)}(n)}\n\\frac{\\partial y_{\\ell}^{(L)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\frac{\\partial \\varphi_L(v_{\\ell}^{(L)}(n))}{\\partial v_{\\ell}^{(L)}(n)}\\frac{\\partial v_{\\ell}^{(L)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}.\n\\end{align*}\\]\nObserve que\n\\[\nv_{\\ell}^{(L)}(n)=\\mathbf{w}_{\\ell}^{(L)}(n-1)\\mathbf{x}^{(L)}(n)=b_{\\ell}^{(L)}(n-1)+\\sum_{m=1}^{N_{L-1}}w_{\\ell m}^{(L)}(n-1)y_m^{(L-1)}(n).\n\\]\nNo cálculo de \\(v_{\\ell}^{(L)}(n)\\), o único termo que depende de \\(\\textbf{w}_k^{(L-1)}(n-1)\\) é \\(w_{\\ell k}^{(L)}(n-1)y_k^{(L-1)}(n)\\), já que a saída \\(y_k^{(L-1)}(n)\\) é calculada utilizando os pesos \\(\\textbf{w}_k^{(L-1)}(n-1)\\). Assim, obtemos\n\\[\\begin{align*}\n\\frac{\\partial  e_\\ell^2(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\frac{\\partial y_{k}^{(L-1)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\frac{\\partial \\varphi_{L-1}(v_{k}^{(L-1)}(n))}{\\partial v_{k}^{(L-1)}(n)}\\frac{\\partial v_{k}^{(L-1)}(n)}{\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\frac{\\partial \\mathbf{w}_{k}^{(L-1)}(n-1)\\mathbf{x}^{(L-1)}(n)} {\\partial \\textbf{w}_k^{(L-1)}(n-1)}\\\\\n&=-2e_{\\ell}(n)\\varphi_L'(v_{\\ell}^{(L)}(n))w_{\\ell k}^{(L)}(n-1)\\varphi_{L-1}'(v_{k}^{(L-1)}(n))[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\end{align*}\\]\nIdentificando \\(\\delta^{(L)}_{\\ell}(n)\\) na expressão anterior e substituindo o resultado na expressão do gradiente, obtém-se\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} =  -\\frac{2}{N_L}\\;\\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\;\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1)[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\]\nDefinindo agora o gradiente local da camada \\(L-1\\) como\n\\[\n\\delta_{k}^{(L-1)}(n)\\triangleq \\varphi_{L-1}'(v_{k}^{(L-1)}(n))\\;\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1),\n\\]\no vetor gradiente pode ser reescrito como\n\\[\n\\boldsymbol{\\nabla}_{\\mathbf{w}_k^{(L-1)}}J_{\\rm MSE} =  -\\frac{2}{N_L}\\delta_{k}^{(L-1)}(n)[\\mathbf{x}^{(L-1)}(n)]^{\\rm T}.\n\\]\nComparando a expressão do gradiente local \\(\\delta_{k}^{(L-1)}(n)\\) da camada \\(L-1\\) com a expressão do gradiente local \\(\\delta_{k}^{(L)}(n)\\) da camada \\(L\\), o somatório\n\\[\n\\sum_{\\ell=1}^{N_L}\\delta_{\\ell}^{(L)}(n)w_{\\ell k}^{(L)}(n-1)\n\\]\nfaz o papel de erro do neurônio \\(k\\) da camada \\(L-1\\). Essa retropropagação dos erros deve continuar até a primeira camada oculta. O fluxo do sinal na retropropagação para as camadas \\(L\\) e \\(L-1\\) está esquematizado na Figura 3. O erro do neurônio \\(k\\) da camada \\(L-1\\) é o sinal obtido no ponto indicado pelo círculo azul na figura.\n\n\n\n\n\n\nFigura 3: Fluxo do sinal na retropropagação considerando as camadas \\(L\\) e \\(L-1\\).\n\n\n\nGeneralizando, define-se o gradiente local para qualquer camada oculta \\(j\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\delta_{k}^{(j)}(n)\\triangleq \\varphi_j'(v_{k}^{(j)}(n))\\;\\sum_{\\ell=1}^{N_{j+1}}\\delta_{\\ell}^{(j+1)}(n)w_{\\ell k}^{(j+1)}(n-1)\n$}\n\\end{equation*}\\]\ne para a camada de saída \\(L\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\delta_{k}^{(L)}(n)\\triangleq \\varphi_L'(v_{k}^{(L)}(n))e_k(n).\n$}\n\\end{equation*}\\]\nDefinindo os vetores\n\\[\n\\boldsymbol{\\delta}^{(j)}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    \\delta_{1}^{(j)}(n) \\\\\n                                    \\delta_{2}^{(j)}(n) \\\\\n                                    \\vdots \\\\\n                                    \\delta_{N_j}^{(j)}(n)\n                                  \\end{array}\n\\right],\\;\\;\\;\n\\mathbf{e}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    e_{1}(n) \\\\\n                                    e_{2}(n) \\\\\n                                    \\vdots \\\\\n                                    e_{N_L}(n)\n                                  \\end{array}\\right],\\;\\;\\;\n\\mathbf{d}_{\\varphi}^{(j)}(n)\\triangleq\n\\left[\\begin{array}{c}\n                                    \\varphi_j'(v_1^{(j)}(n)) \\\\\n                                    \\varphi_j'(v_2^{(j)}(n)) \\\\\n                                    \\vdots \\\\\n                                    \\varphi_j'(v_{N_j}^{(j)}(n))\n                                  \\end{array}\\right]\n\\]\ne a matriz \\(\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\) excluíndo a coluna de biases da matriz \\({\\mathbf{W}}^{(j+1)}(n-1)\\), ou seja,\n\\[\n\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\triangleq \\begin{bmatrix}\n     w_{11}^{(j+1)}(n-1)     & w_{12}^{(j+1)}(n-1)            & \\dots  & w_{1N_{j}}^{(j+1)}(n-1)\\\\\n     w_{21}^{(j+1)}(n-1)     & w_{22}^{(j+1)}(n-1)            & \\dots  & w_{2N_{j}}^{(j+1)}(n-1)\\\\\n      \\vdots           & \\vdots                   & \\ddots & \\vdots\\\\\n     w_{N_{j+1}1}^{(j+1)}(n-1) & w_{N_{j+1}2}^{(j+1)}(n-1)         & \\dots  & w_{N_{j+1}N_{j}}^{(j+1)}(n-1)\n    \\end{bmatrix}_{N_{j+1}\\times N_{j}},\n\\] podemos escrever para a camada de saída\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\boldsymbol{\\delta}^{(L)}(n)=\\nonumber\\mathbf{d}_{\\varphi}^{(L)}(n)\\odot\\mathbf{e}(n)\n$}\n\\end{equation*}\\]\ne para as camadas ocultas\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\boldsymbol{\\delta}^{(j)}(n)=\\nonumber\\mathbf{d}_{\\varphi}^{(j)}(n)\\odot\\left\\{\\left[\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\right]^{\\rm T}\\;\\boldsymbol{\\delta}^{(j+1)}(n)\\right\\}\\nonumber\n$}\n\\end{equation*}\\]\nem que \\(\\odot\\) representa a multiplicação elemento por elemento entre dois vetores. Essa forma de calcular os vetores de gradientes locais é mais eficiente, já que todos os elementos são calculados de uma vez em cada camada .\nÉ comum incorporar a constante \\(2/N_L\\) que aparece nos cálculos dos gradientes ao passo de adaptação \\(\\eta\\). Dessa forma, as equações de atualização dos vetores de pesos do neurônio \\(k\\) da camada \\(j\\) podem ser escritas como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}_{k}^{(j)}(n)=\\mathbf{w}_{k}^{(j)}(n-1)+\\eta\\delta_{k}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}\n$}\n\\end{equation*}\\]\n\\(k=1, 2, \\ldots, N_j\\;\\;,\\) \\(j=1, 2 \\ldots, L\\). Definindo a matriz\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}\n\\]\npodemos atualizar a matriz de pesos da camada \\(j\\) como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n).\n$}\n\\end{equation*}\\]\nComo a implementação matricial é mais eficiente, essa forma de atualização é preferida. Os pesos e biases precisam ser inicializados. No LMS, esses parâmetros são geralmente inicializados com zero. No entanto, se fizermos o mesmo no algoritmo backpropagation, dependendo da função de ativação, esses parâmetros não serão atualizados. Para exemplificar, vamos considerar a tangente hiperbólica como função de ativação. Essa função é definida como\n\\[\n\\varphi_j(v)={\\rm tanh}(v)=\\frac{e^v-e^{-v}}{e^v+e^{-v}}.\n\\]\nNote que se inicializarmos os pesos e biases com zero, as saídas dos combinadores lineares dos neurônios também serão iguais zero e como \\({\\rm tanh}(0)=0\\), as saídas dos neurônios e consequentemente os gradientes serão nulos. Dessa forma, não ocorre atualização dos pesos e biases. Em geral, não temos nenhuma informação prévia sobre a solução “ótima” dos pesos e biases. Por isso, costuma-se inicializar esses parâmetros a partir de uma distribuição uniforme. O intervalo da distribuição depende do problema. Podemos considerar, por exemplo, valores uniformemente distribuídos no intervalo \\([-10^{-2}, 10^{-2}]\\).\nDiferente do que acontece no algoritmo LMS, a função custo minimizada pelo algoritmo backpropagation tem inúmeros mínimos locais devido às não linearidades inseridas pelas funções de ativação. Suponha hipoteticamente que temos apenas dois parâmetros \\(w_0\\) e \\(w_1\\) a serem ajustados. Na Figura 4 (a) temos o MSE a ser minimizado pelo LMS, que apresenta um único ponto de mínimo, que é a solução de Wiener como vimos anteriormente. Já na Figura 4 (b), temos uma função custo com inúmeros mínimos locais. Não temos mais uma função convexa e o gradiente é nulo nesses pontos de mínimo. Isso faz com que o algoritmo pare de atualizar e atinja uma solução subótima, que por sua vez, pode estar muito distante do mínimo global da função custo. Vários fatores fazem com que o algoritmo backpropagation pare em um mínimo local: o passo de adaptação, a inicialização, determinadas funções de ativação, etc. A solução subótima nem sempre é adequada. Por isso, várias soluções para fazer com que o algoritmo saia dos mínimos locais foram propostas na literatura como veremos posteriormente.\n\n\n\n\n\n\nFigura 4: a) Função custo do MSE a ser minimizada pelo LMS; b) Função custo do MSE, a ser minimizada pelo backpropagation [Fonte].\n\n\n\nNo Algoritmo 1, é apresentado o pseudocódigo do algoritmo backpropagation no modo de treinamento estocástico. Apesar da dedução ter sido feita neste modo, ele raramente é utilizado de forma estocástica. Em vez disso, é mais comum considerar os modos de treinamento batch e mini-batch. Como fizemos a formulação desses modos no algoritmo LMS, sua extensão para o backpropagation é direta e deixaremos a cargo do leitor.\n\n\nExemplo 1 Sumário do algoritmo backpropagation para treinamento da rede MLP no modo estocástico. \\(N_t\\) é o número de dados de treinamento.\nInicialização: as matrizes \\(\\mathbf{W}^{(j)}(0),\\;j=1,2,\\ldots, L\\) devem ser inicializadas com números aleatórios uniformemente distribuídos    Para \\(n=1,2,\\ldots,\\) calcule:      Cálculo progressivo      \\(\\mathbf{y}^{(0)} = [y_{1}\\; y_{2}\\;\\cdots\\; y_{N_0}]^{\\rm T} = [x_{1n}\\; x_{2n}\\;\\cdots\\; x_{N_0n}]^{\\rm T}\\)      Para \\(j=1,2,\\ldots, L,\\) calcule:        \\(\\mathbf{x}^{(j)}(n)=\\left[\\begin{array}{c}\n                                                      1 \\\\\n                                                      \\mathbf{y}^{(j-1)}(n)\\\\\n                                                    \\end{array}\n                                      \\right]\\)        \\(\\mathbf{v}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)\\mathbf{x}^{(j)}(n)\\)        \\(\\mathbf{y}^{(j)}(n)=\\varphi_j(\\mathbf{v}^{(j)}(n))\\)        \\(\\mathbf{d}_{\\varphi}^{(j)}(n)=\\varphi_j'(\\mathbf{v}^{(j)}(n))\\)      Fim      \\(\\mathbf{e}(n)=\\mathbf{d}(n)-\\mathbf{y}^{(L)}(n)\\)      Cálculo regressivo      Para \\(j=L,L-1,\\ldots, 1,\\) calcule:        Se \\(j=L\\)          \\(\\boldsymbol{\\delta}^{(j)}(n)=\\mathbf{d}_{\\varphi}^{(L)}(n)\\odot \\mathbf{e}(n)\\)        Caso contrário          \\(\\boldsymbol{\\delta}^{(j)}(n)=\\mathbf{d}_{\\varphi}^{(j)}(n)\\odot\\left\\{\\left[\\overline{\\mathbf{W}}^{(j+1)}(n-1)\\right]^{\\rm T}\\;\\boldsymbol{\\delta}^{(j+1)}(n)\\right\\}\\)        Fim        \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n) [\\mathbf{x}^{(j)}(n)]^{T}\\)        \\(\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+ \\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\)      Fim    Fim"
  },
  {
    "objectID": "t_mlp.html#utilizando-a-rede-mlp-no-problema-das-meias-luas",
    "href": "t_mlp.html#utilizando-a-rede-mlp-no-problema-das-meias-luas",
    "title": "A rede perceptron multicamada",
    "section": "",
    "text": "Vamos voltar ao exemplo de classificação das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\). Vimos que essa situação exigia uma separação não linear que tanto o algoritmo LMS quanto o perceptron de Rosenblatt não são capazes de fornecer. Vamos considerar agora a solução obtida por uma rede MLP com a seguinte configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) no modo de treinamento mini-batch (\\(N_0=2\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\) e o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!1\\). Considerou-se ainda a função custo do erro quadrático médio (MSE).\nNa Figura 5 são mostradas a função custo ao longo das épocas, a classificação dos dados de teste e a separação das regiões. Observa-se que a função custo não terminou de convergir apesar das \\(N_e=10^4\\) épocas. No entanto, o valor que ela atinge na última época é de aproximadamente \\(J_{\\rm MSE}\\approx 9,7 \\times 10^{-6}\\), o que corresponde a \\(-50,\\!1~\\text{dB}\\). Caso não tenha ocorrido overfitting, esse valor é baixo o suficiente para conseguir separar adequadamente as regiões. Para comprovar, foram gerados \\(N_{\\rm teste}=2000\\) dados de teste que foram classificados pela MLP considerando os pesos e os biases da última iteração. Podemos observar na figura que não há erros de classificação (taxa de erro igual a zero) e a separação obtida é não linear, como esperado.\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nPara ilustrar a evolução da fronteira de decisão ao longo das épocas, a classificação dos dados de teste e a separação das regiões são mostradas na Figura 6 para diferentes valores de \\(N_e\\). Para cada caso, os pesos obtidos na época em questão foram considerados fixos e usados para classificação dos dados de teste. As respectivas taxas de erro estão mostradas na Tabela 1. Nota-se que \\(N_e=5\\) épocas é muito pouco para que a MLP consiga classificar os dados. A medida que o número de épocas aumenta, a separação vai tomando uma forma mais adequada, o que faz com que a taxa de erros diminua, como esperado. Para \\(N_e=400\\), a taxa de erros é de 1,65% e para \\(N_e=500\\) não há mais erros de classificação. Diante disso, \\(500\\) épocas são suficientes para que a MLP consiga classificar corretamente os dados.\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões ao longo das épocas obtidas com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\n\n\n\nTabela 1: Taxas de erro das classificações da Figura 6.\n\n\n\n\n\n\\(N_e\\)\nTaxa de erro (%)\n\n\n\n\n5\n50,00\n\n\n9\n17,05\n\n\n50\n11,95\n\n\n200\n10,65\n\n\n250\n10,08\n\n\n300\n9,00\n\n\n400\n1,65\n\n\n500\n0,00\n\n\n\n\n\n\nApesar do excelente resultado da Figura 5, nenhuma técnica foi utilizada para fazer com que o algoritmo backpropagation não ficasse parado em mínimos locais. Ao inicializar os pesos e biases considerando a mesma distribuição mas sorteando valores diferentes, é possível que o algoritmo fique parado em mínimos locais que levam a soluções subótimas. Essa situação pode ser observada na Figura 7. Observa-se que a função custo atinge aproximadamente \\(J_{\\rm MSE}\\approx 0,18\\), o que corresponde a \\(-7,4\\) dB. Apesar da separação não linear, há vários pontos da Região A (azul) classificados erroneamente como pertencentes à Região B (vermelha), o que leva a uma taxa de erros de \\(7,\\!8\\%\\).\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com uma rede MLP (\\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\)) treinada em mini-batch com o algoritmo backpropagation (\\(N_0=2\\), \\(\\eta=0,1\\), \\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=10^4\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nA conclusão desse exemplo é que a rede MLP é capaz de proporcionar soluções não lineares que podem ser adequadas a vários problemas de classificação e regressão. No entanto, é importante utilizar técnicas que façam com que o algoritmo backpropagation não fique parado em mínimos locais. Antes de discutirmos essas técnicas, vamos abordar a seguir o Teorema de Cybenko que diz que a MLP é um aproximador universal de funções."
  },
  {
    "objectID": "t_mlp.html#mlp-como-aproximador-universal-de-funções",
    "href": "t_mlp.html#mlp-como-aproximador-universal-de-funções",
    "title": "A rede perceptron multicamada",
    "section": "",
    "text": "Uma rede MLP treinada com o algoritmo backpropagation pode ser vista como um sistema capaz de realizar um mapeamento entrada-saída de forma não linear. Considere uma rede MLP com \\(N_0\\) entradas e \\(N_L\\) saídas. A relação entrada-saída da rede define um mapeamento de um espaço Euclidiano de entrada de dimensão \\(N_0\\) a um espaço Euclidiano de saída de dimensão \\(N_L\\), que é infinitamente e continuamente diferenciável desde que a função de ativação também seja. Neste contexto, cabe a seguinte pergunta: qual o número mínimo de camadas ocultas que a rede MLP precisa ter para fornecer uma aproximação de qualquer mapeamento contínuo? A resposta para essa pergunta envolve o Teorema da Aproximação Universal, enunciado a seguir.\n\n\n\n\n\n\nImportanteTeorema da Aproximação Universal\n\n\n\nSeja \\(\\varphi(\\cdot)\\) uma função contínua, não constante, limitada e monotônica crescente. Vamos utilizar \\(I_{N_0}\\) para denotar o hipercubo unitário \\([0,\\;1]^{N_0}\\) de dimensão \\(N_0\\). O espaço de funções contínuas em \\(I_{N_0}\\) é denotado por \\(C(I_{N_0})\\). Então, dada qualquer função \\(f \\in C(I_{N_0})\\) e \\(\\varepsilon&gt;0\\), existe um inteiro \\(N_1\\) e conjuntos de constantes reais \\(\\alpha_i\\), \\(b_i\\) e \\(w_{ij}\\), \\(i=1, 2, \\ldots, N_1\\) e \\(j=1, 2, \\ldots,N_0\\) tal que se pode definir\n\\[\nF(x_1, x_2, \\ldots, x_{N_0})=\\displaystyle\\sum_{i=1}^{N_1}\\alpha_i\\varphi\\left(\\displaystyle\\sum_{j=1}^{N_0}w_{ij}x_j+b_i\\right)\n\\]\ncomo uma aproximação da função \\(f(\\cdot)\\), ou seja,\n\\[\n|F(x_1, x_2, \\ldots, x_{N_0})-f(x_1, x_2, \\ldots, x_{N_0})|&lt;\\varepsilon\n\\]\npara todos \\(x_1, x_2, \\ldots, x_{N_0}\\) do espaço de entrada.\n\n\nO Teorema da Aproximação Universal é diretamente aplicável à rede MLP. Primeiramente, cabe observar que a tangente hiperbólica, comumente usada como função de ativação, é não constante, limitada e monotonicamente crescente. Portanto, ela satisfaz as condições impostas para a função \\(\\varphi(\\cdot)\\). Além disso,\n\\[\n\\displaystyle\\sum_{i=1}^{N_1}\\alpha_i\\varphi\\left(\\displaystyle\\sum_{j=1}^{N_0}w_{ij}x_j+b_i\\right)\n\\]\nrepresenta a saída de uma MLP descrita a seguir:\n\na rede possui \\(N_0\\) entradas, indicadas por \\(x_1, x_2, \\ldots, x_{N_0}\\), e uma única camada oculta composta por \\(N_1\\) neurônios;\no neurônio oculto \\(i\\) tem pesos \\(w_{i1}, w_{i2}, \\ldots, w_{iN_0}\\) e bias \\(b_i\\);\na saída da rede é uma combinação linear das saídas dos neurônios ocultos, com \\(\\alpha_1\\), \\(\\alpha_2, \\ldots,\\) \\(\\alpha_{N_1}\\) sendo os pesos da saída.\n\nA partir desse teorema, pode-se afirmar que uma única camada oculta é suficiente para que uma rede MLP obtenha uma aproximação uniforme para um determinado conjunto de treinamento, representado pelas entradas \\(x_1, x_2, \\ldots, x_{N_0}\\), e uma saída desejada \\(f(x_1, x_2, \\ldots, x_{N_0})\\). Por isso, as redes MLP são conhecidas como aproximadores universais de funções. Esse resultado foi demonstrado pela primeira vez por Cybenko em 1988 e por isso, também é chamado de Teorema de Cybenko na literatura de redes neurais. Apesar desse resultado interessante, o teorema não nos diz que uma única camada oculta é ótima no sentido de tempo de aprendizado, simplicidade de implementação ou capacidade de generalização. Mais detalhes podem ser encontrados, por exemplo, em (Haykin 2009)."
  },
  {
    "objectID": "t_mlp.html#footnotes",
    "href": "t_mlp.html#footnotes",
    "title": "A rede perceptron multicamada",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nEm alguns livros, considera-se também a entrada como uma camada, como é o caso, por exemplo, da formulação de (Haykin 2009). Aqui, vamos considerar camadas apenas as que possuem neurônios. Dessa forma, a primeira camada da rede MLP será sempre oculta.↩︎\nComo passamos a considerar os vetores de pesos e bias como vetores linha, o vetor gradiente correspondente também é linha.↩︎"
  },
  {
    "objectID": "ex_aula_pca.html",
    "href": "ex_aula_pca.html",
    "title": "Exercício - PCA e LDA",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_mlp_pytorch.html",
    "href": "ex_aula_mlp_pytorch.html",
    "title": "Exercício - MLP com PyTorch",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "trab_final.html",
    "href": "trab_final.html",
    "title": "Instruções para a entrega do trabalho final",
    "section": "",
    "text": "Parte da avaliação consiste na apresentação de um projeto para solução de um problema prático utilizando um modelo de aprendizado de máquina baseado em rede neural MLP.\nNo dia 05/11, deverá ser entregue um plano de trabalho, que consiste em um resumo de uma página, descrevendo o problema e o banco de dados a ser utilizado.\n\nConsidere utilizar um banco de dados público como os disponíveis em https://research.google/resources/datasets/ ou https://www.kaggle.com/datasets. Busque um problema que seja complicado o suficiente para demandar um modelo não linear mas que não seja excessivamente complicado para não demandar um tempo de treinamento muito grande;\nProcure encontrar um problema menos explorado, no sentido de não ter um número grande de soluções publicadas. Leve em conta que a solução apresentada deve ser original e não deve ser baseada em modelos já publicados;\nLembre-se que o banco de dados deverá ser dividido em uma parte para treinamento e outra para teste do modelo.\n\nNo dia 02/12, deverá ser entregue um texto no formato de artigo, com 5 páginas, que não será publicado, juntamente com o código desenvolvido para o projeto.\n\nO texto deve descrever o problema, o banco de dados, o modelo utilizado, as métricas utilizadas para medida de desempenho, os resultados obtidos e as conclusões;\nO texto pode ser escrito em português ou inglês;\nDevem ser incluídas justificativas para as escolhas realizadas: arquitetura do modelo, hiperparâmetros, função custo, otimizador, medidas de desempenho, etc.;\nO modelo deve obrigatoriamente consistir em uma rede neural MLP;\nNão é permitido o uso de modelos pré-treinados, ainda que seja para inicialização do treinamento (estratégia de ajuste fino);\nPara implementação do modelo e treinamento da rede neural, a sugestão é que seja utilizado um framework de redes neurais, como o PyTorch. No entanto, isso não é obrigatório, podendo ser utilizada outra solução ou mesmo outra linguagem de programação, caso desejado;\nAlém do texto, deverão ser entregues os códigos para (i) preparação do ambiente (configurações gerais, transformação dos dados, etc.) (ii) treinamento e (iii) avaliação do modelo treinado. A sugestão é que seja utilizado um Jupyter Notebook com o código e texto explicativo.\n\nO trabalho deve ser apresentado no dia 03/12, durante a aula e as apresentações devem durar 15 minutos (~12 minutos de apresentação e ~3 minutos para perguntas).\nSeguem alguns títulos de trabalhos apresentados nos anos passados:\n\nClassificação de imagens de melanomas usando uma rede MLP\nSpeed and bearing prediction of an autonomous vehicle using an MLP network\nClassificação de tipo de pavimento com MLP utilizando a base de dados PVS\nClassificação de imagens histopatológicas de câncer pulmonar utilizando MLP e PCA\nClassification of spoken digits using multi-layer perceptron networks\nDrag and lift coefficient prediction for wind propulsion\nUsing multi-layer perceptron to identify liver disease\nPredição de hipertensão com redes neurais MLP usando dados demográficos e biométricos\nNeural network approach to non-cooperative target identification based on doppler signature\nPrevisão de vitória ou derrota no jogo blackjack utilizando redes neurais\nClassificação da popularidade de uma música utilizando as suas características auditivas dadas pelo Spotify, por meio de perceptron multicamada"
  },
  {
    "objectID": "ex_aula_rl_2.html",
    "href": "ex_aula_rl_2.html",
    "title": "Exercício - Regressão Linear 2",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_mlp_3.html",
    "href": "ex_aula_mlp_3.html",
    "title": "Exercício - MLP 3",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "5892 - Fundamentos de Adaptação e Aprendizado de Máquina",
    "section": "",
    "text": "Teoria e exercícios para a disciplina\nAutores: Magno T. M. Silva e Renato Candido"
  },
  {
    "objectID": "t_hiperparametros.html",
    "href": "t_hiperparametros.html",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Há diferentes maneiras de evitar que o algoritmo backpropagation fique parado em um mínimo local da função custo, evitando assim que a rede atinja soluções subótimas no treinamento. Outro problema que pode acontecer no treinamento das redes neurais é o chamado overfitting, em que a solução da rede fica especializada nos dados de treinamento e tem um desempenho ruim com os dados de teste. Vimos um exemplo de overfitting com a regressão linear. A seguir, vamos abordar as técnicas mais usadas para evitar esses problemas. Boa parte delas envolve o ajuste de hiperparâmetros, que por sua vez, são todos os parâmetros da rede que não são “aprendidos” durante o treinamento. Por exemplo, o passo de adaptação \\(\\eta\\) é um hiperparâmetro enquanto os pesos não o são.\n\n\nO cálculo do gradiente para atualização do vetor de pesos de um determinado neurônio requer o conhecimento da derivada da função de ativação \\(\\varphi(\\cdot)\\) associada a ele. Para atualização do gradiente, é importante que essa derivada exista e seja não nula. Por isso, as funções sinal e degrau, usadas no neurônio de Rosenblatt, não são adequadas. A seguir vamos descrever as funções de wativação mais usadas na MLP.\n\n\nA função sigmoidal, também conhecida como função logística, é definida como1\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm sgm}(a\\,v_k^{(j)})=\\displaystyle\\frac{1}{1+e^{-a\\, v_k^{(j)}}},\\;\\;\\;\\; a&gt;0,\n$}\n\\end{equation*}\\]\nem que \\(v_k^{(j)}\\) é o resultado da soma do bias com a combinação linear entre as entradas e os pesos do neurônio \\(k\\) da camada \\(j\\) e \\(a\\) é um parâmetro positivo ajustável. A derivada dessa função é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi'_j(v_k^{(j)})=\\frac{\\rm d}{{\\rm d}v_k^{(j)}}{\\rm sgm}(a\\,v_k^{(j)})=\\displaystyle \\frac{a\\,e^{-a\\, v_k^{(j)}}}{\\left[1+e^{-a\\, v_k^{(j)}}\\right]^2}= a \\varphi_j(v_k^{(j)})[1-\\varphi_j(v_k^{(j)})].\n$}\n\\end{equation*}\\]\nComo \\(\\varphi_j(v_k^{(j)})=y_k^{(j)}\\) é a saída do neurônio \\(k\\) da camada \\(j\\), ainda podemos escrever\n\\[\n\\varphi'_j(v_k^{(j)})= a\\, y_k^{(j)}(1-y_k^{(j)}).\n\\]\nNa Figura 1 são mostrados gráficos da função sigmoidal e de sua derivada para dois valores de \\(a\\). Pode-se observar que a saída do neurônio com função sigmoidal fica no intervalo \\([0,\\; 1]\\). Quanto maior o valor do parâmetro \\(a\\) mais abrupta é a mudança do patamar \\(0\\) para o patamar \\(1\\) e consequentemente maior a derivada em \\(v_k^{(j)}=0\\).\n\n\n\n\n\n\nFigura 1: Função sigmoidal e sua derivada para dois valores do parâmetro \\(a\\).\n\n\n\n\n\n\nOutra função de ativação muito utilizada na MLP é a tangente hiperbólica. Essa é a função de ativação que utilizamos nos experimentos com a rede MLP até agora (com \\(a=1\\)). A tangente hiperbólica é definida como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm tanh}(a\\,v_k^{(j)})=\\frac{e^{a\\,v_k^{(j)}}-e^{-a\\,v_k^{(j)}}}{e^{a\\,v_k^{(j)}}+e^{-a\\,v_k^{(j)}}},\\;\\;\\;a&gt;0,\n  $}\n\\end{equation*}\\] sendo \\(a\\) uma constante positiva. Sua derivada é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi_j'(v_k^{(j)})=\\frac{\\rm d}{{\\rm d}v_k^{(j)}}{\\rm tanh}(a\\,v_k^{(j)})=a\\,\\left[1-{\\rm tanh}^2(v_k^{(j)})\\right].\n$}\n\\end{equation*}\\]\nLembrando que a saída do neurônio \\(k\\) com função de ativação tangente hiperbólica é dada por \\(y_k^{(j)}={\\rm tanh}(v_k^{(j)})\\), também podemos escrever\n\\[\n\\varphi'_j(v_k^{(j)})=\\frac{1}{a}(a-y_k^{(j)})(a+y_k^{(j)}).\n\\]\nNa Figura 2 são mostrados gráficos da função tangente hiperbólica e de sua derivada para dois valores de \\(a\\). Pode-se observar que a saída do neurônio com essa função fica no intervalo \\([-1,\\; 1]\\). Quanto maior o valor do parâmetro \\(a\\) mais abrupta é a mudança do patamar \\(-1\\) para o patamar \\(1\\) e consequentemente maior a derivada em \\(v_k^{(j)}=0\\).\n\n\n\n\n\n\nFigura 2: Função tangente hiperbólica e sua derivada para dois valores do parâmetro \\(a\\).\n\n\n\n\n\n\nA unidade linear retificada (Rectified Linear Unit - ReLU) é uma função de ativação dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm ReLU}(v_k^{(j)})=\\max(0, v_k^{(j)})=\\left\\{\\begin{array}{cc}\n                                     0, & v_k^{(j)}\\leq 0 \\\\\n                                     v_k^{(j)}, & v_k^{(j)}&gt;0\n                                   \\end{array}\n                                 \\right.\n\n$}\n\\end{equation*}\\]\nEssa função também é conhecida como função rampa e é análoga ao retificador de meia-onda, o que justifica seu nome. Sua derivada é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j'(v_k^{(j)})={\\rm ReLU}'(v_k^{(j)})=\\left\\{\\begin{array}{cc}\n                                     0, & v_k^{(j)}&lt; 0 \\\\\n                                     1, & v_k^{(j)}&gt;0\\\\\n                                     \\nexists, & v_k^{(j)}=0.\n                                   \\end{array}\n                                 \\right.\n\n$}\n\\end{equation*}\\]\nNa Figura 3 são mostradas a função ReLU e a sua derivada. Observe que a função ReLU não é diferenciável em \\(v_k^{(j)}=0\\). Como ela é diferenciável em todos os outros valores de \\(v_k^{(j)}\\), o valor de sua derivada em zero pode ser arbitrariamente escolhido como 0 ou 1. Em geral, o treinamento de redes MLP profundas que usam essa função é mais rápido quando comparado ao treinamento das redes MLP que usam a tangente hiperbólica. Essa função foi baseada no princípio de que os modelos são mais facilmente otimizados quando o seu comportamento é próximo do linear.\n\n\n\n\n\n\nFigura 3: Função ReLU e sua derivada (a derivada em \\(v_k^{(j)}=0\\) foi arbitrariamente escolhida como 0)\n\n\n\nNa literatura, há diferentes variantes da ReLU, como:\n\nSoftplus;\nGaussian Error Linear Unit (GELU);\nLeaky rectified linear unit (Leaky ReLU);\nParametric rectified linear unit (PReLU);\nExponential linear unit (ELU);\nSigmoid linear unit (SiLU).\n\nAlgumas dessas funções são diferenciáveis em todos os pontos, o que evita ter que escolher arbitrariamente o valor da derivada em \\(v_k^{(j)}=0\\). Apesar da existência dessas variantes, a ReLU ainda é a mais utilizada em redes profundas. Ela apresenta algumas vantagens como:\n\nativação esparsa: em uma rede inicializada aleatoriamente, apenas 50% dos neurônios ocultos são ativados (saída não nula);\nmelhor propagação do gradiente: consegue escapar de mínimos locais em comparação com as funções sigmoidal ou tangente hiperbólica;\ncomputação eficiente;\ninvariante à escala: \\(\\max(0,\\,ax)= a\\,\\max(0,\\,x),\\;\\;a&gt;0\\).\n\nApesar dessas vantagens, a ReLU é ilimitada, o que pode levar o algoritmo de treinamento à divergência. Além disso, neurônios com ReLU podem se tornar inativos para essencialmente todas as entradas. Nesse estado, nenhum gradiente é retropropagado e o neurônio “morre”. Em alguns casos, muitos neurônios podem ficar inativos, diminuindo efetivamente a capacidade do modelo. Esse problema geralmente surge quando a taxa de aprendizado (passo de adaptação) é muito alta e pode ser evitado usando a função leaky ReLU, que atribui uma pequena inclinação positiva para entradas negativas.\n\n\n\nEm problemas de classificação multiclasse, é comum considerar uma rede com \\(N_L\\) neurônios de saída, sendo \\(N_L\\) o número de classes. Nesse caso, a saída esperada da rede é a ativação de apenas um dos \\(N_L\\) neurônios e a inativação dos \\(N_L-1\\) restantes. Para isso, costuma-se usar a função de ativação softmax nos neurônios de saída. Como a função sigmoidal, a função softmax limita a saída do neurônio entre 0 e 1. Porém, ela também leva em conta as saídas dos demais neurônios da camada. Dessa forma, considera-se uma normalização fazendo com que a soma de todas as saídas dos neurônios seja unitária, o que faz com que o vetor saída da rede seja um vetor de probabilidades. A função softmax para o \\(k\\)-ésimo neurônio da camada de saída é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi_L(v_k^{(L)})={\\rm Softmax}(v_k^{(L)})=\\frac{e^{v_k^{(L)}}}{\\displaystyle \\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}},\n$}\n\\end{equation*}\\]\nem que \\(0\\leq\\varphi_L(v_k^{(L)})\\leq 1\\) e \\(\\sum_{\\ell=1}^{N_L}\\varphi_L(v_\\ell^{(L)})=1\\). A derivada dessa função é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'_L(v_k^{(L)})={\\rm Softmax}'(v_k^{(L)})=\\frac{e^{v_k^{(L)}}\\left[\\displaystyle \\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}-e^{v_k^{(L)}}\\right]}{\\displaystyle \\left[\\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}\\right]^2}.\n$}\n\\end{equation*}\\]\n\n\n\n\nA escolha da função custo depende da finalidade da rede neural. Quando empregada em problemas de regressão, é comum usar o erro quadrático médio (Mean Squared Error - MSE) definido por\n\\[\\begin{equation}\\label{mse}\nJ_{\\rm MSE} = \\frac{1}{N_L} \\sum_{\\ell=1}^{N_L} e_{\\ell}^2(n),\n\\end{equation}\\] em que \\[\\begin{equation}\\label{e_n}\ne_{\\ell}(n) = d_{\\ell}(n) - y_{\\ell}^{(L)}(n)\n\\end{equation}\\]\nsão os erros dos neurônios da camada de saída da rede. Apesar de não ser a função custo mais adequada para problemas de classificação, o MSE foi utilizado nos problemas das meias-luas apresentados até o momento.\nQuando a rede é empregada em problemas de classificação, é comum usar a entropia cruzada, uma vez que ela é mais adequada para erros de categorização. No caso de classificação binária em que as categorias são \\(d = 0\\) ou \\(d=1\\) e existe apenas um neurônio de saída, a entropia cruzada é dada por\n\\[\nJ_{\\rm EC} = -  \\left[ d_1(n) \\ln\\left({y_{1}^{(L)}(n)}\\right) + [1 - d_1(n)] \\ln{\\left(1 -y_{1}^{(L)}(n)\\right)}\\right].\n\\]\nPara entender essa função, considere novamente o problema das meias-luas, mantendo \\(d=1\\) para a Região A, mas considerando que \\(d=0\\) para a Região B. Quando \\(y_1^{(L)}(n)\\geq 0,5\\) a rede classifica o dado como pertencente à Região A e para \\(y_1^{(L)}(n)&lt;0,5\\) o dado é classificado como pertencente à Região B. Dessa forma, a saída da rede pode ser interpretada como a probabilidade do dado de entrada pertencer à Região A. Quando \\(d_1(n)=y_1^{(L)}(n) \\in \\{0, 1\\}\\), \\(J_{\\rm EC}=0\\), que é o valor mínimo que essa função custo pode assumir. Para \\(d_1(n)=1\\) e \\(y_1^{(L)}(n)=0,\\!1\\), a rede erra, pois classifica o dado como pertencente à Região B enquanto ele de fato pertence à Região A e \\(J_{\\rm EC}=-1\\times \\ln(0,1)=2,\\!3026.\\) A função custo tem o mesmo valor para \\(d_1(n)=0\\) e \\(y_1^{(L)}(n)=0,\\!9\\), caso em que também há erro de classificação. No caso de classificação entre \\(N_L\\) classes, essa função é chamada de entropia cruzada categórica e é dada por\n\\[\nJ_{\\rm ECC} = -  \\frac{1}{N_L}\\sum_{\\ell=1}^{N_L} d_\\ell(n)  \\ln\\left(y_{\\ell}^{(L)}(n)\\right).\n\\]\nUma das maneiras de se reduzir overfitting é usar regularização na função custo. Isso controla o ajuste dos pesos, possibilitando que a rede tenha uma boa capacidade de generalização. A regularização \\(\\ell_2\\) é a mais comum e consiste em somar à função custo o termo \\[\\frac{\\lambda}{2N_L}\\sum_{\\ell=1}^{N_L}\\|\\mathbf{w}_\\ell^{(L)}(n-1)\\|^2,\\] em que \\(\\lambda\\) é um hiperparâmetro. Assim, ao minimizar a função custo somada a esse termo, o algoritmo também procura minimizar a norma dos vetores de peso da camada de saída, evitando dessa forma que ocorra divergência (Bishop 2006).\nExistem também outras funções custo cujas derivadas não são determinadas analiticamente, mas podem ser obtidas por diferenciação automática (autodiff), que é um conjunto de técnicas usadas para avaliar derivadas de funções numéricas expressas como programas de computador. Mais detalhes sobre autodiff podem ser obtidos em (Baydin et al. 2018).\n\n\n\nSabemos que a inicialização é fundamental para que a rede MLP evite mínimos locais. Nos experimentos com as meias-luas que apresentamos até agora, os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2},\\;10^{-2}]\\). Como não se tem ideia dos valores dos parâmetros ótimos, de fato os pesos precisam ser inicializados de forma aleatória. O problema da forma que inicializamos é definir o intervalo da distribuição uniforme. O intervalo “ideal” depende do conjunto de dados, da arquitetura da rede etc. e sua escolha se torna mais difícil ainda em redes profundas. Além disso, uma pergunta que poderíamos fazer é: a inicialização dos parâmetros da rede considerando uma distribuição uniforme é a mais adequada?\nNo algoritmo backpropagation, o cálculo do gradiente local de uma determinada camada \\(j\\) da rede depende dos gradientes locais das camadas posteriores, ou seja, o gradiente local \\(\\delta^{(j)}_k\\) carrega consigo a multiplicação de todos os gradientes locais das camadas mais profundas da rede. Para redes neurais profundas, se os gradientes locais forem menores do que um, as atualizações dos pesos e biases das camadas mais rasas acabam assumindo valores muito pequenos, tornando o processo de aprendizado lento e ineficiente. Analogamente, para gradientes locais sempre maiores que um, as atualizações dos pesos das camadas menos profundas acabam assumindo valores muito elevados, levando o algoritmo de treinamento à divergência. Esse problema é conhecido como desvanecimento ou explosão dos gradientes. O objetivo das técnicas de inicialização de parâmetros é evitar esse problema. Dessa forma, os pesos e biases precisam ser inicializados dentro de um intervalo específico.\nA seguir, vamos abordar duas técnicas de inicialização frequentemente usadas na literatura (Brownlee 2021).\n\n\nA inicialização de Xavier foi proposta originalmente no artigo (Glorot e Bengio 2010). Para entender a ideia dessa inicialização, vamos primeiramente considerar que os neurônios da rede MLP têm função de ativação do tipo sigmoidal e pesos grandes. Como a função do tipo sigmoidal é plana para valores grandes da entrada, as ativações ficarão saturadas e os gradientes começarão a se aproximar de zero.\nPara evitar esse problema, a inicialização de Xavier busca garantir que a variância de \\(y^{(j)}_k\\) seja mantida igual ao longo das camadas, o que pode evitar o problema de desvanecimento ou explosão dos gradientes. Considerando função de ativação linear, temos\n\\[\ny_k^{(j)}=b_k^{(j)}+w_{k1}^{(j)}y_1^{(j-1)}+w_{k2}^{(j)}y_2^{(j-1)}+\\cdots+w_{kN_{j-1}}^{(j)}y_{N_{j-1}}^{(j-1)}.\n\\]\nCalculando a variância de \\(y_k^{(j)}\\), obtém-se\n\\[\n{\\rm var}(y_k^{(j)})={\\rm var}\\left(b_k^{(j)}+w_{k1}^{(j)}y_1^{(j-1)}+w_{k2}^{(j)}y_2^{(j-1)}+\\cdots+w_{kN_{j-1}}^{(j)}y_{N_{j-1}}^{(j-1)}\\right).\n\\]\nAssumindo que os biases foram inicializados com zero, sua variância também é nula. Portanto, precisamos calcular apenas a variância dos termos do lado direito da equação que contém os pesos. Assumindo independência entre os pesos e as entradas da camada \\(j\\), temos\n\\[\n{\\rm var}(w_{k\\ell}^{(j)}y_\\ell^{(j-1)})=[{\\rm E}\\{y_\\ell^{(j-1)}\\}]^2{\\rm var}(w_{k\\ell}^{(j)})+[{\\rm E}\\{w_{k\\ell}^{(j)}\\}]^2{\\rm var}(y_\\ell^{(j-1)})+{\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}),\n\\]\n\\(\\ell=1,2,\\cdots,N_{j-1}.\\) Considerando ainda que as entradas e os pesos têm médias nulas, a expressão anterior se reduz a\n\\[\n{\\rm var}(w_{k\\ell}^{(j)}y_\\ell^{(j-1)})={\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}).\n\\]\nUsando esse resultado no cálculo da variância de \\(y_k^{(j)}\\), chega-se a\n\\[\n{\\rm var}(y_k^{(j)})=N_{j-1}{\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}).\n\\]\nComo se deseja que \\({\\rm var}(y_k^{(j)})={\\rm var}(y_\\ell^{(j-1)})\\), obtemos\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})=\\frac{1}{N_{j-1}}.\n\\]\nDiante desse resultado, a inicialização de Xavier propõe inicializar os pesos utilizando uma distribuição normal com média nula e desvio padrão \\(1/\\sqrt{N_{j-1}}\\), ou seja\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{1}{N_{j-1}}\\right).\n$}\n\\end{equation*}\\]\nUma variante dessa inicialização, conhecida na literatura como inicialização de Glorot, leva em conta também o número de número de neurônios da camada \\(j\\), ou seja\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{2}{N_{j-1}+N_j}\\right).\n$}\n\\end{equation*}\\]\nA ideia dessa inicialização é preservar também a variância do sinal retropropagado e para isso, considera que a variância do peso é aproximada por\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})\\approx\\frac{1}{(N_{j-1}+N_j)/2}.\n\\]\nHá ainda variantes dessas inicializações que utilizam a distribuição uniforme. Assim, a inicialização de Xavier com distribuição uniforme é\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{3}{N_{j-1}}},\\;+\\sqrt{\\frac{3}{N_{j-1}}}\\,\\right]\n\n$}\n\\end{equation*}\\]\ne a inicialização de Glorot com distribuição uniforme é\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{6}{N_{j-1}+N_j}},\\;+\\sqrt{\\frac{6}{N_{j-1}+N_j}}\\,\\right].\n$}\n\\end{equation*}\\]\n\n\n\nO problema de desvanecimento ou explosão dos gradientes visto com funções de ativação do tipo sigmoidal geralmente não ocorre quando se usa ReLU. Diante disso, foi proposta uma inicialização alternativa à de Xavier para neurônios que consideram ReLU, conhecida como inicialização de He, no artigo (He et al. 2015). Basicamente, a inicialização de He propõe que os pesos tenham o dobro da variância calculada anteriormente, ou seja,\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})=\\frac{2}{N_{j-1}},\n\\]\no que leva à seguinte inicialização considerando a distribuição normal\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{2}{N_{j-1}}\\right)\n$}\n\\end{equation*}\\]\ne à seguinte variante para distribuição uniforme\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{6}{N_{j-1}}},\\;+\\sqrt{\\frac{6}{N_{j-1}}}\\,\\right].\n$}\n\\end{equation*}\\]\n\n\n\n\nUm dos principais hiperparâmetros que precisam ser ajustados no treinamento de uma rede neural é o passo de adaptação ou taxa de aprendizagem. Se o passo for muito baixo, a convergência do algoritmo de treinamento será muito lenta como mostrado na Figura 4 (a). Em contrapartida, um passo muito elevado pode levar o algoritmo à divergência, caso ilustrado na Figura 4 (c). Na Figura 4 (b), considera-se um passo ideal que proporciona uma rápida convergência. O passo de adaptação ideal depende da superfície de desempenho, que, por sua vez, depende da arquitetura da rede e do conjunto de dados. O treinamento da rede pode ser acelerado quando se utiliza uma taxa de aprendizagem ideal (Jordan 2018).\n\n\n\n\n\n\nFigura 4: Três passos de adaptação diferentes: (a) passo muito baixo que requer muitas iterações até que o algoritmo atinja o mínimo da função custo; (b) passo ótimo que faz com que o algoritmo atinja o mínimo rapidamente e (c) passo muito elevado que pode levar o algoritmo à divergência. [Fonte].\n\n\n\nUma das técnicas mais utilizadas para ajustar o passo de adaptação é a learning rate annealing. Nessa técnica, o valor do passo deve ser relativamente alto no início e diminuir gradualmente ao longo do treinamento. Com um passo elevado no início do treinamento, os pesos e biases são ajustados rapidamente para valores “bons”, ou seja, uma taxa alta pode fazer com que o algoritmo “pule” mínimos locais. Em seguida, uma taxa de aprendizagem pequena faz um ajuste fino, possibilitando o algoritmo explorar as partes “mais profundas” da função custo. A forma mais comum de fazer isso é considerar o decaimento do passo em escada ou exponencial, como ilustrado na Figura 5.\nNo decaimento em escada com degraus uniformes da Figura 5 (a), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0-\\Delta \\eta \\lfloor k/\\Delta k\\rfloor,\n\\]\nem que \\(\\eta_0\\) é o valor inicial do passo, \\(\\Delta \\eta\\) o valor do decaimento e \\(\\Delta k\\) o número de épocas em que o passo é mantido fixo. No caso da Figura 5 (a), foram usados \\(\\eta_0=0,\\!1\\), \\(\\Delta \\eta=0,\\!0101\\) e \\(\\Delta k=20\\).\nNo decaimento em escada com degraus não uniformes da Figura 5 (b), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0\\Delta \\eta^{\\lfloor k/\\Delta k\\rfloor}.\n\\]\nNo caso da Figura 5 (b), foram usados \\(\\eta_0=0,\\!1\\), \\(\\Delta \\eta=0,\\!5\\) e \\(\\Delta k=20\\).\nPor fim, no decaimento exponencial da Figura 5 (c), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0 e^{-a k},\\;\\;a&gt;0.\n\\]\nNo caso da Figura 5 (c), foram usados \\(\\eta_0=0,\\!1\\) e \\(a=0,\\!01\\).\n\n\n\n\n\n\nFigura 5: Learning rate annealing: (a) decaimento em escada uniforme, (b) decaimento em escada não uniforme e (c) decaimento exponencial.\n\n\n\nO desafio de usar esquemas de ajuste dos passos de adaptação é que seus hiperparâmetros precisam ser definidos com antecedência e dependem da arquitetura da rede e do problema. Além disso, pode ser conveniente adaptar pesos de neurônios de camadas diferentes com passos distintos. Algoritmos de otimização como Adam e RMSprop resolvem esses problemas, pois ajustam os passos de adaptação de forma automática com o uso de regularização, como veremos posteriormente.\n\n\n\nAbordamos anteriormente o treinamento do algoritmo LMS nos modos batch, mini-batch e estocástico2. Como o algoritmo LMS foi proposto para aplicações de tempo real, o modo estocástico é o mais utilizado. A cada dado de entrada se deseja ter o dado de saída correspondente com o menor atraso possível, ou seja, o treinamento ocorre junto com a inferência. A saída e o erro calculados no treinamento são utilizados para atualizar os pesos e ao mesmo tempo para se obter a estimativa ou classificação desejada.\nNo caso das redes neurais, o modo mini-batch é o mais utilizado. Geralmente, a inferência não é realizada durante o treinamento. A saída e o erro são utilizados no treinamento apenas para atualizar os pesos do algoritmo. Depois do treinamento, fixam-se os pesos para então se fazer a inferência e testar o classificador ou regressor. Apesar de termos abordado os três modos de treinamento apenas no algoritmo LMS, a extensão para redes neurais é direta.\nO uso de mini-batch no processo de aprendizado consiste em dividir aleatoriamente o conjunto de treinamento da rede em blocos de tamanho previamente definido, embaralhando-se as amostras do conjunto. A atualização dos pesos e biases ocorre apenas depois que são calculados os gradientes de todos os elementos de um mini-batch. Dessa forma, a atualização dos parâmetros da rede está associada à média dos gradientes de um mini-batch. Considera-se passada uma época quando todos os mini-batches são percorridos. Após cada época do algoritmo de otimização, a divisão do conjunto de treinamento entre mini-batches é refeita de maneira aleatória, embaralhando-se novamente o conjunto de treinamento. O tamanho de cada mini-batch é um hiperparâmetro e não muda no decorrer das épocas.\nQuando se considera que cada mini-batch é formado apenas por uma amostra do conjunto de treinamento, diz-se que o método de atualização de parâmetros é estocástico. O uso do método estocástico para atualização de parâmetros de uma rede neural é pouco eficiente, pois a atualização ocorre em direções distintas do mínimo da função custo, o que faz com que o algoritmo leve mais épocas para convergir. O método estocástico também anula as vantagens computacionais de uma implementação matricial do algoritmo, uma vez que as atualizações são realizadas sobre cada amostra de treinamento.\nQuando um mini-batch possui todos os elementos do conjunto de treinamento, nomeia-se o método de atualização de parâmetros apenas como batch. Com o método batch, os parâmetros são sempre atualizados na direção do mínimo da função custo. Diante disso, o batch seria o modo de treinamento ideal se não houvesse limitações computacionais. Como é necessário esperar que todo o conjunto de treinamento seja percorrido para se realizar a atualização dos parâmetros, o modo de treinamento batch é muito demorado e computacionalmente ineficiente quando comparado com o mini-batch.\n\n\n\nOutro problema que pode aparecer no treinamento das redes neurais é o overfitting, que ocorre quando há uma diferença significativa entre o desempenho da rede sobre seu conjunto de treinamento e sobre um outro conjunto distinto de dados, o conjunto de teste. Neste caso, a rede se especializa tanto no conjunto de treinamento, que não apresenta capacidade de generalização satisfatória para outros dados. Uma das técnicas mais utilizadas para evitar esse problema é o dropout. Essa técnica basicamente inativa aleatoriamente, em cada iteração do algoritmo backpropagation, diferentes neurônios de cada camada oculta da rede. Cada neurônio é inativado com probabilidade \\(p\\), sendo \\(p\\) o hiperparâmetro associado a essa esquema. Na Figura 6, exemplifica-se a aplicação do dropout com \\(p = 0,\\!5\\). Observe que metade dos neurônios de cada camada oculta (neurônios destacadas em vermelho) foram inativados em uma determinada iteração. Quando um neurônio é inativado, seu gradiente é nulo de modo que seus pesos não são atualizados. Heuristicamente, a eliminação temporária de diferentes conjuntos de neurônios leva ao treinamento de redes neurais distintas. Dessa forma, o procedimento de eliminação é equivalente ao cálculo da média dos efeitos de um grande número de redes distintas. Como elas vão se adaptar de diferentes maneiras, isso possibilita a redução do overfitting, pois será mais difícil para a rede se especializar nos dados de treinamento (Goodfellow, Bengio, e Courville 2016).\n\n\n\n\n\n\nFigura 6: Exemplo de aplicação do dropout em uma rede MLP com \\(p = 0,5\\).\n\n\n\n\n\n\nComo vimos anteriormente, o algoritmo LMS é uma aproximação estocástica do algoritmo do gradiente exato (steepest descent). Vimos também que existe um compromisso entre a velocidade de convergência e a precisão da solução. Quanto menor o passo de adaptação, mais lento é o algoritmo e os pesos variam menos em torno da solução de Wiener. Quanto maior o passo, maior a sua velocidade de convergência e maior também a variação dos pesos torno da solução ótima. O algoritmo também pode divergir dependendo do valor do passo e neste caso, os pesos vão para infinito. O mesmo acontece com o algoritmo backpropagation: quanto menor for o passo de adaptação, menores serão as mudanças nos pesos da rede de uma iteração para outra, mais suave será a trajetória no espaço dos pesos e mais lenta a taxa de aprendizagem. Se aumentarmos muito o passo de adaptação para acelerar a taxa de aprendizagem, as mudanças dos pesos de uma iteração para outra também aumentam e o algoritmo pode divergir.\nUm método simples de aumentar a taxa de aprendizagem sem causar divergência é modificar a adaptação do backpropagation incluindo um termo chamado momentum. Antes de introduzir esse termo, vamos lembrar da atualização da matriz de pesos da Camada \\(j\\) da MLP com o algoritmo backpropagation:\n\\[\n\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n),\n\\]\nem que\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}.\n\\]\nDefinindo agora a matriz \\[\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)\\triangleq\\mathbf{W}^{(j)}(n-1)-\\mathbf{W}^{(j)}(n-2),\n\\]\nque representa a diferença entre a matriz de pesos da iteração \\(n-1\\) e da iteração \\(n-2\\), a atualização do backpropagation com momentum fica\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n),\n$}\n\\end{equation*}\\]\nem que \\(0\\leq \\alpha&lt;1\\) é a constante de momentum. Observe que \\(\\alpha=0\\) leva essa atualização à forma padrão do backpropagation sem momentum. Usando a definição \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\), podemos reescrever essa equação de atualização como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n).\n$}\n\\end{equation*}\\]\nPara entender o efeito do termo de momentum, note que\n\\[\\begin{align*}\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(1)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(2)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\nonumber\\\\\n&=\\alpha\\left[\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)\\right]+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\nonumber\\\\\n&=\\alpha^2\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\left[\\alpha\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)+\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\right]\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(3)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(2)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(3)\\nonumber\\\\\n&=\\alpha^3\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\left[\\alpha^2\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)+\\alpha\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)+\\boldsymbol{\\Delta}_{\\delta}^{(j)}(3)\\right]\\nonumber\\\\\n&\\vdots\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)&=\\alpha^n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\sum_{k=1}^n \\alpha^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\\nonumber\n\\end{align*}\\]\nO termo \\(\\alpha^n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)\\) tende a zero a medida que o número de iterações aumenta, uma vez que \\(0\\leq\\alpha&lt;1\\) e os pesos são inicializados com valores finitos. Assim, podemos escrever\n\\[\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)=\\eta\\sum_{k=1}^n \\alpha^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\n\\]\nEssa equação nos possibilita entender os efeitos benéficos do momentum, enumerados a seguir (Haykin 2009):\n\no ajuste \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) representa a soma de uma série temporal ponderada exponencialmente. Como \\(0\\leq \\alpha&lt;1\\), consideram-se pesos maiores para ajustes recentes e pesos menores para os mais antigos. Dessa forma, \\(\\alpha\\) também é chamado na literatura de fator de esquecimento;\nquando o termo \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) tem o mesmo sinal algébrico em sucessivas iterações, a matriz \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) cresce em magnitude e a matriz de pesos \\(\\mathbf{W}^{(j)}(n)\\) é ajustada com uma grande quantidade. Diante disso, o momentum tende a acelerar a convergência do backpropagation em direções de descida mais íngreme;\nquando o sinal algébrico do termo \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) muda em sucessivas iterações, a matriz \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) diminui em magnitude e a matriz de pesos \\(\\mathbf{W}^{(j)}(n)\\) é ajustada com uma pequena quantidade. Diante disso, o momentum tem o efeito de estabilizador em direções que oscilam em sinal.\n\nEm suma, a incorporação do momentum no algoritmo backpropagation pode trazer alguns efeitos benéficos no aprendizado, incluindo a possibilidade de evitar que o algoritmo fique estagnado em um mínimo local.\nA seguir vamos comparar o backpropagation com e sem momentum.\n\n\nNo exemplo das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), vimos que uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada com o backpropagation sem momentum é capaz de classificar corretamente os dados dependendo da inicialização. Para verificar o efeito benéfico de se utilizar momentum, vamos considerar uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(10(\\text{tanh})\\)-\\(1(\\text{tanh})\\). Essa mudança de configuração se deve ao fato de que o backpropagation com momentum na configuração anterior se comporta de maneira análoga ao caso sem momentum. Considerou-se ainda o modo de treinamento mini-batch (\\(N_t=1000,\\) \\(N_b=50\\) e \\(N_e=2000\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\), o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!1\\) e a constante de momentum igual a \\(\\alpha=0,\\!9\\). Além disso, considerou-se a função custo do erro quadrático médio (MSE).\nNa Figura 7, são mostradas a função custo ao longo das épocas de treinamento, a classificação dos dados de teste e a separação das regiões para uma determinada inicialização. Verifica-se que o algoritmo backpropagation sem momentum não consegue escapar do mínimo local, obtendo \\(7,\\!9\\%\\) de taxa de erro de classificação. Ao se utilizar momentum, percebe-se que o algoritmo apresenta um MSE próximo do caso sem momentum durante as \\(450\\) épocas iniciais do treinamento. Depois disso, eles seguem caminhos diferentes: o algoritmo sem momentum fica parado no mínimo local correspondente a um MSE aproximadamente \\(-7\\) dB, enquanto o algoritmo com momentum consegue atingir um MSE de aproximadamente \\(-43\\) dB na época \\(2000\\). Isso é suficiente para não gerar erros de classificação.\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento, classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões obtidas com uma rede MLP \\(2\\)-\\(3(\\text{tanh})\\)-\\(10(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada em mini-batch (\\(N_t=1000\\), \\(N_b=50\\)) com o algoritmo backpropagation sem momentum (\\(\\eta=0,\\!1\\)) e com momentum (\\(\\eta=0,1\\), \\(\\alpha=0,\\!9\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nO comportamento observado na Figura 7 nem sempre se repete, pois depende da inicialização. Em muitos casos, os algoritmos com e sem momentum apresentam comportamentos semelhantes. Ainda podem ocorrer situações em que o algoritmo com momentum não consegue evitar mínimos locais, enquanto o algoritmo sem momentum consegue. Apesar disso, o uso de momentum é considerado benéfico na maior parte das vezes. Isso de deve ao fato de que quando implementado junto com outras técnicas pode fazer com que a rede atinja valores de MSE mais baixos no treinamento, o que é indício de que mínimos locais foram evitados.\n\n\n\n\nA escolha do algoritmo de otimização é essencial em Aprendizado de Máquina. O algoritmo de otimização Adam (adaptive moment estimation) (Kingma e Ba 2015) é uma extensão do algoritmo do gradiente estocástico e tem sido muito utilizado recentemente. Ao introduzir o algoritmo, os autores listam os benefícios de se usar Adam em problemas de otimização não convexa:\n\nsimples de implementar, computacionalmente eficiente e requer poucos requisitos de memória;\nadequado quando se usa muitos dados e/ou parâmetros;\napropriado para problemas não estacionários e problemas com gradientes muito ruidosos e/ou esparsos; e\nos hiperparâmetros têm interpretação intuitiva e são simples de ajustar.\n\nO otimizador Adam atualiza os pesos e biases de uma rede neural a partir dos gradientes calculados na iteração atual e em iterações passadas, de forma a tornar mais estável o processo de aprendizado da rede, evitando-se assim variações excessivas em direções que não são a do mínimo da função custo. Ele combina o gradiente estocástico com momentum com o otimizador RMSprop (root mean squared propagation). Para introduzir esse otimizador, vamos antes introduzir o otimizador RMSprop.\nÀ medida que os dados se propagam na rede, os gradientes calculados para atualização dos parâmetros podem ficar muito pequenos ou muito grandes. Gradientes muito pequenos podem levar à estagnação do backpropagation. Em contrapartida, gradientes muito grandes podem levar à divergência do algoritmo. O otimizador RMSprop foi proposto por G. Hinton, um dos “pais” do backpropagation, para lidar com esse problema usando uma média móvel dos gradientes ao quadrado. Isso gera uma normalização no algoritmo, que passa a ser encarado como um algoritmo de passo variável. Assim, quando os gradientes são grandes, o método diminui o passo para evitar a divergência e quando os gradientes são pequenos, ele aumenta o passo para evitar a estagnação. A título de curiosidade, o algoritmo RMSprop foi proposto por Hinton na sexta aula do curso Neural Networks for Machine Learning e diferente do Adam, não foi publicado.\nQuando deduzimos o algoritmo backpropagation, definimos a matriz\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T},\n\\]\nque contém o negativo dos vetores gradiente de todos os neurônios da Camada \\(j\\). Vamos agora definir a matriz \\(\\mathbf{S}^{(j)}(n)\\), calculada recursivamente como\n\\[\n\\mathbf{S}^{(j)}(n) = \\beta_2\\mathbf{S}^{(j)}(n-1) + (1-\\beta_2)\\left[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\right]^{\\odot 2},\n\\]\nem que \\(\\mathbf{S}^{(j)}(0)=\\boldsymbol{0}\\), \\(0\\ll \\beta_2&lt; 1\\) é um hiperparâmetro que faz o papel de um fator de esquecimento e a operação \\([\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)]^{\\odot 2}\\) indica que cada elemento da matriz \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) é elevado ao quadrado. Levando em conta a inicialização com valores nulos, a equação recursiva para a matriz \\(\\mathbf{S}^{(j)}(n)\\) pode ser reescrita como\n\\[\n\\mathbf{S}^{(j)}(n)=(1-\\beta_2)\\displaystyle \\sum_{k=1}^{n}\\beta_2^{n-k}\\left[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k)\\right]^{\\odot 2}.\n\\]\nA menos da constante \\((1-\\beta_2)\\), observa-se que essa estimativa considera pesos maiores para os gradientes ao quadrado mais recentes e pesos menores para os mais antigos, o que caracteriza uma janela exponencial. Utilizando a matriz \\(\\mathbf{S}^{(j)}(n)\\), a atualização dos pesos e biases da Camada \\(j\\) da rede segundo o otimizador RMSprop é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n) = \\mathbf{W}^{(j)}(n-1) + \\eta\\;{\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)} \\oslash \\left[{\\left[{\\mathbf{S}}^{(j)}(n)\\right]^{\\odot \\frac{1}{2}} + \\varepsilon}\\mathbf{1}\\right],\n$}\n\\end{equation*}\\]\nem que \\(\\oslash\\) se refere a divisão de Hadamard, que resulta em uma matriz em que cada elemento é igual à divisão do respectivo elemento da matriz à esquerda pelo respectivo elemento da matriz à direita, \\(\\varepsilon\\) é uma constante positiva pequena (e.g., \\(\\varepsilon=10^{-8}\\)) usada para evitar divisões por zero e \\(\\mathbf{1}\\) é uma matriz com todos os elementos iguais a 1 e dimensões adequadas para que a soma seja possível de ser calculada. Para entender melhor essas operações, suponha que na iteração \\(n\\) dispomos das matrizes\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\left[\\begin{array}{cc}\n                a & b \\\\\n                c & d\n              \\end{array}\n\\right]\\;\\;\\;\\text{e}\\;\\;\\; {\\mathbf{S}}^{(j)}(n)=\\left[\\begin{array}{cc}\n                 e & f \\\\\n                 g & h\n               \\end{array}\n\\right].\n\\]\nAssim,\n\\[\n{\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)} \\oslash \\left({\\left[{\\mathbf{S}}^{(j)}(n)\\right]^{\\odot \\frac{1}{2}} + \\varepsilon\\mathbf{1}}\\right)=\\left[\\begin{array}{ccc}\n                 \\displaystyle\\frac{a}{\\sqrt{e}+\\varepsilon} && \\displaystyle\\frac{b}{\\sqrt{f}+\\varepsilon} \\\\\n                 &&\\\\\n                 \\displaystyle\\frac{c}{\\sqrt{g}+\\varepsilon} && \\displaystyle\\frac{d}{\\sqrt{h}+\\varepsilon}\n               \\end{array}\n\\right].\n\\]\nEm vez de usar o negativo dos gradientes de \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\), o otimizador Adam também considera uma janela exponencial para estimar esses gradientes. Para isso, define-se a matriz\n\\[\n\\mathbf{V}^{(j)}(n) = \\beta_1\\mathbf{V}^{(j)}(n-1) + (1-\\beta_1)\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\n\\]\nem que \\(\\mathbf{V}^{(j)}(0)=\\boldsymbol{0}\\) e \\(0\\ll \\beta_1&lt; 1\\) é um hiperparâmetro que também faz o papel de um fator de esquecimento. Novamente, levando em conta a inicialização com valores nulos, a equação recursiva para \\(\\mathbf{V}^{(j)}(n)\\) pode ser reescrita como\n\\[\n\\mathbf{V}^{(j)}(n)=(1-\\beta_1)\\displaystyle \\sum_{k=1}^{n}\\beta_1^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\n\\]\nAs inicializações das matrizes \\(\\mathbf{S}^{(j)}\\) e \\(\\mathbf{V}^{(j)}\\) com elementos nulos podem gerar distorções no início do treinamento do algoritmo. Observe que na atualização do RMSprop, o valor da matriz \\(\\mathbf{S}^{(j)}(n)\\) para \\(n=1\\) é \\(\\mathbf{S}^{(j)}(1)=(1-\\beta_2)[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)]^{\\odot 2}\\), o que tende a ser muito pequeno já que \\(0\\ll \\beta_2 &lt;1\\). Para amenizar isso, são definidas as as matrizes de correção\n\\[\\begin{align*}\n\\overline{\\mathbf{V}}^{(j)}(n) &= \\frac{1}{1 - \\beta_1^n}\\,{\\mathbf{V}^{(j)}(n)}\\;\\; \\textnormal{e} \\nonumber\\\\\n\\overline{\\mathbf{S}}^{(j)}(n) &= \\frac{1}{1 - \\beta_2^n}\\,{\\mathbf{S}^{(j)}(n)}. \\nonumber\n\\end{align*}\\]\nComo \\(0\\ll \\beta_1, \\beta_2&lt;1\\), as matrizes corrigidas \\(\\overline{\\mathbf{V}}^{(j)}(n)\\) e \\(\\overline{\\mathbf{S}}^{(j)}(n)\\) tendem às matrizes \\({\\mathbf{V}}^{(j)}(n)\\) e \\({\\mathbf{S}}^{(j)}(n)\\), respectivamente, a medida que \\(n\\) aumenta. Ou seja, o efeito da correção ocorre apenas no início do treinamento, como esperado. Utilizando essas matrizes corrigidas, a atualização dos pesos e bias da Camada \\(j\\) da rede segundo o otimizador Adam é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n) = \\mathbf{W}^{(j)}(n-1) + \\eta\\;{\\overline{\\mathbf{V}}^{(j)}(n)} \\oslash \\left[{\\left[{\\overline{\\mathbf{S}}^{(j)}(n)}\\right]^{\\odot \\frac{1}{2}} + \\varepsilon\\mathbf{1}}\\right].\n$}\n\\end{equation*}\\]\nA seguir vamos comparar o backpropagation com o RMSprop e Adam.\n\n\nNo exemplo das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), vimos que uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada com o backpropagation sem momentum é capaz de classificar corretamente os dados dependendo da inicialização. No entanto, quando consideramos uma rede mais profunda, a probabilidade do backpropagation de ficar parado em mínimos locais é alta. Como exemplo, vamos considerar uma MLP com cinco camadas e configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) no modo de treinamento mini-batch (\\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=5000\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\) e o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!5\\) para todos os algoritmos. Além disso, considerou-se a função custo do erro quadrático médio (MSE). Por fim, os hiperparâmetros do algoritmo RMSprop foram selecionados como \\(\\beta_2=0,\\!99\\) e \\(\\varepsilon=10^{-4}\\) e do Adam como \\(\\beta_1=\\beta_2=0,\\!99\\) e \\(\\varepsilon=10^{-4}\\).\nNa Figura 8, na Figura 9 e na Figura 10, são mostradas a função custo ao longo das épocas, a classificação dos dados de teste e a separação das regiões para três inicializações diferentes, respectivamente. Nos três casos, verifica-se que o algoritmo backpropagation sem momentum, denotado como SGD (stochastic gradient), não consegue escapar do mínimo local, obtendo \\(50\\%\\) de taxa de erro de classificação. No caso da Figura 8, os comportamentos do RMSprop e do Adam são muito parecidos: ambos atingem um MSE de aproximadamente \\(-100\\) dB no treinamento e apresentam taxas de erro de classificação iguais a zero. Mudando a inicialização, observamos na Figura 9 um comportamento diferente para o Adam, que apesar de escapar de um mínimo local logo depois da época \\(3700\\), apresenta um MSE que oscila em torno de \\(-15\\) dB, o que levou a um erro de classificação de \\(1\\%\\). Neste caso, o RMSprop atinge novamente um MSE de aproximadamente \\(-100\\) dB no treinamento e mantém a taxa de erro de classificação igual a zero. Mudando novamente a inicialização, observamos na Figura 10 que o Adam atingiu novamente o patamar de \\(-100\\) dB no treinamento e \\(0\\%\\) de taxa de erro de classificação. Já o RMSprop ficou parado em um mínimo local que levou a um MSE de aproximadamente \\(-7\\) dB e a uma taxa de erro de classificação de \\(5,\\!5\\%\\).\nA partir desse experimento, verifica-se que mudar o algoritmo de otimização é benéfico para evitar mínimos locais, principalmente quando comparamos o RMSprop e o Adam com o SGD em redes profundas. No entanto, a adoção de um desses algoritmos de otimização apenas não é suficiente para evitar mínimos locais, como vimos na Figura 9 e na Figura 10. Considerando o Adam e o RMSprop, observa-se na literatura que o Adam tem sido preferido na maior parte das aplicações. No entanto, o Adam tem algumas desvantagens:\n\nnão converge adequadamente em alguns exemplos simples, como pudemos comprovar no exemplo da Figura 9;\no erro de generalização pode ser grande em muitos problemas de visão computacional;\nrequer mais memória que o método do gradiente (SGD); e\ntem dois hiperparâmetros e portanto, alguns ajustes podem ser necessários.\n\n\n\n\n\n\n\nFigura 8: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento, classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões obtidas com uma rede MLP \\(2\\)-\\(3(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada em mini-batch (\\(N_t=1000\\), \\(N_b=50\\)) com o algoritmo SGD (\\(\\eta=0,5\\)), RMSprop (\\(\\eta=0,5\\), \\(\\beta_2=0,99\\), \\(\\varepsilon=10^{-4}\\)) e Adam (\\(\\eta=0,5\\), \\(\\beta_1=\\beta_2=0,99\\), \\(\\varepsilon=10^{-4}\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\n\n\n\n\n\n\nFigura 9: Veja legenda da Figura 8.\n\n\n\n\n\n\n\n\n\nFigura 10: Veja legenda da Figura 8.\n\n\n\n\n\n\n\nA essência do aprendizado de uma rede MLP com o algoritmo backpropagation é aproximar um mapeamento entrada-saída por meio dos pesos e biases, utilizado um conjunto de exemplos rotulados. Espera-se que a rede aprenda o suficiente com os dados do passado e que seja capaz de generalizar para dados futuros. No processo de aprendizagem, é importante selecionar a “melhor” rede (número de camadas, número de neurônios, funções de ativação, passo de adaptação, etc.) dentro de um conjunto de redes candidatas, considerando um determinado critério. Além disso, o MSE tende a diminuir monotonicamente ao longo das épocas de treinamento. Em geral, quanto maior o número de épocas, mais baixo é o MSE. No entanto, um MSE baixo no treinamento não corresponde necessariamente a um desempenho satisfatório da rede com o conjunto de teste, ou seja, pode haver overfitting. A pergunta que cabe fazer aqui é: quando devemos parar de treinar já que um treinamento longo pode gerar overfitting?\nPara responder essa pergunta e selecionar a melhor rede, é comum utilizar um conjunto de dados de validação. Neste caso, o conjunto de dados disponível deve ser primeiramente particionado de maneira aleatória entre treinamento e teste. O conjunto de treinamento, por sua vez, deve ser particionado em dois subconjuntos disjuntos:\n\nsubconjunto de estimação, usado para treinar o modelo;\nsubconjunto de validação, usado para testar o modelo durante o treinamento.\n\nA ideia de usar um conjunto de validação distinto do conjunto de treinamento e de teste é validar o modelo durante o treinamento com um conjunto de dados diferente do utilizado para estimar os parâmetros. A avaliação final do modelo para observar sua capacidade de generalização deve ser sempre feita com os dados do conjunto de teste, que não foram usados durante o treinamento, considerando fixos os pesos e biases da rede.\nNormalmente, uma rede MLP treinada com o algoritmo backpropagation aprende em etapas, passando da realização de funções de mapeamento simples para funções de mapeamento mais complexas à medida que o treinamento avança. Esse processo pode ser verificado pela diminuição do MSE ao longo das épocas de treinamento: ele começa com um valor alto, diminui rapidamente e depois continua a diminuir lentamente quando a rede atinge um mínimo local da superfície de erro. Como o principal objetivo é obter uma rede com uma boa capacidade de generalização, é muito difícil descobrir quando parar de treinar, baseando-se apenas na curva de aprendizado do treinamento. Em particular, é possível que ocorra overfitting se o treinamento não for interrompido no ponto certo.\nPodemos identificar o começo do overfitting por meio da validação cruzada (cross-validation). O subconjunto de exemplos de estimação é usado para treinar a rede da maneira usual, exceto por uma pequena modificação: o treinamento é interrompido periodicamente depois de um determinado número de épocas e e a rede é testada com o subconjunto de validação. Mais especificamente, o “processo de estimação seguido de validação” periódico ocorre da seguinte forma (Haykin 2009):\n\napós um intervalo de treinamento - a cada cinco épocas, por exemplo - os pesos e biases da MLP são mantidos fixos e apenas o cálculo progressivo é realizado. O erro de validação é então medido para cada exemplo do subconjunto de validação;\nquando a fase de validação é concluída, o treinamento é retomado em um novo intervalo e o processo é repetido.\n\nNa Figura 11 são mostradas duas curvas de aprendizado: uma obtida com o subconjunto de estimação (treinamento) e outra obtida com os dados do subconjunto de validação. Normalmente, o modelo não se sai tão bem no subconjunto de validação quanto no subconjunto de estimação. A curva de aprendizado de estimação diminui monotonicamente ao longo das épocas. Em contrapartida, a curva de validação diminui monotonicamente até um ponto de mínimo e a partir deste ponto começa a aumentar à medida que o o treinamento continua. Observando a curva de aprendizado de estimação, pode parecer que seria melhor continuar o treinamento além do ponto de mínimo da curva de validação. No entanto, o que a rede está aprendendo além desse ponto é essencialmente o ruído contido nos dados de treinamento, o que leva ao overfitting. Diante disso, o treinamento deve ser interrompido quando a curva de validação atinge seu valor mínimo.\n\n\n\n\n\n\nFigura 11: Curvas de erro de estimação e validação. O treinamento deve parar na época correspondente ao mínimo da curva de erro de validação. Fonte: Figura adaptada de (Haykin 2009).\n\n\n\nA validação cruzada descrita anteriormente é conhecida como holdout method. Existem outras variantes da validação cruzada na literatura. Uma das mais utilizadas é a conhecida como multifold cross-validation, que é particularmente útil quando os exemplos de treinamento são escassos (Leite 2020). Nesse método, o conjunto de treinamento disponível de \\(N_t\\) exemplos é dividido em \\(K\\) subconjuntos com \\(K&gt;1\\), sendo \\(N_t\\) divisível por \\(K\\). O modelo é treinado com todos os subconjuntos exceto um e o erro de validação é medido testando o modelo no subconjunto que é deixado de fora. Este procedimento é repetido \\(K\\) vezes, cada vez usando um subconjunto diferente para validação, conforme ilustrado na Figura 12 para \\(K=5\\). O desempenho do modelo é avaliado pela média do erro quadrado de validação em todas as tentativas do experimento. A desvantagem dessa variante é o custo computacional envolvido, uma vez que o modelo tem que ser treinado \\(K\\) vezes, sendo \\(1&lt;K\\leq N_t\\).\n\n\n\n\n\n\nFigura 12: Multifold cross-validation: para cada treinamento, o subconjunto de dados destacado em azul é usado para validar o modelo treinado com os dados destacados em magenta. [Fonte]\n\n\n\nA validação cruzada é útil não só para evitar overfitting, mas também para validar a arquitetura da rede. Dessa forma, uma vez definido o número de camadas de uma rede MLP, por exemplo, o modelo de treinamento e validação da Figura 12 pode ser utilizado para verificar se o número de camadas é adequado com base no erro de validação. Se o erro de validação cai nos \\(K\\) treinamentos, então o número de camadas parece ser adequado. Isso também pode ser utilizado para comparar redes MLP com diferentes arquiteturas para ajudar na escolha da arquitetura mais adequada.\n\n\n\nSuponha que se deseja prever se um paciente tem ou não uma determina doença com base em determinadas medidas diagnósticas, incluídas no conjunto de dados. Considere que um dos dados é o ácido úrico, cujos valores de referência estão no intervalo \\([3,4,\\; 7,0]~\\rm{mg/dL}\\) e um outro dado é o número de plaquetas, cujos valores de referência estão no intervalo \\([151.000,\\; 304.000]/{\\rm mm}^3\\). Esses diferentes intervalos tornam o treinamento mais desafiador. Por exemplo, considere um regressor com uma única camada e dois pesos, em que os dois dados de entrada possuem faixas de valores muito diferentes. Alterações no valor de um dos pesos produzem mudanças muito maiores na saída e na função de erro do que mudanças semelhantes no outro peso. Nessa situação, pode ser benéfico mudar a escala das variáveis de entrada para que fiquem em intervalos semelhantes. Vamos ver três técnicas a seguir.\n\n\nNessa técnica, os valores dos dados de entrada são normalizados para que fiquem em um intervalo padrão, geralmente \\([0,\\; 1]\\) ou \\([-1, 1]\\). Como antes, considere que o conjunto de dados tenha \\(N\\) elementos, organizados como uma sequência de \\(M\\) valores de \\(x\\), ou seja, \\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1}), (x_{12}, x_{22}, \\cdots, x_{M2}),\\cdots, (x_{1N}, x_{2N}, \\cdots, x_{MN})\\}.\n\\] Para normalização no intervalo \\([0,\\; 1]\\), basta fazer \\[\n\\widetilde{x}_{kn}=\\frac{x_{kn}-x_{k,\\text{min}}}{x_{k,\\text{max}}-x_{k,\\text{min}}},\n\\] \\(k=1,2, \\cdots, M\\), \\(n=1,2,\\cdots, N\\), \\(x_{k,\\text{min}}=\\min\\{x_{kn}\\}_{n=1}^N\\) e \\(x_{k,\\text{max}}=\\max\\{x_{kn}\\}_{n=1}^N\\). Considerando a normalização no intervalo \\([-1,\\; 1]\\), basta fazer \\[\n\\widetilde{\\widetilde{x}}_{kn}=2\\widetilde{x}_{kn}-1.\n\\] Essa normalização é uma boa opção quando os dados seguem uma distribuição aproximadamente uniforme em todo o intervalo.\nDependendo da distribuição dos dados, pode ser útil considerar um dimensionamento logarítmico, em que \\(\\widetilde{x}_{kn}=\\ln(x_{kn})\\). Esse tipo de normalização é utilizado quando a distribuição dos recursos é muito assimétrica em pelo menos um dos lados da cauda. Por exemplo, quando os dados apresentam uma distribuição exponencial.\n\n\n\nA fim de reescalar os dados de entrada para pertencerem a intervalos semelhantes é comum normalizá-los para que tenham média zero e variância unitária. Para isso, calcula-se primeiramente a média e variância dos dados, ou seja, \\[\n\\mu_k=\\frac{1}{N}\\sum_{n=1}^{N}x_{kn}\n\\] e \\[\n\\sigma_k^2=\\frac{1}{N}\\sum_{n=1}^{N}(x_{kn}-\\mu_k)^2.\n\\] Em seguida, considera-se \\[\n\\widetilde{x}_{kn}=\\frac{x_{kn}-\\mu_k}{\\sigma_k}.\n\\]\nNa (fighistnorm1?) são mostrados dois histogramas: o histograma da esquerda representa a distribuição dos valores de plaquetas de um conjunto de pacientes e à direita o histograma dos valores dessa característica normalizados.\n\n\n\nHistograma de dados de plaquetas não normalizados (à direita) e normalizados com média zero e variância unitária (à esquerda).\n\n\nEssa normalização é uma boa opção quando os dados seguem uma distribuição normal ou próxima de normal. No entanto, é comum também considerá-la em casos de distribuição não normal.\n\n\n\nO corte é uma técnica utilizada para minimizar a influência de outliers extremos. Diferente do truncamento, que simplesmente ignora valores de outliers, o corte substitui os valores que aparecem fora do intervalo de maior ocorrência pelo valor máximo desse intervalo. Considere por exemplo o histograma mostrado na (fighistnorm2?), à esquerda. É possível observar que o intervalo de maior ocorrência dos valores dessa característica é \\([150, 450]\\). No entanto, há valores acima de 450, que podem ser considerados *outliers. A normalização por corte substitui todos os valores acima de 450 por 450 e por isso aparece um pico nesse valor, como mostrado no histograma na (fighistnorm2?), à direita. O corte impede que o modelo utilize dados sem importância.\n\n\n\nHistograma de dados não normalizados (à direita) e normalizados pelo corte.\n\n\n\n\n\nAlém de normalizar os dados de entrada, é importante também normalizar as entradas das funções de ativação ou as saídas das camadas ocultas de uma rede neural. Se houver grande variação na faixa de valores das variáveis de uma determinada camada oculta, então normalizar esses valores para que tenham média zero e variância unitária deve tornar o problema de aprendizado mais fácil para a próxima camada. No entanto, diferentemente da normalização dos valores de entrada, que pode ser feita uma única vez antes do início do treinamento, a normalização dos valores das variáveis das camadas ocultas precisa ser repetida durante o treinamento, toda vez que os valores dos pesos forem atualizados. Isso é chamado de normalização em lote batch normalization, proposta por Ioffe e Szegedy em 2015. Uma motivação adicional para a batch normalization surge dos fenômenos de desvanecimento ou explosão dos gradientes, que costumam ocorrer no treinamento de redes neurais profundas.\nPara entender como a batch normalization é definida, considere uma camada oculta \\(j\\) de uma rede MLP com múltiplas camadas. A saída do neurônio \\(k\\) dessa camada é dada por \\(y_k^{(j)}=\\varphi_j(v_k^{(j)})\\). Portanto, temos a opção de normalizar os valores de \\(v_k^{(j)}\\) ou os valores de saída \\(y_k^{(j)}\\), \\(k=1, 2, \\cdots, N_j\\) da camada \\(j\\). Na prática, qualquer uma das abordagens pode ser utilizada, e aqui ilustramos o procedimento normalizando os valores de \\(v_k^{(j)}\\).\nComo os valores dos pesos são atualizados após cada mini-batch, aplica-se a normalização a cada mini-batch. Especificamente, para um mini-batch de tamanho \\(N_b\\), definem-se \\[\\mu_k^{(j)}=\\frac{1}{N_b}\\sum_{n=1}^{N_b}v_{k,n}^{(j)}\\] e \\[\n{\\sigma_k^{(j)}}^2=\\frac{1}{N_b}\\sum_{n=1}^{N_b}\\left(v_{k,n}^{(j)}-\\mu_k^{(j)}\\right)^2.\n\\] Em seguida, considera-se \\[\n\\widetilde{v}_{k,n}^{(j)}=\\frac{v_{k,n}^{(j)}-\\mu_k^{(j)}}{\\sqrt{{\\sigma_k^{(j)}}^2+\\delta}},\n\\] em que as somas levam em conta variáveis de um dado mini-batch já que \\(n=1,2,\\cdots, N_b\\) e \\(\\delta\\) é uma constante pequena utilizada para evitar problemas numéricos quando \\({\\sigma_k^{(j)}}^2\\) for pequeno.\nAo normalizar os valores de \\(v_k^{(j)}\\) em uma determinada camada da rede, reduzimos o número de graus de liberdade nos parâmetros dessa camada e, consequentemente, diminuímos sua capacidade de representação. Podemos compensar isso reescalando os valores de \\(\\widetilde{v}_{k,n}^{(j)}\\) do mini-batch para que tenham média \\(\\beta_k^{(j)}\\) e desvio padrão \\(\\gamma_k^{(j)}\\), usando \\[\n\\overset{\\approx}{v}_{k,n}^{(j)}=\\gamma_k^{(j)}\\widetilde{v}_{k,n}^{(j)}+\\beta_k^{(j)},\n\\] em que onde \\(\\beta_k^{(j)}\\) e \\(\\gamma_k^{(j)}\\) são parâmetros adaptativos aprendidos com o método do gradiente juntamente com os pesos e vieses da rede. Esses parâmetros ajustáveis representam uma diferença fundamental em relação à normalização dos dados de entrada.\nPode parecer que essa transformação simplesmente desfaz o efeito da normalização, já que a média e a variância agora podem novamente se adaptar a valores arbitrários. No entanto, a diferença crucial está na forma como os parâmetros evoluem durante o treinamento. Na rede original, a média e a variância em um mini-batch são determinadas por uma função complexa de todos os pesos e vieses da camada, enquanto na transformação com os parâmetros \\(\\beta_k^{(j)}\\) e \\(\\gamma_k^{(j)}\\), elas são determinadas diretamente por esses parâmetros independentes, que acabam sendo mais fáceis de aprender durante a descida do gradiente.\nA batch normalization pode ser vista como uma camada adicional na rede neural, de modo que cada camada oculta padrão pode ser seguida por uma camada de normalização em mini-batch. Uma vez que a rede foi treinada e queremos fazer previsões sobre novos dados, não temos mais disponíveis os mini-batches de treinamento e, portanto, não podemos determinar a média e a variância a partir de apenas um exemplo de dado. Para resolver isso, poderíamos, em princípio, calcular \\(\\mu_k^{(j)}\\) e \\({\\sigma_k^{(j)}}^2\\) para cada camada em todo o conjunto de treinamento após realizarmos a atualização final dos pesos e vieses. No entanto, isso exigiria processar todo o conjunto de dados apenas para avaliar essas quantidades e, portanto, costuma ser muito custoso. Em vez disso, calculam-se médias móveis de \\(\\mu_k^{(j)}\\) e \\({\\sigma_k^{(j)}}^2\\) ao longo do treinamento.\nEmbora a batch normalization seja muito eficaz na prática, há incerteza sobre por que ela funciona tão bem. Essa normalização foi originalmente motivada pela observação de que atualizações nos pesos das camadas anteriores da rede alteram a distribuição dos valores vistos pelas camadas posteriores, um fenômeno chamado de deslocamento interno de covariáveis (internal covariate shift). No entanto, estudos posteriores (Santurkar et al., 2018) sugerem que o deslocamento de covariáveis não é um fator significativo e que a melhoria no treinamento resulta, na verdade, de uma maior suavidade na função de erro.\n\n[(bihop2024?)](Google Developers 2023)"
  },
  {
    "objectID": "t_hiperparametros.html#função-de-ativação",
    "href": "t_hiperparametros.html#função-de-ativação",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "O cálculo do gradiente para atualização do vetor de pesos de um determinado neurônio requer o conhecimento da derivada da função de ativação \\(\\varphi(\\cdot)\\) associada a ele. Para atualização do gradiente, é importante que essa derivada exista e seja não nula. Por isso, as funções sinal e degrau, usadas no neurônio de Rosenblatt, não são adequadas. A seguir vamos descrever as funções de wativação mais usadas na MLP.\n\n\nA função sigmoidal, também conhecida como função logística, é definida como1\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm sgm}(a\\,v_k^{(j)})=\\displaystyle\\frac{1}{1+e^{-a\\, v_k^{(j)}}},\\;\\;\\;\\; a&gt;0,\n$}\n\\end{equation*}\\]\nem que \\(v_k^{(j)}\\) é o resultado da soma do bias com a combinação linear entre as entradas e os pesos do neurônio \\(k\\) da camada \\(j\\) e \\(a\\) é um parâmetro positivo ajustável. A derivada dessa função é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi'_j(v_k^{(j)})=\\frac{\\rm d}{{\\rm d}v_k^{(j)}}{\\rm sgm}(a\\,v_k^{(j)})=\\displaystyle \\frac{a\\,e^{-a\\, v_k^{(j)}}}{\\left[1+e^{-a\\, v_k^{(j)}}\\right]^2}= a \\varphi_j(v_k^{(j)})[1-\\varphi_j(v_k^{(j)})].\n$}\n\\end{equation*}\\]\nComo \\(\\varphi_j(v_k^{(j)})=y_k^{(j)}\\) é a saída do neurônio \\(k\\) da camada \\(j\\), ainda podemos escrever\n\\[\n\\varphi'_j(v_k^{(j)})= a\\, y_k^{(j)}(1-y_k^{(j)}).\n\\]\nNa Figura 1 são mostrados gráficos da função sigmoidal e de sua derivada para dois valores de \\(a\\). Pode-se observar que a saída do neurônio com função sigmoidal fica no intervalo \\([0,\\; 1]\\). Quanto maior o valor do parâmetro \\(a\\) mais abrupta é a mudança do patamar \\(0\\) para o patamar \\(1\\) e consequentemente maior a derivada em \\(v_k^{(j)}=0\\).\n\n\n\n\n\n\nFigura 1: Função sigmoidal e sua derivada para dois valores do parâmetro \\(a\\).\n\n\n\n\n\n\nOutra função de ativação muito utilizada na MLP é a tangente hiperbólica. Essa é a função de ativação que utilizamos nos experimentos com a rede MLP até agora (com \\(a=1\\)). A tangente hiperbólica é definida como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm tanh}(a\\,v_k^{(j)})=\\frac{e^{a\\,v_k^{(j)}}-e^{-a\\,v_k^{(j)}}}{e^{a\\,v_k^{(j)}}+e^{-a\\,v_k^{(j)}}},\\;\\;\\;a&gt;0,\n  $}\n\\end{equation*}\\] sendo \\(a\\) uma constante positiva. Sua derivada é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi_j'(v_k^{(j)})=\\frac{\\rm d}{{\\rm d}v_k^{(j)}}{\\rm tanh}(a\\,v_k^{(j)})=a\\,\\left[1-{\\rm tanh}^2(v_k^{(j)})\\right].\n$}\n\\end{equation*}\\]\nLembrando que a saída do neurônio \\(k\\) com função de ativação tangente hiperbólica é dada por \\(y_k^{(j)}={\\rm tanh}(v_k^{(j)})\\), também podemos escrever\n\\[\n\\varphi'_j(v_k^{(j)})=\\frac{1}{a}(a-y_k^{(j)})(a+y_k^{(j)}).\n\\]\nNa Figura 2 são mostrados gráficos da função tangente hiperbólica e de sua derivada para dois valores de \\(a\\). Pode-se observar que a saída do neurônio com essa função fica no intervalo \\([-1,\\; 1]\\). Quanto maior o valor do parâmetro \\(a\\) mais abrupta é a mudança do patamar \\(-1\\) para o patamar \\(1\\) e consequentemente maior a derivada em \\(v_k^{(j)}=0\\).\n\n\n\n\n\n\nFigura 2: Função tangente hiperbólica e sua derivada para dois valores do parâmetro \\(a\\).\n\n\n\n\n\n\nA unidade linear retificada (Rectified Linear Unit - ReLU) é uma função de ativação dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j(v_k^{(j)})={\\rm ReLU}(v_k^{(j)})=\\max(0, v_k^{(j)})=\\left\\{\\begin{array}{cc}\n                                     0, & v_k^{(j)}\\leq 0 \\\\\n                                     v_k^{(j)}, & v_k^{(j)}&gt;0\n                                   \\end{array}\n                                 \\right.\n\n$}\n\\end{equation*}\\]\nEssa função também é conhecida como função rampa e é análoga ao retificador de meia-onda, o que justifica seu nome. Sua derivada é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\varphi_j'(v_k^{(j)})={\\rm ReLU}'(v_k^{(j)})=\\left\\{\\begin{array}{cc}\n                                     0, & v_k^{(j)}&lt; 0 \\\\\n                                     1, & v_k^{(j)}&gt;0\\\\\n                                     \\nexists, & v_k^{(j)}=0.\n                                   \\end{array}\n                                 \\right.\n\n$}\n\\end{equation*}\\]\nNa Figura 3 são mostradas a função ReLU e a sua derivada. Observe que a função ReLU não é diferenciável em \\(v_k^{(j)}=0\\). Como ela é diferenciável em todos os outros valores de \\(v_k^{(j)}\\), o valor de sua derivada em zero pode ser arbitrariamente escolhido como 0 ou 1. Em geral, o treinamento de redes MLP profundas que usam essa função é mais rápido quando comparado ao treinamento das redes MLP que usam a tangente hiperbólica. Essa função foi baseada no princípio de que os modelos são mais facilmente otimizados quando o seu comportamento é próximo do linear.\n\n\n\n\n\n\nFigura 3: Função ReLU e sua derivada (a derivada em \\(v_k^{(j)}=0\\) foi arbitrariamente escolhida como 0)\n\n\n\nNa literatura, há diferentes variantes da ReLU, como:\n\nSoftplus;\nGaussian Error Linear Unit (GELU);\nLeaky rectified linear unit (Leaky ReLU);\nParametric rectified linear unit (PReLU);\nExponential linear unit (ELU);\nSigmoid linear unit (SiLU).\n\nAlgumas dessas funções são diferenciáveis em todos os pontos, o que evita ter que escolher arbitrariamente o valor da derivada em \\(v_k^{(j)}=0\\). Apesar da existência dessas variantes, a ReLU ainda é a mais utilizada em redes profundas. Ela apresenta algumas vantagens como:\n\nativação esparsa: em uma rede inicializada aleatoriamente, apenas 50% dos neurônios ocultos são ativados (saída não nula);\nmelhor propagação do gradiente: consegue escapar de mínimos locais em comparação com as funções sigmoidal ou tangente hiperbólica;\ncomputação eficiente;\ninvariante à escala: \\(\\max(0,\\,ax)= a\\,\\max(0,\\,x),\\;\\;a&gt;0\\).\n\nApesar dessas vantagens, a ReLU é ilimitada, o que pode levar o algoritmo de treinamento à divergência. Além disso, neurônios com ReLU podem se tornar inativos para essencialmente todas as entradas. Nesse estado, nenhum gradiente é retropropagado e o neurônio “morre”. Em alguns casos, muitos neurônios podem ficar inativos, diminuindo efetivamente a capacidade do modelo. Esse problema geralmente surge quando a taxa de aprendizado (passo de adaptação) é muito alta e pode ser evitado usando a função leaky ReLU, que atribui uma pequena inclinação positiva para entradas negativas.\n\n\n\nEm problemas de classificação multiclasse, é comum considerar uma rede com \\(N_L\\) neurônios de saída, sendo \\(N_L\\) o número de classes. Nesse caso, a saída esperada da rede é a ativação de apenas um dos \\(N_L\\) neurônios e a inativação dos \\(N_L-1\\) restantes. Para isso, costuma-se usar a função de ativação softmax nos neurônios de saída. Como a função sigmoidal, a função softmax limita a saída do neurônio entre 0 e 1. Porém, ela também leva em conta as saídas dos demais neurônios da camada. Dessa forma, considera-se uma normalização fazendo com que a soma de todas as saídas dos neurônios seja unitária, o que faz com que o vetor saída da rede seja um vetor de probabilidades. A função softmax para o \\(k\\)-ésimo neurônio da camada de saída é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi_L(v_k^{(L)})={\\rm Softmax}(v_k^{(L)})=\\frac{e^{v_k^{(L)}}}{\\displaystyle \\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}},\n$}\n\\end{equation*}\\]\nem que \\(0\\leq\\varphi_L(v_k^{(L)})\\leq 1\\) e \\(\\sum_{\\ell=1}^{N_L}\\varphi_L(v_\\ell^{(L)})=1\\). A derivada dessa função é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'_L(v_k^{(L)})={\\rm Softmax}'(v_k^{(L)})=\\frac{e^{v_k^{(L)}}\\left[\\displaystyle \\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}-e^{v_k^{(L)}}\\right]}{\\displaystyle \\left[\\sum_{\\ell=1}^{N_L}e^{v_\\ell^{(L)}}\\right]^2}.\n$}\n\\end{equation*}\\]"
  },
  {
    "objectID": "t_hiperparametros.html#função-custo",
    "href": "t_hiperparametros.html#função-custo",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "A escolha da função custo depende da finalidade da rede neural. Quando empregada em problemas de regressão, é comum usar o erro quadrático médio (Mean Squared Error - MSE) definido por\n\\[\\begin{equation}\\label{mse}\nJ_{\\rm MSE} = \\frac{1}{N_L} \\sum_{\\ell=1}^{N_L} e_{\\ell}^2(n),\n\\end{equation}\\] em que \\[\\begin{equation}\\label{e_n}\ne_{\\ell}(n) = d_{\\ell}(n) - y_{\\ell}^{(L)}(n)\n\\end{equation}\\]\nsão os erros dos neurônios da camada de saída da rede. Apesar de não ser a função custo mais adequada para problemas de classificação, o MSE foi utilizado nos problemas das meias-luas apresentados até o momento.\nQuando a rede é empregada em problemas de classificação, é comum usar a entropia cruzada, uma vez que ela é mais adequada para erros de categorização. No caso de classificação binária em que as categorias são \\(d = 0\\) ou \\(d=1\\) e existe apenas um neurônio de saída, a entropia cruzada é dada por\n\\[\nJ_{\\rm EC} = -  \\left[ d_1(n) \\ln\\left({y_{1}^{(L)}(n)}\\right) + [1 - d_1(n)] \\ln{\\left(1 -y_{1}^{(L)}(n)\\right)}\\right].\n\\]\nPara entender essa função, considere novamente o problema das meias-luas, mantendo \\(d=1\\) para a Região A, mas considerando que \\(d=0\\) para a Região B. Quando \\(y_1^{(L)}(n)\\geq 0,5\\) a rede classifica o dado como pertencente à Região A e para \\(y_1^{(L)}(n)&lt;0,5\\) o dado é classificado como pertencente à Região B. Dessa forma, a saída da rede pode ser interpretada como a probabilidade do dado de entrada pertencer à Região A. Quando \\(d_1(n)=y_1^{(L)}(n) \\in \\{0, 1\\}\\), \\(J_{\\rm EC}=0\\), que é o valor mínimo que essa função custo pode assumir. Para \\(d_1(n)=1\\) e \\(y_1^{(L)}(n)=0,\\!1\\), a rede erra, pois classifica o dado como pertencente à Região B enquanto ele de fato pertence à Região A e \\(J_{\\rm EC}=-1\\times \\ln(0,1)=2,\\!3026.\\) A função custo tem o mesmo valor para \\(d_1(n)=0\\) e \\(y_1^{(L)}(n)=0,\\!9\\), caso em que também há erro de classificação. No caso de classificação entre \\(N_L\\) classes, essa função é chamada de entropia cruzada categórica e é dada por\n\\[\nJ_{\\rm ECC} = -  \\frac{1}{N_L}\\sum_{\\ell=1}^{N_L} d_\\ell(n)  \\ln\\left(y_{\\ell}^{(L)}(n)\\right).\n\\]\nUma das maneiras de se reduzir overfitting é usar regularização na função custo. Isso controla o ajuste dos pesos, possibilitando que a rede tenha uma boa capacidade de generalização. A regularização \\(\\ell_2\\) é a mais comum e consiste em somar à função custo o termo \\[\\frac{\\lambda}{2N_L}\\sum_{\\ell=1}^{N_L}\\|\\mathbf{w}_\\ell^{(L)}(n-1)\\|^2,\\] em que \\(\\lambda\\) é um hiperparâmetro. Assim, ao minimizar a função custo somada a esse termo, o algoritmo também procura minimizar a norma dos vetores de peso da camada de saída, evitando dessa forma que ocorra divergência (Bishop 2006).\nExistem também outras funções custo cujas derivadas não são determinadas analiticamente, mas podem ser obtidas por diferenciação automática (autodiff), que é um conjunto de técnicas usadas para avaliar derivadas de funções numéricas expressas como programas de computador. Mais detalhes sobre autodiff podem ser obtidos em (Baydin et al. 2018)."
  },
  {
    "objectID": "t_hiperparametros.html#inicialização",
    "href": "t_hiperparametros.html#inicialização",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Sabemos que a inicialização é fundamental para que a rede MLP evite mínimos locais. Nos experimentos com as meias-luas que apresentamos até agora, os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2},\\;10^{-2}]\\). Como não se tem ideia dos valores dos parâmetros ótimos, de fato os pesos precisam ser inicializados de forma aleatória. O problema da forma que inicializamos é definir o intervalo da distribuição uniforme. O intervalo “ideal” depende do conjunto de dados, da arquitetura da rede etc. e sua escolha se torna mais difícil ainda em redes profundas. Além disso, uma pergunta que poderíamos fazer é: a inicialização dos parâmetros da rede considerando uma distribuição uniforme é a mais adequada?\nNo algoritmo backpropagation, o cálculo do gradiente local de uma determinada camada \\(j\\) da rede depende dos gradientes locais das camadas posteriores, ou seja, o gradiente local \\(\\delta^{(j)}_k\\) carrega consigo a multiplicação de todos os gradientes locais das camadas mais profundas da rede. Para redes neurais profundas, se os gradientes locais forem menores do que um, as atualizações dos pesos e biases das camadas mais rasas acabam assumindo valores muito pequenos, tornando o processo de aprendizado lento e ineficiente. Analogamente, para gradientes locais sempre maiores que um, as atualizações dos pesos das camadas menos profundas acabam assumindo valores muito elevados, levando o algoritmo de treinamento à divergência. Esse problema é conhecido como desvanecimento ou explosão dos gradientes. O objetivo das técnicas de inicialização de parâmetros é evitar esse problema. Dessa forma, os pesos e biases precisam ser inicializados dentro de um intervalo específico.\nA seguir, vamos abordar duas técnicas de inicialização frequentemente usadas na literatura (Brownlee 2021).\n\n\nA inicialização de Xavier foi proposta originalmente no artigo (Glorot e Bengio 2010). Para entender a ideia dessa inicialização, vamos primeiramente considerar que os neurônios da rede MLP têm função de ativação do tipo sigmoidal e pesos grandes. Como a função do tipo sigmoidal é plana para valores grandes da entrada, as ativações ficarão saturadas e os gradientes começarão a se aproximar de zero.\nPara evitar esse problema, a inicialização de Xavier busca garantir que a variância de \\(y^{(j)}_k\\) seja mantida igual ao longo das camadas, o que pode evitar o problema de desvanecimento ou explosão dos gradientes. Considerando função de ativação linear, temos\n\\[\ny_k^{(j)}=b_k^{(j)}+w_{k1}^{(j)}y_1^{(j-1)}+w_{k2}^{(j)}y_2^{(j-1)}+\\cdots+w_{kN_{j-1}}^{(j)}y_{N_{j-1}}^{(j-1)}.\n\\]\nCalculando a variância de \\(y_k^{(j)}\\), obtém-se\n\\[\n{\\rm var}(y_k^{(j)})={\\rm var}\\left(b_k^{(j)}+w_{k1}^{(j)}y_1^{(j-1)}+w_{k2}^{(j)}y_2^{(j-1)}+\\cdots+w_{kN_{j-1}}^{(j)}y_{N_{j-1}}^{(j-1)}\\right).\n\\]\nAssumindo que os biases foram inicializados com zero, sua variância também é nula. Portanto, precisamos calcular apenas a variância dos termos do lado direito da equação que contém os pesos. Assumindo independência entre os pesos e as entradas da camada \\(j\\), temos\n\\[\n{\\rm var}(w_{k\\ell}^{(j)}y_\\ell^{(j-1)})=[{\\rm E}\\{y_\\ell^{(j-1)}\\}]^2{\\rm var}(w_{k\\ell}^{(j)})+[{\\rm E}\\{w_{k\\ell}^{(j)}\\}]^2{\\rm var}(y_\\ell^{(j-1)})+{\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}),\n\\]\n\\(\\ell=1,2,\\cdots,N_{j-1}.\\) Considerando ainda que as entradas e os pesos têm médias nulas, a expressão anterior se reduz a\n\\[\n{\\rm var}(w_{k\\ell}^{(j)}y_\\ell^{(j-1)})={\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}).\n\\]\nUsando esse resultado no cálculo da variância de \\(y_k^{(j)}\\), chega-se a\n\\[\n{\\rm var}(y_k^{(j)})=N_{j-1}{\\rm var}(w_{k\\ell}^{(j)}){\\rm var}(y_\\ell^{(j-1)}).\n\\]\nComo se deseja que \\({\\rm var}(y_k^{(j)})={\\rm var}(y_\\ell^{(j-1)})\\), obtemos\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})=\\frac{1}{N_{j-1}}.\n\\]\nDiante desse resultado, a inicialização de Xavier propõe inicializar os pesos utilizando uma distribuição normal com média nula e desvio padrão \\(1/\\sqrt{N_{j-1}}\\), ou seja\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{1}{N_{j-1}}\\right).\n$}\n\\end{equation*}\\]\nUma variante dessa inicialização, conhecida na literatura como inicialização de Glorot, leva em conta também o número de número de neurônios da camada \\(j\\), ou seja\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{2}{N_{j-1}+N_j}\\right).\n$}\n\\end{equation*}\\]\nA ideia dessa inicialização é preservar também a variância do sinal retropropagado e para isso, considera que a variância do peso é aproximada por\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})\\approx\\frac{1}{(N_{j-1}+N_j)/2}.\n\\]\nHá ainda variantes dessas inicializações que utilizam a distribuição uniforme. Assim, a inicialização de Xavier com distribuição uniforme é\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{3}{N_{j-1}}},\\;+\\sqrt{\\frac{3}{N_{j-1}}}\\,\\right]\n\n$}\n\\end{equation*}\\]\ne a inicialização de Glorot com distribuição uniforme é\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{6}{N_{j-1}+N_j}},\\;+\\sqrt{\\frac{6}{N_{j-1}+N_j}}\\,\\right].\n$}\n\\end{equation*}\\]\n\n\n\nO problema de desvanecimento ou explosão dos gradientes visto com funções de ativação do tipo sigmoidal geralmente não ocorre quando se usa ReLU. Diante disso, foi proposta uma inicialização alternativa à de Xavier para neurônios que consideram ReLU, conhecida como inicialização de He, no artigo (He et al. 2015). Basicamente, a inicialização de He propõe que os pesos tenham o dobro da variância calculada anteriormente, ou seja,\n\\[\n{\\rm var}(w_{k\\ell}^{(j)})=\\frac{2}{N_{j-1}},\n\\]\no que leva à seguinte inicialização considerando a distribuição normal\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm N}\\left(0,\\,\\frac{2}{N_{j-1}}\\right)\n$}\n\\end{equation*}\\]\ne à seguinte variante para distribuição uniforme\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  w_{k\\ell}^{(j)}\\sim {\\rm U}\\left[-\\sqrt{\\frac{6}{N_{j-1}}},\\;+\\sqrt{\\frac{6}{N_{j-1}}}\\,\\right].\n$}\n\\end{equation*}\\]"
  },
  {
    "objectID": "t_hiperparametros.html#passo-de-adaptação",
    "href": "t_hiperparametros.html#passo-de-adaptação",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Um dos principais hiperparâmetros que precisam ser ajustados no treinamento de uma rede neural é o passo de adaptação ou taxa de aprendizagem. Se o passo for muito baixo, a convergência do algoritmo de treinamento será muito lenta como mostrado na Figura 4 (a). Em contrapartida, um passo muito elevado pode levar o algoritmo à divergência, caso ilustrado na Figura 4 (c). Na Figura 4 (b), considera-se um passo ideal que proporciona uma rápida convergência. O passo de adaptação ideal depende da superfície de desempenho, que, por sua vez, depende da arquitetura da rede e do conjunto de dados. O treinamento da rede pode ser acelerado quando se utiliza uma taxa de aprendizagem ideal (Jordan 2018).\n\n\n\n\n\n\nFigura 4: Três passos de adaptação diferentes: (a) passo muito baixo que requer muitas iterações até que o algoritmo atinja o mínimo da função custo; (b) passo ótimo que faz com que o algoritmo atinja o mínimo rapidamente e (c) passo muito elevado que pode levar o algoritmo à divergência. [Fonte].\n\n\n\nUma das técnicas mais utilizadas para ajustar o passo de adaptação é a learning rate annealing. Nessa técnica, o valor do passo deve ser relativamente alto no início e diminuir gradualmente ao longo do treinamento. Com um passo elevado no início do treinamento, os pesos e biases são ajustados rapidamente para valores “bons”, ou seja, uma taxa alta pode fazer com que o algoritmo “pule” mínimos locais. Em seguida, uma taxa de aprendizagem pequena faz um ajuste fino, possibilitando o algoritmo explorar as partes “mais profundas” da função custo. A forma mais comum de fazer isso é considerar o decaimento do passo em escada ou exponencial, como ilustrado na Figura 5.\nNo decaimento em escada com degraus uniformes da Figura 5 (a), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0-\\Delta \\eta \\lfloor k/\\Delta k\\rfloor,\n\\]\nem que \\(\\eta_0\\) é o valor inicial do passo, \\(\\Delta \\eta\\) o valor do decaimento e \\(\\Delta k\\) o número de épocas em que o passo é mantido fixo. No caso da Figura 5 (a), foram usados \\(\\eta_0=0,\\!1\\), \\(\\Delta \\eta=0,\\!0101\\) e \\(\\Delta k=20\\).\nNo decaimento em escada com degraus não uniformes da Figura 5 (b), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0\\Delta \\eta^{\\lfloor k/\\Delta k\\rfloor}.\n\\]\nNo caso da Figura 5 (b), foram usados \\(\\eta_0=0,\\!1\\), \\(\\Delta \\eta=0,\\!5\\) e \\(\\Delta k=20\\).\nPor fim, no decaimento exponencial da Figura 5 (c), o passo da \\(k\\)-ésima época é calculado como\n\\[\n\\eta(k)=\\eta_0 e^{-a k},\\;\\;a&gt;0.\n\\]\nNo caso da Figura 5 (c), foram usados \\(\\eta_0=0,\\!1\\) e \\(a=0,\\!01\\).\n\n\n\n\n\n\nFigura 5: Learning rate annealing: (a) decaimento em escada uniforme, (b) decaimento em escada não uniforme e (c) decaimento exponencial.\n\n\n\nO desafio de usar esquemas de ajuste dos passos de adaptação é que seus hiperparâmetros precisam ser definidos com antecedência e dependem da arquitetura da rede e do problema. Além disso, pode ser conveniente adaptar pesos de neurônios de camadas diferentes com passos distintos. Algoritmos de otimização como Adam e RMSprop resolvem esses problemas, pois ajustam os passos de adaptação de forma automática com o uso de regularização, como veremos posteriormente."
  },
  {
    "objectID": "t_hiperparametros.html#mini-batch",
    "href": "t_hiperparametros.html#mini-batch",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Abordamos anteriormente o treinamento do algoritmo LMS nos modos batch, mini-batch e estocástico2. Como o algoritmo LMS foi proposto para aplicações de tempo real, o modo estocástico é o mais utilizado. A cada dado de entrada se deseja ter o dado de saída correspondente com o menor atraso possível, ou seja, o treinamento ocorre junto com a inferência. A saída e o erro calculados no treinamento são utilizados para atualizar os pesos e ao mesmo tempo para se obter a estimativa ou classificação desejada.\nNo caso das redes neurais, o modo mini-batch é o mais utilizado. Geralmente, a inferência não é realizada durante o treinamento. A saída e o erro são utilizados no treinamento apenas para atualizar os pesos do algoritmo. Depois do treinamento, fixam-se os pesos para então se fazer a inferência e testar o classificador ou regressor. Apesar de termos abordado os três modos de treinamento apenas no algoritmo LMS, a extensão para redes neurais é direta.\nO uso de mini-batch no processo de aprendizado consiste em dividir aleatoriamente o conjunto de treinamento da rede em blocos de tamanho previamente definido, embaralhando-se as amostras do conjunto. A atualização dos pesos e biases ocorre apenas depois que são calculados os gradientes de todos os elementos de um mini-batch. Dessa forma, a atualização dos parâmetros da rede está associada à média dos gradientes de um mini-batch. Considera-se passada uma época quando todos os mini-batches são percorridos. Após cada época do algoritmo de otimização, a divisão do conjunto de treinamento entre mini-batches é refeita de maneira aleatória, embaralhando-se novamente o conjunto de treinamento. O tamanho de cada mini-batch é um hiperparâmetro e não muda no decorrer das épocas.\nQuando se considera que cada mini-batch é formado apenas por uma amostra do conjunto de treinamento, diz-se que o método de atualização de parâmetros é estocástico. O uso do método estocástico para atualização de parâmetros de uma rede neural é pouco eficiente, pois a atualização ocorre em direções distintas do mínimo da função custo, o que faz com que o algoritmo leve mais épocas para convergir. O método estocástico também anula as vantagens computacionais de uma implementação matricial do algoritmo, uma vez que as atualizações são realizadas sobre cada amostra de treinamento.\nQuando um mini-batch possui todos os elementos do conjunto de treinamento, nomeia-se o método de atualização de parâmetros apenas como batch. Com o método batch, os parâmetros são sempre atualizados na direção do mínimo da função custo. Diante disso, o batch seria o modo de treinamento ideal se não houvesse limitações computacionais. Como é necessário esperar que todo o conjunto de treinamento seja percorrido para se realizar a atualização dos parâmetros, o modo de treinamento batch é muito demorado e computacionalmente ineficiente quando comparado com o mini-batch."
  },
  {
    "objectID": "t_hiperparametros.html#dropout",
    "href": "t_hiperparametros.html#dropout",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Outro problema que pode aparecer no treinamento das redes neurais é o overfitting, que ocorre quando há uma diferença significativa entre o desempenho da rede sobre seu conjunto de treinamento e sobre um outro conjunto distinto de dados, o conjunto de teste. Neste caso, a rede se especializa tanto no conjunto de treinamento, que não apresenta capacidade de generalização satisfatória para outros dados. Uma das técnicas mais utilizadas para evitar esse problema é o dropout. Essa técnica basicamente inativa aleatoriamente, em cada iteração do algoritmo backpropagation, diferentes neurônios de cada camada oculta da rede. Cada neurônio é inativado com probabilidade \\(p\\), sendo \\(p\\) o hiperparâmetro associado a essa esquema. Na Figura 6, exemplifica-se a aplicação do dropout com \\(p = 0,\\!5\\). Observe que metade dos neurônios de cada camada oculta (neurônios destacadas em vermelho) foram inativados em uma determinada iteração. Quando um neurônio é inativado, seu gradiente é nulo de modo que seus pesos não são atualizados. Heuristicamente, a eliminação temporária de diferentes conjuntos de neurônios leva ao treinamento de redes neurais distintas. Dessa forma, o procedimento de eliminação é equivalente ao cálculo da média dos efeitos de um grande número de redes distintas. Como elas vão se adaptar de diferentes maneiras, isso possibilita a redução do overfitting, pois será mais difícil para a rede se especializar nos dados de treinamento (Goodfellow, Bengio, e Courville 2016).\n\n\n\n\n\n\nFigura 6: Exemplo de aplicação do dropout em uma rede MLP com \\(p = 0,5\\)."
  },
  {
    "objectID": "t_hiperparametros.html#momentum",
    "href": "t_hiperparametros.html#momentum",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Como vimos anteriormente, o algoritmo LMS é uma aproximação estocástica do algoritmo do gradiente exato (steepest descent). Vimos também que existe um compromisso entre a velocidade de convergência e a precisão da solução. Quanto menor o passo de adaptação, mais lento é o algoritmo e os pesos variam menos em torno da solução de Wiener. Quanto maior o passo, maior a sua velocidade de convergência e maior também a variação dos pesos torno da solução ótima. O algoritmo também pode divergir dependendo do valor do passo e neste caso, os pesos vão para infinito. O mesmo acontece com o algoritmo backpropagation: quanto menor for o passo de adaptação, menores serão as mudanças nos pesos da rede de uma iteração para outra, mais suave será a trajetória no espaço dos pesos e mais lenta a taxa de aprendizagem. Se aumentarmos muito o passo de adaptação para acelerar a taxa de aprendizagem, as mudanças dos pesos de uma iteração para outra também aumentam e o algoritmo pode divergir.\nUm método simples de aumentar a taxa de aprendizagem sem causar divergência é modificar a adaptação do backpropagation incluindo um termo chamado momentum. Antes de introduzir esse termo, vamos lembrar da atualização da matriz de pesos da Camada \\(j\\) da MLP com o algoritmo backpropagation:\n\\[\n\\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n),\n\\]\nem que\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T}.\n\\]\nDefinindo agora a matriz \\[\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)\\triangleq\\mathbf{W}^{(j)}(n-1)-\\mathbf{W}^{(j)}(n-2),\n\\]\nque representa a diferença entre a matriz de pesos da iteração \\(n-1\\) e da iteração \\(n-2\\), a atualização do backpropagation com momentum fica\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\mathbf{W}^{(j)}(n)=\\mathbf{W}^{(j)}(n-1)+\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n),\n$}\n\\end{equation*}\\]\nem que \\(0\\leq \\alpha&lt;1\\) é a constante de momentum. Observe que \\(\\alpha=0\\) leva essa atualização à forma padrão do backpropagation sem momentum. Usando a definição \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\), podemos reescrever essa equação de atualização como\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n  \\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n-1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n).\n$}\n\\end{equation*}\\]\nPara entender o efeito do termo de momentum, note que\n\\[\\begin{align*}\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(1)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(2)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(1)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\nonumber\\\\\n&=\\alpha\\left[\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)\\right]+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\nonumber\\\\\n&=\\alpha^2\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\left[\\alpha\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)+\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)\\right]\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(3)&=\\alpha\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(2)+\\eta\\boldsymbol{\\Delta}_{\\delta}^{(j)}(3)\\nonumber\\\\\n&=\\alpha^3\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\left[\\alpha^2\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)+\\alpha\\boldsymbol{\\Delta}_{\\delta}^{(j)}(2)+\\boldsymbol{\\Delta}_{\\delta}^{(j)}(3)\\right]\\nonumber\\\\\n&\\vdots\\nonumber\\\\\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)&=\\alpha^n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)+\\eta\\sum_{k=1}^n \\alpha^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\\nonumber\n\\end{align*}\\]\nO termo \\(\\alpha^n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(0)\\) tende a zero a medida que o número de iterações aumenta, uma vez que \\(0\\leq\\alpha&lt;1\\) e os pesos são inicializados com valores finitos. Assim, podemos escrever\n\\[\n\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)=\\eta\\sum_{k=1}^n \\alpha^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\n\\]\nEssa equação nos possibilita entender os efeitos benéficos do momentum, enumerados a seguir (Haykin 2009):\n\no ajuste \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) representa a soma de uma série temporal ponderada exponencialmente. Como \\(0\\leq \\alpha&lt;1\\), consideram-se pesos maiores para ajustes recentes e pesos menores para os mais antigos. Dessa forma, \\(\\alpha\\) também é chamado na literatura de fator de esquecimento;\nquando o termo \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) tem o mesmo sinal algébrico em sucessivas iterações, a matriz \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) cresce em magnitude e a matriz de pesos \\(\\mathbf{W}^{(j)}(n)\\) é ajustada com uma grande quantidade. Diante disso, o momentum tende a acelerar a convergência do backpropagation em direções de descida mais íngreme;\nquando o sinal algébrico do termo \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) muda em sucessivas iterações, a matriz \\(\\boldsymbol{\\Delta}\\mathbf{W}^{(j)}(n)\\) diminui em magnitude e a matriz de pesos \\(\\mathbf{W}^{(j)}(n)\\) é ajustada com uma pequena quantidade. Diante disso, o momentum tem o efeito de estabilizador em direções que oscilam em sinal.\n\nEm suma, a incorporação do momentum no algoritmo backpropagation pode trazer alguns efeitos benéficos no aprendizado, incluindo a possibilidade de evitar que o algoritmo fique estagnado em um mínimo local.\nA seguir vamos comparar o backpropagation com e sem momentum.\n\n\nNo exemplo das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), vimos que uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada com o backpropagation sem momentum é capaz de classificar corretamente os dados dependendo da inicialização. Para verificar o efeito benéfico de se utilizar momentum, vamos considerar uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(10(\\text{tanh})\\)-\\(1(\\text{tanh})\\). Essa mudança de configuração se deve ao fato de que o backpropagation com momentum na configuração anterior se comporta de maneira análoga ao caso sem momentum. Considerou-se ainda o modo de treinamento mini-batch (\\(N_t=1000,\\) \\(N_b=50\\) e \\(N_e=2000\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\), o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!1\\) e a constante de momentum igual a \\(\\alpha=0,\\!9\\). Além disso, considerou-se a função custo do erro quadrático médio (MSE).\nNa Figura 7, são mostradas a função custo ao longo das épocas de treinamento, a classificação dos dados de teste e a separação das regiões para uma determinada inicialização. Verifica-se que o algoritmo backpropagation sem momentum não consegue escapar do mínimo local, obtendo \\(7,\\!9\\%\\) de taxa de erro de classificação. Ao se utilizar momentum, percebe-se que o algoritmo apresenta um MSE próximo do caso sem momentum durante as \\(450\\) épocas iniciais do treinamento. Depois disso, eles seguem caminhos diferentes: o algoritmo sem momentum fica parado no mínimo local correspondente a um MSE aproximadamente \\(-7\\) dB, enquanto o algoritmo com momentum consegue atingir um MSE de aproximadamente \\(-43\\) dB na época \\(2000\\). Isso é suficiente para não gerar erros de classificação.\n\n\n\n\n\n\nFigura 7: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento, classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões obtidas com uma rede MLP \\(2\\)-\\(3(\\text{tanh})\\)-\\(10(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada em mini-batch (\\(N_t=1000\\), \\(N_b=50\\)) com o algoritmo backpropagation sem momentum (\\(\\eta=0,\\!1\\)) e com momentum (\\(\\eta=0,1\\), \\(\\alpha=0,\\!9\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\nO comportamento observado na Figura 7 nem sempre se repete, pois depende da inicialização. Em muitos casos, os algoritmos com e sem momentum apresentam comportamentos semelhantes. Ainda podem ocorrer situações em que o algoritmo com momentum não consegue evitar mínimos locais, enquanto o algoritmo sem momentum consegue. Apesar disso, o uso de momentum é considerado benéfico na maior parte das vezes. Isso de deve ao fato de que quando implementado junto com outras técnicas pode fazer com que a rede atinja valores de MSE mais baixos no treinamento, o que é indício de que mínimos locais foram evitados."
  },
  {
    "objectID": "t_hiperparametros.html#otimizador-adam",
    "href": "t_hiperparametros.html#otimizador-adam",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "A escolha do algoritmo de otimização é essencial em Aprendizado de Máquina. O algoritmo de otimização Adam (adaptive moment estimation) (Kingma e Ba 2015) é uma extensão do algoritmo do gradiente estocástico e tem sido muito utilizado recentemente. Ao introduzir o algoritmo, os autores listam os benefícios de se usar Adam em problemas de otimização não convexa:\n\nsimples de implementar, computacionalmente eficiente e requer poucos requisitos de memória;\nadequado quando se usa muitos dados e/ou parâmetros;\napropriado para problemas não estacionários e problemas com gradientes muito ruidosos e/ou esparsos; e\nos hiperparâmetros têm interpretação intuitiva e são simples de ajustar.\n\nO otimizador Adam atualiza os pesos e biases de uma rede neural a partir dos gradientes calculados na iteração atual e em iterações passadas, de forma a tornar mais estável o processo de aprendizado da rede, evitando-se assim variações excessivas em direções que não são a do mínimo da função custo. Ele combina o gradiente estocástico com momentum com o otimizador RMSprop (root mean squared propagation). Para introduzir esse otimizador, vamos antes introduzir o otimizador RMSprop.\nÀ medida que os dados se propagam na rede, os gradientes calculados para atualização dos parâmetros podem ficar muito pequenos ou muito grandes. Gradientes muito pequenos podem levar à estagnação do backpropagation. Em contrapartida, gradientes muito grandes podem levar à divergência do algoritmo. O otimizador RMSprop foi proposto por G. Hinton, um dos “pais” do backpropagation, para lidar com esse problema usando uma média móvel dos gradientes ao quadrado. Isso gera uma normalização no algoritmo, que passa a ser encarado como um algoritmo de passo variável. Assim, quando os gradientes são grandes, o método diminui o passo para evitar a divergência e quando os gradientes são pequenos, ele aumenta o passo para evitar a estagnação. A título de curiosidade, o algoritmo RMSprop foi proposto por Hinton na sexta aula do curso Neural Networks for Machine Learning e diferente do Adam, não foi publicado.\nQuando deduzimos o algoritmo backpropagation, definimos a matriz\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\boldsymbol{\\delta}^{(j)}(n)[\\mathbf{x}^{(j)}(n)]^{\\rm T},\n\\]\nque contém o negativo dos vetores gradiente de todos os neurônios da Camada \\(j\\). Vamos agora definir a matriz \\(\\mathbf{S}^{(j)}(n)\\), calculada recursivamente como\n\\[\n\\mathbf{S}^{(j)}(n) = \\beta_2\\mathbf{S}^{(j)}(n-1) + (1-\\beta_2)\\left[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\right]^{\\odot 2},\n\\]\nem que \\(\\mathbf{S}^{(j)}(0)=\\boldsymbol{0}\\), \\(0\\ll \\beta_2&lt; 1\\) é um hiperparâmetro que faz o papel de um fator de esquecimento e a operação \\([\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)]^{\\odot 2}\\) indica que cada elemento da matriz \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\) é elevado ao quadrado. Levando em conta a inicialização com valores nulos, a equação recursiva para a matriz \\(\\mathbf{S}^{(j)}(n)\\) pode ser reescrita como\n\\[\n\\mathbf{S}^{(j)}(n)=(1-\\beta_2)\\displaystyle \\sum_{k=1}^{n}\\beta_2^{n-k}\\left[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k)\\right]^{\\odot 2}.\n\\]\nA menos da constante \\((1-\\beta_2)\\), observa-se que essa estimativa considera pesos maiores para os gradientes ao quadrado mais recentes e pesos menores para os mais antigos, o que caracteriza uma janela exponencial. Utilizando a matriz \\(\\mathbf{S}^{(j)}(n)\\), a atualização dos pesos e biases da Camada \\(j\\) da rede segundo o otimizador RMSprop é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n) = \\mathbf{W}^{(j)}(n-1) + \\eta\\;{\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)} \\oslash \\left[{\\left[{\\mathbf{S}}^{(j)}(n)\\right]^{\\odot \\frac{1}{2}} + \\varepsilon}\\mathbf{1}\\right],\n$}\n\\end{equation*}\\]\nem que \\(\\oslash\\) se refere a divisão de Hadamard, que resulta em uma matriz em que cada elemento é igual à divisão do respectivo elemento da matriz à esquerda pelo respectivo elemento da matriz à direita, \\(\\varepsilon\\) é uma constante positiva pequena (e.g., \\(\\varepsilon=10^{-8}\\)) usada para evitar divisões por zero e \\(\\mathbf{1}\\) é uma matriz com todos os elementos iguais a 1 e dimensões adequadas para que a soma seja possível de ser calculada. Para entender melhor essas operações, suponha que na iteração \\(n\\) dispomos das matrizes\n\\[\n\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)=\\left[\\begin{array}{cc}\n                a & b \\\\\n                c & d\n              \\end{array}\n\\right]\\;\\;\\;\\text{e}\\;\\;\\; {\\mathbf{S}}^{(j)}(n)=\\left[\\begin{array}{cc}\n                 e & f \\\\\n                 g & h\n               \\end{array}\n\\right].\n\\]\nAssim,\n\\[\n{\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)} \\oslash \\left({\\left[{\\mathbf{S}}^{(j)}(n)\\right]^{\\odot \\frac{1}{2}} + \\varepsilon\\mathbf{1}}\\right)=\\left[\\begin{array}{ccc}\n                 \\displaystyle\\frac{a}{\\sqrt{e}+\\varepsilon} && \\displaystyle\\frac{b}{\\sqrt{f}+\\varepsilon} \\\\\n                 &&\\\\\n                 \\displaystyle\\frac{c}{\\sqrt{g}+\\varepsilon} && \\displaystyle\\frac{d}{\\sqrt{h}+\\varepsilon}\n               \\end{array}\n\\right].\n\\]\nEm vez de usar o negativo dos gradientes de \\(\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\\), o otimizador Adam também considera uma janela exponencial para estimar esses gradientes. Para isso, define-se a matriz\n\\[\n\\mathbf{V}^{(j)}(n) = \\beta_1\\mathbf{V}^{(j)}(n-1) + (1-\\beta_1)\\boldsymbol{\\Delta}_{\\delta}^{(j)}(n)\n\\]\nem que \\(\\mathbf{V}^{(j)}(0)=\\boldsymbol{0}\\) e \\(0\\ll \\beta_1&lt; 1\\) é um hiperparâmetro que também faz o papel de um fator de esquecimento. Novamente, levando em conta a inicialização com valores nulos, a equação recursiva para \\(\\mathbf{V}^{(j)}(n)\\) pode ser reescrita como\n\\[\n\\mathbf{V}^{(j)}(n)=(1-\\beta_1)\\displaystyle \\sum_{k=1}^{n}\\beta_1^{n-k}\\boldsymbol{\\Delta}_{\\delta}^{(j)}(k).\n\\]\nAs inicializações das matrizes \\(\\mathbf{S}^{(j)}\\) e \\(\\mathbf{V}^{(j)}\\) com elementos nulos podem gerar distorções no início do treinamento do algoritmo. Observe que na atualização do RMSprop, o valor da matriz \\(\\mathbf{S}^{(j)}(n)\\) para \\(n=1\\) é \\(\\mathbf{S}^{(j)}(1)=(1-\\beta_2)[\\boldsymbol{\\Delta}_{\\delta}^{(j)}(1)]^{\\odot 2}\\), o que tende a ser muito pequeno já que \\(0\\ll \\beta_2 &lt;1\\). Para amenizar isso, são definidas as as matrizes de correção\n\\[\\begin{align*}\n\\overline{\\mathbf{V}}^{(j)}(n) &= \\frac{1}{1 - \\beta_1^n}\\,{\\mathbf{V}^{(j)}(n)}\\;\\; \\textnormal{e} \\nonumber\\\\\n\\overline{\\mathbf{S}}^{(j)}(n) &= \\frac{1}{1 - \\beta_2^n}\\,{\\mathbf{S}^{(j)}(n)}. \\nonumber\n\\end{align*}\\]\nComo \\(0\\ll \\beta_1, \\beta_2&lt;1\\), as matrizes corrigidas \\(\\overline{\\mathbf{V}}^{(j)}(n)\\) e \\(\\overline{\\mathbf{S}}^{(j)}(n)\\) tendem às matrizes \\({\\mathbf{V}}^{(j)}(n)\\) e \\({\\mathbf{S}}^{(j)}(n)\\), respectivamente, a medida que \\(n\\) aumenta. Ou seja, o efeito da correção ocorre apenas no início do treinamento, como esperado. Utilizando essas matrizes corrigidas, a atualização dos pesos e bias da Camada \\(j\\) da rede segundo o otimizador Adam é dada por\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{W}^{(j)}(n) = \\mathbf{W}^{(j)}(n-1) + \\eta\\;{\\overline{\\mathbf{V}}^{(j)}(n)} \\oslash \\left[{\\left[{\\overline{\\mathbf{S}}^{(j)}(n)}\\right]^{\\odot \\frac{1}{2}} + \\varepsilon\\mathbf{1}}\\right].\n$}\n\\end{equation*}\\]\nA seguir vamos comparar o backpropagation com o RMSprop e Adam.\n\n\nNo exemplo das meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), vimos que uma MLP com configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada com o backpropagation sem momentum é capaz de classificar corretamente os dados dependendo da inicialização. No entanto, quando consideramos uma rede mais profunda, a probabilidade do backpropagation de ficar parado em mínimos locais é alta. Como exemplo, vamos considerar uma MLP com cinco camadas e configuração \\(2\\)-\\(3(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) no modo de treinamento mini-batch (\\(N_t=1000\\), \\(N_b=50\\) e \\(N_e=5000\\)). Os pesos e biases foram inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\) e o passo de adaptação foi considerado fixo e igual a \\(\\eta=0,\\!5\\) para todos os algoritmos. Além disso, considerou-se a função custo do erro quadrático médio (MSE). Por fim, os hiperparâmetros do algoritmo RMSprop foram selecionados como \\(\\beta_2=0,\\!99\\) e \\(\\varepsilon=10^{-4}\\) e do Adam como \\(\\beta_1=\\beta_2=0,\\!99\\) e \\(\\varepsilon=10^{-4}\\).\nNa Figura 8, na Figura 9 e na Figura 10, são mostradas a função custo ao longo das épocas, a classificação dos dados de teste e a separação das regiões para três inicializações diferentes, respectivamente. Nos três casos, verifica-se que o algoritmo backpropagation sem momentum, denotado como SGD (stochastic gradient), não consegue escapar do mínimo local, obtendo \\(50\\%\\) de taxa de erro de classificação. No caso da Figura 8, os comportamentos do RMSprop e do Adam são muito parecidos: ambos atingem um MSE de aproximadamente \\(-100\\) dB no treinamento e apresentam taxas de erro de classificação iguais a zero. Mudando a inicialização, observamos na Figura 9 um comportamento diferente para o Adam, que apesar de escapar de um mínimo local logo depois da época \\(3700\\), apresenta um MSE que oscila em torno de \\(-15\\) dB, o que levou a um erro de classificação de \\(1\\%\\). Neste caso, o RMSprop atinge novamente um MSE de aproximadamente \\(-100\\) dB no treinamento e mantém a taxa de erro de classificação igual a zero. Mudando novamente a inicialização, observamos na Figura 10 que o Adam atingiu novamente o patamar de \\(-100\\) dB no treinamento e \\(0\\%\\) de taxa de erro de classificação. Já o RMSprop ficou parado em um mínimo local que levou a um MSE de aproximadamente \\(-7\\) dB e a uma taxa de erro de classificação de \\(5,\\!5\\%\\).\nA partir desse experimento, verifica-se que mudar o algoritmo de otimização é benéfico para evitar mínimos locais, principalmente quando comparamos o RMSprop e o Adam com o SGD em redes profundas. No entanto, a adoção de um desses algoritmos de otimização apenas não é suficiente para evitar mínimos locais, como vimos na Figura 9 e na Figura 10. Considerando o Adam e o RMSprop, observa-se na literatura que o Adam tem sido preferido na maior parte das aplicações. No entanto, o Adam tem algumas desvantagens:\n\nnão converge adequadamente em alguns exemplos simples, como pudemos comprovar no exemplo da Figura 9;\no erro de generalização pode ser grande em muitos problemas de visão computacional;\nrequer mais memória que o método do gradiente (SGD); e\ntem dois hiperparâmetros e portanto, alguns ajustes podem ser necessários.\n\n\n\n\n\n\n\nFigura 8: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento, classificação dos dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões obtidas com uma rede MLP \\(2\\)-\\(3(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(4(\\text{tanh})\\)-\\(2(\\text{tanh})\\)-\\(1(\\text{tanh})\\) treinada em mini-batch (\\(N_t=1000\\), \\(N_b=50\\)) com o algoritmo SGD (\\(\\eta=0,5\\)), RMSprop (\\(\\eta=0,5\\), \\(\\beta_2=0,99\\), \\(\\varepsilon=10^{-4}\\)) e Adam (\\(\\eta=0,5\\), \\(\\beta_1=\\beta_2=0,99\\), \\(\\varepsilon=10^{-4}\\)); pesos e biases inicializados com números aleatórios gerados a partir de uma distribuição uniforme no intervalo \\([-10^{-2}, 10^{-2}]\\).\n\n\n\n\n\n\n\n\n\nFigura 9: Veja legenda da Figura 8.\n\n\n\n\n\n\n\n\n\nFigura 10: Veja legenda da Figura 8."
  },
  {
    "objectID": "t_hiperparametros.html#validação-cruzada",
    "href": "t_hiperparametros.html#validação-cruzada",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "A essência do aprendizado de uma rede MLP com o algoritmo backpropagation é aproximar um mapeamento entrada-saída por meio dos pesos e biases, utilizado um conjunto de exemplos rotulados. Espera-se que a rede aprenda o suficiente com os dados do passado e que seja capaz de generalizar para dados futuros. No processo de aprendizagem, é importante selecionar a “melhor” rede (número de camadas, número de neurônios, funções de ativação, passo de adaptação, etc.) dentro de um conjunto de redes candidatas, considerando um determinado critério. Além disso, o MSE tende a diminuir monotonicamente ao longo das épocas de treinamento. Em geral, quanto maior o número de épocas, mais baixo é o MSE. No entanto, um MSE baixo no treinamento não corresponde necessariamente a um desempenho satisfatório da rede com o conjunto de teste, ou seja, pode haver overfitting. A pergunta que cabe fazer aqui é: quando devemos parar de treinar já que um treinamento longo pode gerar overfitting?\nPara responder essa pergunta e selecionar a melhor rede, é comum utilizar um conjunto de dados de validação. Neste caso, o conjunto de dados disponível deve ser primeiramente particionado de maneira aleatória entre treinamento e teste. O conjunto de treinamento, por sua vez, deve ser particionado em dois subconjuntos disjuntos:\n\nsubconjunto de estimação, usado para treinar o modelo;\nsubconjunto de validação, usado para testar o modelo durante o treinamento.\n\nA ideia de usar um conjunto de validação distinto do conjunto de treinamento e de teste é validar o modelo durante o treinamento com um conjunto de dados diferente do utilizado para estimar os parâmetros. A avaliação final do modelo para observar sua capacidade de generalização deve ser sempre feita com os dados do conjunto de teste, que não foram usados durante o treinamento, considerando fixos os pesos e biases da rede.\nNormalmente, uma rede MLP treinada com o algoritmo backpropagation aprende em etapas, passando da realização de funções de mapeamento simples para funções de mapeamento mais complexas à medida que o treinamento avança. Esse processo pode ser verificado pela diminuição do MSE ao longo das épocas de treinamento: ele começa com um valor alto, diminui rapidamente e depois continua a diminuir lentamente quando a rede atinge um mínimo local da superfície de erro. Como o principal objetivo é obter uma rede com uma boa capacidade de generalização, é muito difícil descobrir quando parar de treinar, baseando-se apenas na curva de aprendizado do treinamento. Em particular, é possível que ocorra overfitting se o treinamento não for interrompido no ponto certo.\nPodemos identificar o começo do overfitting por meio da validação cruzada (cross-validation). O subconjunto de exemplos de estimação é usado para treinar a rede da maneira usual, exceto por uma pequena modificação: o treinamento é interrompido periodicamente depois de um determinado número de épocas e e a rede é testada com o subconjunto de validação. Mais especificamente, o “processo de estimação seguido de validação” periódico ocorre da seguinte forma (Haykin 2009):\n\napós um intervalo de treinamento - a cada cinco épocas, por exemplo - os pesos e biases da MLP são mantidos fixos e apenas o cálculo progressivo é realizado. O erro de validação é então medido para cada exemplo do subconjunto de validação;\nquando a fase de validação é concluída, o treinamento é retomado em um novo intervalo e o processo é repetido.\n\nNa Figura 11 são mostradas duas curvas de aprendizado: uma obtida com o subconjunto de estimação (treinamento) e outra obtida com os dados do subconjunto de validação. Normalmente, o modelo não se sai tão bem no subconjunto de validação quanto no subconjunto de estimação. A curva de aprendizado de estimação diminui monotonicamente ao longo das épocas. Em contrapartida, a curva de validação diminui monotonicamente até um ponto de mínimo e a partir deste ponto começa a aumentar à medida que o o treinamento continua. Observando a curva de aprendizado de estimação, pode parecer que seria melhor continuar o treinamento além do ponto de mínimo da curva de validação. No entanto, o que a rede está aprendendo além desse ponto é essencialmente o ruído contido nos dados de treinamento, o que leva ao overfitting. Diante disso, o treinamento deve ser interrompido quando a curva de validação atinge seu valor mínimo.\n\n\n\n\n\n\nFigura 11: Curvas de erro de estimação e validação. O treinamento deve parar na época correspondente ao mínimo da curva de erro de validação. Fonte: Figura adaptada de (Haykin 2009).\n\n\n\nA validação cruzada descrita anteriormente é conhecida como holdout method. Existem outras variantes da validação cruzada na literatura. Uma das mais utilizadas é a conhecida como multifold cross-validation, que é particularmente útil quando os exemplos de treinamento são escassos (Leite 2020). Nesse método, o conjunto de treinamento disponível de \\(N_t\\) exemplos é dividido em \\(K\\) subconjuntos com \\(K&gt;1\\), sendo \\(N_t\\) divisível por \\(K\\). O modelo é treinado com todos os subconjuntos exceto um e o erro de validação é medido testando o modelo no subconjunto que é deixado de fora. Este procedimento é repetido \\(K\\) vezes, cada vez usando um subconjunto diferente para validação, conforme ilustrado na Figura 12 para \\(K=5\\). O desempenho do modelo é avaliado pela média do erro quadrado de validação em todas as tentativas do experimento. A desvantagem dessa variante é o custo computacional envolvido, uma vez que o modelo tem que ser treinado \\(K\\) vezes, sendo \\(1&lt;K\\leq N_t\\).\n\n\n\n\n\n\nFigura 12: Multifold cross-validation: para cada treinamento, o subconjunto de dados destacado em azul é usado para validar o modelo treinado com os dados destacados em magenta. [Fonte]\n\n\n\nA validação cruzada é útil não só para evitar overfitting, mas também para validar a arquitetura da rede. Dessa forma, uma vez definido o número de camadas de uma rede MLP, por exemplo, o modelo de treinamento e validação da Figura 12 pode ser utilizado para verificar se o número de camadas é adequado com base no erro de validação. Se o erro de validação cai nos \\(K\\) treinamentos, então o número de camadas parece ser adequado. Isso também pode ser utilizado para comparar redes MLP com diferentes arquiteturas para ajudar na escolha da arquitetura mais adequada."
  },
  {
    "objectID": "t_hiperparametros.html#normalização",
    "href": "t_hiperparametros.html#normalização",
    "title": "Evitando mínimos locais e overfitting",
    "section": "",
    "text": "Suponha que se deseja prever se um paciente tem ou não uma determina doença com base em determinadas medidas diagnósticas, incluídas no conjunto de dados. Considere que um dos dados é o ácido úrico, cujos valores de referência estão no intervalo \\([3,4,\\; 7,0]~\\rm{mg/dL}\\) e um outro dado é o número de plaquetas, cujos valores de referência estão no intervalo \\([151.000,\\; 304.000]/{\\rm mm}^3\\). Esses diferentes intervalos tornam o treinamento mais desafiador. Por exemplo, considere um regressor com uma única camada e dois pesos, em que os dois dados de entrada possuem faixas de valores muito diferentes. Alterações no valor de um dos pesos produzem mudanças muito maiores na saída e na função de erro do que mudanças semelhantes no outro peso. Nessa situação, pode ser benéfico mudar a escala das variáveis de entrada para que fiquem em intervalos semelhantes. Vamos ver três técnicas a seguir.\n\n\nNessa técnica, os valores dos dados de entrada são normalizados para que fiquem em um intervalo padrão, geralmente \\([0,\\; 1]\\) ou \\([-1, 1]\\). Como antes, considere que o conjunto de dados tenha \\(N\\) elementos, organizados como uma sequência de \\(M\\) valores de \\(x\\), ou seja, \\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1}), (x_{12}, x_{22}, \\cdots, x_{M2}),\\cdots, (x_{1N}, x_{2N}, \\cdots, x_{MN})\\}.\n\\] Para normalização no intervalo \\([0,\\; 1]\\), basta fazer \\[\n\\widetilde{x}_{kn}=\\frac{x_{kn}-x_{k,\\text{min}}}{x_{k,\\text{max}}-x_{k,\\text{min}}},\n\\] \\(k=1,2, \\cdots, M\\), \\(n=1,2,\\cdots, N\\), \\(x_{k,\\text{min}}=\\min\\{x_{kn}\\}_{n=1}^N\\) e \\(x_{k,\\text{max}}=\\max\\{x_{kn}\\}_{n=1}^N\\). Considerando a normalização no intervalo \\([-1,\\; 1]\\), basta fazer \\[\n\\widetilde{\\widetilde{x}}_{kn}=2\\widetilde{x}_{kn}-1.\n\\] Essa normalização é uma boa opção quando os dados seguem uma distribuição aproximadamente uniforme em todo o intervalo.\nDependendo da distribuição dos dados, pode ser útil considerar um dimensionamento logarítmico, em que \\(\\widetilde{x}_{kn}=\\ln(x_{kn})\\). Esse tipo de normalização é utilizado quando a distribuição dos recursos é muito assimétrica em pelo menos um dos lados da cauda. Por exemplo, quando os dados apresentam uma distribuição exponencial.\n\n\n\nA fim de reescalar os dados de entrada para pertencerem a intervalos semelhantes é comum normalizá-los para que tenham média zero e variância unitária. Para isso, calcula-se primeiramente a média e variância dos dados, ou seja, \\[\n\\mu_k=\\frac{1}{N}\\sum_{n=1}^{N}x_{kn}\n\\] e \\[\n\\sigma_k^2=\\frac{1}{N}\\sum_{n=1}^{N}(x_{kn}-\\mu_k)^2.\n\\] Em seguida, considera-se \\[\n\\widetilde{x}_{kn}=\\frac{x_{kn}-\\mu_k}{\\sigma_k}.\n\\]\nNa (fighistnorm1?) são mostrados dois histogramas: o histograma da esquerda representa a distribuição dos valores de plaquetas de um conjunto de pacientes e à direita o histograma dos valores dessa característica normalizados.\n\n\n\nHistograma de dados de plaquetas não normalizados (à direita) e normalizados com média zero e variância unitária (à esquerda).\n\n\nEssa normalização é uma boa opção quando os dados seguem uma distribuição normal ou próxima de normal. No entanto, é comum também considerá-la em casos de distribuição não normal.\n\n\n\nO corte é uma técnica utilizada para minimizar a influência de outliers extremos. Diferente do truncamento, que simplesmente ignora valores de outliers, o corte substitui os valores que aparecem fora do intervalo de maior ocorrência pelo valor máximo desse intervalo. Considere por exemplo o histograma mostrado na (fighistnorm2?), à esquerda. É possível observar que o intervalo de maior ocorrência dos valores dessa característica é \\([150, 450]\\). No entanto, há valores acima de 450, que podem ser considerados *outliers. A normalização por corte substitui todos os valores acima de 450 por 450 e por isso aparece um pico nesse valor, como mostrado no histograma na (fighistnorm2?), à direita. O corte impede que o modelo utilize dados sem importância.\n\n\n\nHistograma de dados não normalizados (à direita) e normalizados pelo corte.\n\n\n\n\n\nAlém de normalizar os dados de entrada, é importante também normalizar as entradas das funções de ativação ou as saídas das camadas ocultas de uma rede neural. Se houver grande variação na faixa de valores das variáveis de uma determinada camada oculta, então normalizar esses valores para que tenham média zero e variância unitária deve tornar o problema de aprendizado mais fácil para a próxima camada. No entanto, diferentemente da normalização dos valores de entrada, que pode ser feita uma única vez antes do início do treinamento, a normalização dos valores das variáveis das camadas ocultas precisa ser repetida durante o treinamento, toda vez que os valores dos pesos forem atualizados. Isso é chamado de normalização em lote batch normalization, proposta por Ioffe e Szegedy em 2015. Uma motivação adicional para a batch normalization surge dos fenômenos de desvanecimento ou explosão dos gradientes, que costumam ocorrer no treinamento de redes neurais profundas.\nPara entender como a batch normalization é definida, considere uma camada oculta \\(j\\) de uma rede MLP com múltiplas camadas. A saída do neurônio \\(k\\) dessa camada é dada por \\(y_k^{(j)}=\\varphi_j(v_k^{(j)})\\). Portanto, temos a opção de normalizar os valores de \\(v_k^{(j)}\\) ou os valores de saída \\(y_k^{(j)}\\), \\(k=1, 2, \\cdots, N_j\\) da camada \\(j\\). Na prática, qualquer uma das abordagens pode ser utilizada, e aqui ilustramos o procedimento normalizando os valores de \\(v_k^{(j)}\\).\nComo os valores dos pesos são atualizados após cada mini-batch, aplica-se a normalização a cada mini-batch. Especificamente, para um mini-batch de tamanho \\(N_b\\), definem-se \\[\\mu_k^{(j)}=\\frac{1}{N_b}\\sum_{n=1}^{N_b}v_{k,n}^{(j)}\\] e \\[\n{\\sigma_k^{(j)}}^2=\\frac{1}{N_b}\\sum_{n=1}^{N_b}\\left(v_{k,n}^{(j)}-\\mu_k^{(j)}\\right)^2.\n\\] Em seguida, considera-se \\[\n\\widetilde{v}_{k,n}^{(j)}=\\frac{v_{k,n}^{(j)}-\\mu_k^{(j)}}{\\sqrt{{\\sigma_k^{(j)}}^2+\\delta}},\n\\] em que as somas levam em conta variáveis de um dado mini-batch já que \\(n=1,2,\\cdots, N_b\\) e \\(\\delta\\) é uma constante pequena utilizada para evitar problemas numéricos quando \\({\\sigma_k^{(j)}}^2\\) for pequeno.\nAo normalizar os valores de \\(v_k^{(j)}\\) em uma determinada camada da rede, reduzimos o número de graus de liberdade nos parâmetros dessa camada e, consequentemente, diminuímos sua capacidade de representação. Podemos compensar isso reescalando os valores de \\(\\widetilde{v}_{k,n}^{(j)}\\) do mini-batch para que tenham média \\(\\beta_k^{(j)}\\) e desvio padrão \\(\\gamma_k^{(j)}\\), usando \\[\n\\overset{\\approx}{v}_{k,n}^{(j)}=\\gamma_k^{(j)}\\widetilde{v}_{k,n}^{(j)}+\\beta_k^{(j)},\n\\] em que onde \\(\\beta_k^{(j)}\\) e \\(\\gamma_k^{(j)}\\) são parâmetros adaptativos aprendidos com o método do gradiente juntamente com os pesos e vieses da rede. Esses parâmetros ajustáveis representam uma diferença fundamental em relação à normalização dos dados de entrada.\nPode parecer que essa transformação simplesmente desfaz o efeito da normalização, já que a média e a variância agora podem novamente se adaptar a valores arbitrários. No entanto, a diferença crucial está na forma como os parâmetros evoluem durante o treinamento. Na rede original, a média e a variância em um mini-batch são determinadas por uma função complexa de todos os pesos e vieses da camada, enquanto na transformação com os parâmetros \\(\\beta_k^{(j)}\\) e \\(\\gamma_k^{(j)}\\), elas são determinadas diretamente por esses parâmetros independentes, que acabam sendo mais fáceis de aprender durante a descida do gradiente.\nA batch normalization pode ser vista como uma camada adicional na rede neural, de modo que cada camada oculta padrão pode ser seguida por uma camada de normalização em mini-batch. Uma vez que a rede foi treinada e queremos fazer previsões sobre novos dados, não temos mais disponíveis os mini-batches de treinamento e, portanto, não podemos determinar a média e a variância a partir de apenas um exemplo de dado. Para resolver isso, poderíamos, em princípio, calcular \\(\\mu_k^{(j)}\\) e \\({\\sigma_k^{(j)}}^2\\) para cada camada em todo o conjunto de treinamento após realizarmos a atualização final dos pesos e vieses. No entanto, isso exigiria processar todo o conjunto de dados apenas para avaliar essas quantidades e, portanto, costuma ser muito custoso. Em vez disso, calculam-se médias móveis de \\(\\mu_k^{(j)}\\) e \\({\\sigma_k^{(j)}}^2\\) ao longo do treinamento.\nEmbora a batch normalization seja muito eficaz na prática, há incerteza sobre por que ela funciona tão bem. Essa normalização foi originalmente motivada pela observação de que atualizações nos pesos das camadas anteriores da rede alteram a distribuição dos valores vistos pelas camadas posteriores, um fenômeno chamado de deslocamento interno de covariáveis (internal covariate shift). No entanto, estudos posteriores (Santurkar et al., 2018) sugerem que o deslocamento de covariáveis não é um fator significativo e que a melhoria no treinamento resulta, na verdade, de uma maior suavidade na função de erro.\n\n[(bihop2024?)](Google Developers 2023)"
  },
  {
    "objectID": "t_hiperparametros.html#footnotes",
    "href": "t_hiperparametros.html#footnotes",
    "title": "Evitando mínimos locais e overfitting",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nAlguns autores utilizem o termo “sigmoidal” para uma classe de funções em que a logística e a tangente hiperbólica são exemplos. Neste texto, vamos utilizar o termo “sigmoidal” apenas para a função logística.↩︎\nO termo estocástico é utilizado aqui para se referir ao modo de treinamento em que cada exemplo de treinamento individual é utilizado para fazer uma iteração de adaptação dos coeficientes.↩︎"
  },
  {
    "objectID": "ap_python_topicos/python_04.html",
    "href": "ap_python_topicos/python_04.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n0:40 Entrando no Google Colab\n1:40 Usando a integração com o Github\n4:15 Salvando o notebook\n5:30 Vantagens e desvantagens do Google Colab\n9:45 Carregando dados\n11:30 Usando o Google Drive\n14:00 Instalando pacotes pip\n15:22 Instalando pacotes conda\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_04.html#google-colab",
    "href": "ap_python_topicos/python_04.html#google-colab",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n0:40 Entrando no Google Colab\n1:40 Usando a integração com o Github\n4:15 Salvando o notebook\n5:30 Vantagens e desvantagens do Google Colab\n9:45 Carregando dados\n11:30 Usando o Google Drive\n14:00 Instalando pacotes pip\n15:22 Instalando pacotes conda\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_03.html",
    "href": "ap_python_topicos/python_03.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n1:18 Exemplo de arquivo yml para descrever ambientes conda\n3:14 Criando um ambiente usando uma descrição yml\n6:55 Usando canais para instalação de pacotes conda\n10:45 Especificando pacotes pip nos arquivos yml para descrever ambientes conda\n14:46 Especificando qual ambiente conda deve ser usado no Jupyter\n18:00 Incluindo um item na lista de kernels do Jupyter\n27:00 Removendo um item da lista de kernels do Jupyter\n29:55 Usando o Jupyter para executar código demorado\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_03.html#mais-detalhes-sobre-o-jupyter-e-o-conda",
    "href": "ap_python_topicos/python_03.html#mais-detalhes-sobre-o-jupyter-e-o-conda",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução\n1:18 Exemplo de arquivo yml para descrever ambientes conda\n3:14 Criando um ambiente usando uma descrição yml\n6:55 Usando canais para instalação de pacotes conda\n10:45 Especificando pacotes pip nos arquivos yml para descrever ambientes conda\n14:46 Especificando qual ambiente conda deve ser usado no Jupyter\n18:00 Incluindo um item na lista de kernels do Jupyter\n27:00 Removendo um item da lista de kernels do Jupyter\n29:55 Usando o Jupyter para executar código demorado\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_02.html",
    "href": "ap_python_topicos/python_02.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução e execução de células no JupyterLab\n4:34 Usando células markdown\n9:58 Adicionando e removendo células do notebook\n12:55 Inserindo conteúdo LaTeX\n16:06 Obtendo ajuda\n19:40 Comentando blocos de código e exibindo numeração das linhas\n22:44 Depurando código\n32:04 Executando comandos do sistema operacional\n34:30 Formatando código com o Black\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "ap_python_topicos/python_02.html#uso-básico-do-jupyterlab",
    "href": "ap_python_topicos/python_02.html#uso-básico-do-jupyterlab",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "0:00 Introdução e execução de células no JupyterLab\n4:34 Usando células markdown\n9:58 Adicionando e removendo células do notebook\n12:55 Inserindo conteúdo LaTeX\n16:06 Obtendo ajuda\n19:40 Comentando blocos de código e exibindo numeração das linhas\n22:44 Depurando código\n32:04 Executando comandos do sistema operacional\n34:30 Formatando código com o Black\n\nJupyter Notebook com referência aos comandos utilizados\nÍndice de vídeos"
  },
  {
    "objectID": "t_desbalanceado.html",
    "href": "t_desbalanceado.html",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "Em muitos problemas, o conjunto de dados é desbalanceado. Por exemplo, quando se deseja detectar transações fraudulentas, a maioria das transações estará na classe “não fraude” e uma minoria na classe “fraude”. Outro exemplo são os conjuntos de dados de rotatividade de clientes, onde a maioria dos clientes permanece com o serviço e uma minoria cancela sua assinatura. Isso também acontece no problema de classificação de arritmias cardíacas utilizando sinais de eletrocardiograma (ECG). O banco de dados de ECG do MIT-BIH (Massachusetts Institute of Technology – Boston’s Beth Israel Hospital Arrhythmia Database), que é o mais utilizado, contém mais de 80000 batimentos normais (N) e apenas 772 batimentos de arritmia com fusão ventricular com normal (F). Ao treinar uma rede neural com essas duas classes apenas, a acurácia pode ser muito elevada: próxima 100% neste caso. Isso não significa que o classificador é bom. Significa apenas que quase 100% dos dados pertencem uma das classes (N, no caso). O classificador de arritmias decide inteligentemente que, para alcançar uma acurácia elevada, a melhor coisa a fazer é sempre prever a classe N. O que fazer quando o conjunto de dados é desbalanceado? A seguir vamos explorar possíveis soluções.\n\n\nA acurácia não é uma métrica adequada quando se trabalha com conjuntos de dados desbalanceados. As seguintes medidas de desempenho podem fornecer mais informações sobre a precisão do modelo do que a acurácia:\n\nMatriz de confusão : uma divisão das predições em uma tabela mostrando as predições corretas (a diagonal) e os tipos de predições incorretas (quais classes tiveram o maior número de predições incorretas);\nPrecisão: uma medida da exatidão do classificador;\nSensibilidade: uma medida da completude do classificador;\n\\(F_1\\)-score: uma média ponderada de precisão e sensibilidade; e\nCurva ROC: assim como a precisão e a sensibilidade, a acurácia é dividida entre sensibilidade e especificidade e os modelos podem ser escolhidos com base nos limites de equilíbrio desses valores.\n\nA escolha da métrica não soluciona o problema dos dados desbalanceados, mas ajuda a detectá-lo.\n\n\n\nO conjunto de dados pode ser alterado com o objetivo de balanceá-lo. Essa alteração é chamada de amostragem do conjunto de dados e há dois métodos principais:\n\nSobreamostragem que consiste na adição de cópias de exemplos da classe sub-representada; e\nSubamostragem que consiste na exclusão de exemplos da classe sobre-representada.\n\nGeralmente, essas soluções são fáceis de implementar e, por isso, são um excelente ponto de partida. A subamostragem deve ser usada apenas quando o conjunto de dados for muito grande (mais de centenas de milhares de exemplos) e, por isso, é menos utilizada na prática. Em contrapartida, a sobreamostragem pode ser usada quando não há muitos dados no conjunto (menos de dezenas de milhares de exemplos). Não há necessidade de balancear com exatidão as classes (por exemplo, na proporção 1:1 na classificação binária). Outras proporções podem ser mais interessantes. Vamos explorar a sobreamostragem com mais detalhes a seguir.\n\n\n\nUma das formas de realizar a sobreamostragem é gerar amostras sintéticas por meio da amostragem aleatória dos exemplos da classe minoritária. Para isso, pode-se adicionar cópias ruidosas desses exemplos ao conjunto de dados. Neste contexto, o SMOTE (Synthetic Minority Over-sampling Technique) é a técnica mais utilizada. Essa técnica gera amostras sintéticas da classe minoritária, selecionando dois ou mais exemplos semelhantes (usando uma medida de distância) e perturba um exemplo por vez, considerando um ruído dentro da diferença para os exemplos vizinhos. O SMOTE foi proposto no artigo (Chawla et al. 2002), disponível em https://arxiv.org/abs/1106.1813.\nExistem várias implementações do algoritmo SMOTE. No Python, por exemplo, o módulo UnbalancedDataset fornece várias implementações do SMOTE, bem como várias outras técnicas de sobreamostragem.\n\n\n\nUma forma de levar em conta o desbalanceamento das classes é considerar pesos na função custo. No caso multiclasse, considera-se a função custo de entropia cruzada categórica ponderada (weighted categorical cross entropy) definida para \\(N_L\\) classes como \\[\nJ_{\\rm ECCP}=-\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}p_{\\ell}d_{\\ell}(n)\\ln\\left(y_{\\ell}^{(L)}(n)\\right),\n\\]\nem que \\[\np_{\\ell}=\\frac{1}{C_{\\ell}}\n\\]\né o peso da \\(\\ell\\)-ésima classe, definido como o inverso de \\(C_{\\ell}\\) (quantidade de dados da Classe \\(\\ell\\)). Dessa forma, quanto maior o valor de \\(C_{\\ell}\\), menos importância à Classe \\(\\ell\\) é dada na função custo. Cabe observar que a utilização dos pesos na função custo deve ser implementada em conjunto com outras técnicas como o SMOTE, por exemplo.\n\n\n\nHá dois livros interessantes que abordam técnicas para lidar com dados desbalanceados:\n\n(Fernández et al. 2018)\n(Ma e He 2013)"
  },
  {
    "objectID": "t_desbalanceado.html#métrica-de-desempenho",
    "href": "t_desbalanceado.html#métrica-de-desempenho",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "A acurácia não é uma métrica adequada quando se trabalha com conjuntos de dados desbalanceados. As seguintes medidas de desempenho podem fornecer mais informações sobre a precisão do modelo do que a acurácia:\n\nMatriz de confusão : uma divisão das predições em uma tabela mostrando as predições corretas (a diagonal) e os tipos de predições incorretas (quais classes tiveram o maior número de predições incorretas);\nPrecisão: uma medida da exatidão do classificador;\nSensibilidade: uma medida da completude do classificador;\n\\(F_1\\)-score: uma média ponderada de precisão e sensibilidade; e\nCurva ROC: assim como a precisão e a sensibilidade, a acurácia é dividida entre sensibilidade e especificidade e os modelos podem ser escolhidos com base nos limites de equilíbrio desses valores.\n\nA escolha da métrica não soluciona o problema dos dados desbalanceados, mas ajuda a detectá-lo."
  },
  {
    "objectID": "t_desbalanceado.html#amostragem",
    "href": "t_desbalanceado.html#amostragem",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "O conjunto de dados pode ser alterado com o objetivo de balanceá-lo. Essa alteração é chamada de amostragem do conjunto de dados e há dois métodos principais:\n\nSobreamostragem que consiste na adição de cópias de exemplos da classe sub-representada; e\nSubamostragem que consiste na exclusão de exemplos da classe sobre-representada.\n\nGeralmente, essas soluções são fáceis de implementar e, por isso, são um excelente ponto de partida. A subamostragem deve ser usada apenas quando o conjunto de dados for muito grande (mais de centenas de milhares de exemplos) e, por isso, é menos utilizada na prática. Em contrapartida, a sobreamostragem pode ser usada quando não há muitos dados no conjunto (menos de dezenas de milhares de exemplos). Não há necessidade de balancear com exatidão as classes (por exemplo, na proporção 1:1 na classificação binária). Outras proporções podem ser mais interessantes. Vamos explorar a sobreamostragem com mais detalhes a seguir."
  },
  {
    "objectID": "t_desbalanceado.html#sobreamostragem",
    "href": "t_desbalanceado.html#sobreamostragem",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "Uma das formas de realizar a sobreamostragem é gerar amostras sintéticas por meio da amostragem aleatória dos exemplos da classe minoritária. Para isso, pode-se adicionar cópias ruidosas desses exemplos ao conjunto de dados. Neste contexto, o SMOTE (Synthetic Minority Over-sampling Technique) é a técnica mais utilizada. Essa técnica gera amostras sintéticas da classe minoritária, selecionando dois ou mais exemplos semelhantes (usando uma medida de distância) e perturba um exemplo por vez, considerando um ruído dentro da diferença para os exemplos vizinhos. O SMOTE foi proposto no artigo (Chawla et al. 2002), disponível em https://arxiv.org/abs/1106.1813.\nExistem várias implementações do algoritmo SMOTE. No Python, por exemplo, o módulo UnbalancedDataset fornece várias implementações do SMOTE, bem como várias outras técnicas de sobreamostragem."
  },
  {
    "objectID": "t_desbalanceado.html#inclusão-de-pesos-na-função-custo",
    "href": "t_desbalanceado.html#inclusão-de-pesos-na-função-custo",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "Uma forma de levar em conta o desbalanceamento das classes é considerar pesos na função custo. No caso multiclasse, considera-se a função custo de entropia cruzada categórica ponderada (weighted categorical cross entropy) definida para \\(N_L\\) classes como \\[\nJ_{\\rm ECCP}=-\\frac{1}{N_L}\\sum_{\\ell=1}^{N_L}p_{\\ell}d_{\\ell}(n)\\ln\\left(y_{\\ell}^{(L)}(n)\\right),\n\\]\nem que \\[\np_{\\ell}=\\frac{1}{C_{\\ell}}\n\\]\né o peso da \\(\\ell\\)-ésima classe, definido como o inverso de \\(C_{\\ell}\\) (quantidade de dados da Classe \\(\\ell\\)). Dessa forma, quanto maior o valor de \\(C_{\\ell}\\), menos importância à Classe \\(\\ell\\) é dada na função custo. Cabe observar que a utilização dos pesos na função custo deve ser implementada em conjunto com outras técnicas como o SMOTE, por exemplo."
  },
  {
    "objectID": "t_desbalanceado.html#leitura-adicional",
    "href": "t_desbalanceado.html#leitura-adicional",
    "title": "Como lidar com conjuntos de dados desbalanceados",
    "section": "",
    "text": "Há dois livros interessantes que abordam técnicas para lidar com dados desbalanceados:\n\n(Fernández et al. 2018)\n(Ma e He 2013)"
  },
  {
    "objectID": "ex_aula_mlp_4.html",
    "href": "ex_aula_mlp_4.html",
    "title": "Exercício - MLP 4",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ap_python_topicos.html",
    "href": "ap_python_topicos.html",
    "title": "Tópicos de programação com Python",
    "section": "",
    "text": "01 - Configurando o ambiente\n02 - Uso básico do JupyterLab\n03 - Mais detalhes sobre o Jupyter e o conda\n04 - Google Colab\n05 - Alguns detalhes do Python\n06 - NumPy"
  },
  {
    "objectID": "ap_exemplo_pytorch_cnn.html",
    "href": "ap_exemplo_pytorch_cnn.html",
    "title": "Exemplo CNN com PyTorch",
    "section": "",
    "text": "Segue a implementação de uma rede CNN para resolver o problema de classificação de imagens de dígitos numéricos manuscritos do banco de dados MNIST.\nIniciando com a importação da bibliotecas:\n\nimport numpy\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom torchvision import datasets, transforms\n\nAjustando o valor dos hiperparâmetros:\n\n# Ajuste de hiperparâmetros\n\n# passo de adaptação\neta = 0.01\n\n# Tamanho do mini-batch\nNb = 64\n\n# Tamanho do mini-batch usado no teste\nNb_test = 1000\n\n# Número de épocas\nNe = 1\n\nO PyTorch tem algumas rotinas para carregar bancos de dados clássicos. Vamos usar essas rotinas para carregar o MNIST. Com o código abaixo, os dados vão ser obtidos da internet e gravados no local indicado por dir_data, em uma pasta chamada data.\nAlém disso, vamos criar dois objetos DataLoader, para treinamento e teste, que vão se encarregar de ler os dados em partes e misturá-los:\n\ndir_data = \"~/temp\"\n\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        dir_data,\n        train=True,\n        download=True,\n        transform=transforms.Compose(            \n            [transforms.ToTensor()]\n        ),\n    ),\n    batch_size=Nb,\n    shuffle=True,\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        dir_data,\n        train=False,\n        transform=transforms.Compose(            \n            [transforms.ToTensor()]\n        ),\n    ),\n    batch_size=Nb_test,\n    shuffle=True,\n)\n\nVale notar alguns detalhes sobre o código anterior:\n\no DataLoader de treinamento é criado com train=True e o de teste, com train=False, o que garante que não haja dados em comum entre os dois conjuntos;\nÉ feita a configuração de uma transformação de dados ao carregá-los. Para isso, é criado um objeto do tipo transforms.Compose, que permite encadear uma série de transformações a serem aplicadas às imagens, durante o carregamento. Nesse caso, a transformação tem uma única etapa que consistem em converter os valores obtidos para um tensor do PyTorch.\n\nPodemos mostrar algumas imagens do dataset usando o DataLoader que criamos:\n\nplt.figure(figsize=(16, 6))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    image, _ = train_loader.dataset.__getitem__(i)\n    plt.imshow(image.squeeze().numpy())\n    plt.axis('off');\n\n\n\n\n\n\n\n\nO modelo é definido por meio de uma classe que herda de nn.Module:\n\nclass Model(nn.Module):\n    def __init__(self):\n        # Necessário chamar __init__() da classe mãe\n        super().__init__()\n        \n        # Camada convolucional, seguida de ReLU e Pooling\n        # Entra uma imagem 28x28. Com filtro 5x5, padding de 2\n        # e stride 1, a saída também tem 28x28. Após o pooling,\n        # a saída fica com 14x14. A entrada tem 1 canal e a\n        # saída tem 16.\n        self.conv1 = nn.Sequential(         \n            nn.Conv2d(\n                in_channels=1,              \n                out_channels=16,            \n                kernel_size=5,              \n                stride=1,                   \n                padding=2,                  \n            ),                              \n            nn.ReLU(),                      \n            nn.MaxPool2d(kernel_size=2),    \n        )\n        \n        # Camada convolucional\n        # Entrada 14x14, saída 7x7 após o pooling.\n        # 16 canais de entrada e 32 de saída.\n        self.conv2 = nn.Sequential(         \n            nn.Conv2d(16, 32, 5, 1, 2),     \n            nn.ReLU(),                      \n            nn.MaxPool2d(2),                \n        )\n        \n        # Camada totalmente conectada\n        # Na entrada, há 32 canais de 7x7 elementos\n        # e a saída tem 10 neurônios.\n        self.out = nn.Linear(32 * 7 * 7, 10)\n        \n    def forward(self, x):\n        # Aplica primeira camada convolucional\n        x = self.conv1(x)\n        \n        # Aplica segunda camada convolucional\n        x = self.conv2(x)        \n        \n        # Transforma os tensores 32x7x7 em\n        # vetores para serem usados na entrada da\n        # camada totalmente conectada. Vale notar\n        # que a primeira dimensão dos tensores de\n        # dados é usada para representar os diversos\n        # elementos de um batch, por isso permanece\n        # inalterada.\n        x = x.view(x.size(0), -1)       \n        \n        # Calcula a saída e retorna\n        output = self.out(x)\n        return output\n\nInstanciando o modelo e definindo a função custo e o otimizador:\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Instanciando modelo\nmodel = Model().to(device)\n\n# Função custo para treinamento\nloss_function = nn.CrossEntropyLoss()\n\n# Otimizador\noptimizer = torch.optim.Adam(model.parameters(), lr = eta)   \n\nVale notar que a função custo CrossEntropyLoss espera comparar um vetor de \\(C\\) posições com um número de \\(0\\) a \\(C-1\\), conforme descrito na documentação. Além disso, é esperado que os elementos do vetor representem a evidência, ou seja os valores chamados de logits, que não são normalizados e podem valer de \\(-\\infty\\) a \\(\\infty\\). Por isso, na saída da rede, não é usada a função Softmax.\nDefinindo o loop de treinamento:\n\n# Lista usada para guardar o valor da função custo ao longo das iterações\nlosses = []\n\n# Loop das épocas\nfor epoch in range(Ne):\n    # Loop dos mini batches\n    for n, (X, d) in enumerate(train_loader):\n        # Envia os dados para a GPU, caso ela exista\n        X = X.to(device=device)\n        d = d.to(device=device)\n\n        # Ajuste de dimensões\n        # (elementos do mini batch x 1 canal x 28 x 28)\n        X = X.view(-1, 1, 28, 28)\n\n        # Coloca o modelo em modo treinamento\n        model.train()\n\n        # Zera informações de gradientes\n        model.zero_grad()\n\n        # Calcula a saída\n        y = model(X)\n\n        # Calcula o valor da função custo\n        loss = loss_function(y, d)\n\n        # Calcula os gradientes\n        loss.backward()\n\n        # Atualiza os pesos do modelo\n        optimizer.step()\n\n        # Armazena o valor da função custo\n        losses.append(loss.item())\n\n        # Mostra o valor da função custo a cada 100 iterações\n        if n % 100 == 0:\n            N_all = len(train_loader.dataset)\n            n_ex = n * len(X)\n            p = 100. * n / len(train_loader)\n            print(\n                f\"Época: {epoch} [{n_ex}/{N_all} ({p:.0f}%)]\\tLoss: {loss:.6f}\"\n            )            \n\nplt.figure()\nplt.plot(losses)\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\n\nÉpoca: 0 [0/60000 (0%)] Loss: 2.305868\nÉpoca: 0 [6400/60000 (11%)] Loss: 2.312832\nÉpoca: 0 [12800/60000 (21%)]    Loss: 0.152859\nÉpoca: 0 [19200/60000 (32%)]    Loss: 0.138126\nÉpoca: 0 [25600/60000 (43%)]    Loss: 0.188300\nÉpoca: 0 [32000/60000 (53%)]    Loss: 0.123948\nÉpoca: 0 [38400/60000 (64%)]    Loss: 0.048474\nÉpoca: 0 [44800/60000 (75%)]    Loss: 0.195575\nÉpoca: 0 [51200/60000 (85%)]    Loss: 0.096904\nÉpoca: 0 [57600/60000 (96%)]    Loss: 0.043440\n\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\n\n\n\nAvaliando modelo com os dados de teste:\n\n# Variável usada para contabilizar o número de acertos\ncorrect = 0\n\n# Loop dos mini batches\nfor n, (X, d) in enumerate(test_loader):\n    # Envia os dados para a GPU, caso ela exista\n    X = X.to(device=device)\n    d = d.to(device=device)\n\n    # Ajuste de dimensões\n    X = X.view(-1, 1, 28, 28)\n\n    # Coloca o modelo em modo de inferência\n    model.eval()\n\n    # Calcula a saída\n    y = model(X)\n\n    # Cálculo do número de acertos:\n    # 1) Obtém o índice do elemento máximo para cada exemplo do minibatch\n    pred = torch.max(y, 1, keepdim=True)[1]     \n    # 2) Conta o número de acertos e acumula na variável correct\n    # pred.eq(d.view_as(pred)) é um tensor booleano. Dessa forma, o número de\n    # acertos é obtido somando seus elementos. Valores True são tratados como 1. \n    correct += pred.eq(d.view_as(pred)).cpu().sum().item()\n\n# Mostra o desempenho obtido no teste    \naccuracy = 100. * correct / len(test_loader.dataset)\nprint(f\"Acurácia: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\")\n\nAcurácia: 9690/10000 (97%)"
  },
  {
    "objectID": "t_pca.html",
    "href": "t_pca.html",
    "title": "Análise de Componentes Principais",
    "section": "",
    "text": "\\[\n\\newcommand{\\AM}{{\\mathbf A}}\n\\newcommand{\\IM}{{\\mathbf I}}\n\\newcommand{\\vM}{{\\mathbf v}}\n\\newcommand{\\xM}{{\\mathbf x}}\n\\newcommand{\\yM}{{\\mathbf y}}\n\\newcommand{\\E}{{\\rm E}}\n\\]"
  },
  {
    "objectID": "t_pca.html#maximizando-a-variância",
    "href": "t_pca.html#maximizando-a-variância",
    "title": "Análise de Componentes Principais",
    "section": "Maximizando a variância",
    "text": "Maximizando a variância\nConsidere um conjunto de dados \\(\\{\\mathbf{x}_n\\}\\), em que \\(n=1, 2, \\ldots, N\\) e \\(\\mathbf{x}_n \\in \\mathbb{R}^{D}\\), ou seja, os vetores coluna \\(\\mathbf{x}_n\\) têm dimensão \\(D\\times 1\\) e elementos reais. O objetivo do PCA é projetar os dados em um espaço de dimensão \\(M&lt;D\\) e ao mesmo tempo maximizar a variância dos dados projetados. Neste momento, vamos assumir que \\(M\\) é conhecido. Há técnicas para determinar um valor apropriado para \\(M\\), como veremos posteriormente.\nConsidere a projeção em um espaço de uma dimensão \\(M=1\\). Podemos definir a direção deste espaço, usando um vetor coluna de dimensão \\(D\\times 1\\), denotado por \\(\\mathbf{u}_1\\). Por conveniência e sem perda de generalidade, vamos assumir que \\(\\mathbf{u}_1^{\\rm T}\\mathbf{u}_1=\\|\\mathbf{u}_1\\|^2=1\\), já que estamos interessados na direção do vetor \\(\\mathbf{u}_1\\) e não em sua magnitude. Cada vetor \\(\\mathbf{x}_n\\) do conjunto de dados é então projetado no escalar \\(p_{1n}=\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n\\). A média dos dados projetados vale\n\\[\n\\overline{p}_1=\\frac{1}{N}\\sum_{n=1}^{N}p_{1n}=\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n=\\mathbf{u}_1^{\\rm T}\\left[\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n\\right]=\\mathbf{u}_1^{\\rm T}\\overline{\\mathbf{x}}\n\\]\nem que \\(\\overline{\\mathbf{x}}\\) é o valor médio do conjunto de dados. A variância dos dados projetados é dada por\n\\[\n\\sigma_{p_1}^2=\\frac{1}{N}\\sum_{n=1}^{N}(p_{1n}-\\overline{p})^2=\\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n-\\mathbf{u}_1^{\\rm T}\\overline{\\mathbf{x}})^2\n\\] \\[\n=\\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n\\mathbf{x}_n^{\\rm T}\\mathbf{u}_1 + \\mathbf{u}_1^{\\rm T}\\overline{\\mathbf{x}}\\,\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1) - \\frac{2}{N} \\sum_{n=1}^{N}\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1.\n\\]\nNote que o segundo somatório do lado direito da última igualdade dessa expressão vale \\[\n\\frac{2}{N} \\sum_{n=1}^{N}\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1=\n2 \\mathbf{u}_1^{\\rm T}\\left[\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n\\right]\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1= 2 \\mathbf{u}_1^{\\rm T}\\overline{\\mathbf{x}}\\,\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1.\n\\]\nAssim, podemos escrever a variância \\(\\sigma_{p_1}^2\\) como \\[\n\\sigma_{p_1}^2=\\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{u}_1^{\\rm T}\\mathbf{x}_n\\mathbf{x}_n^{\\rm T}\\mathbf{u}_1 - \\mathbf{u}_1^{\\rm T}\\overline{\\mathbf{x}}\\,\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_1)=\n\\mathbf{u}_1^{\\rm T} \\left[\\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{x}_n\\mathbf{x}_n^{\\rm T}-\\overline{\\mathbf{x}}\\,\\overline{\\mathbf{x}}^{\\rm T}) \\right]\\mathbf{u}_1.\n\\]\nDefinindo a matriz de covariância dos dados \\[\n\\mathbf{S}=\\frac{1}{N}\\sum_{n=1}^{N}(\\mathbf{x}_n-\\overline{\\mathbf{x}})(\\mathbf{x}_n-\\overline{\\mathbf{x}})^{\\rm T},\n\\]\ne identificando que ela é termo entre colchetes da expressão anterior, chega-se finalmente a\n\\[\n\\sigma_{p_1}^2=\\mathbf{u}_1^{\\rm T} \\mathbf{S}\\mathbf{u}_1.\n\\]\nVamos agora maximizar a variância \\(\\sigma_{p_1}^2\\) com relação à \\(\\mathbf{u}_1\\). Note que se trata de um critério com restrição, pois \\(\\|\\mathbf{u}_1\\|\\rightarrow \\infty\\) maximiza \\(\\sigma_p^2\\), mas deve ser evitado. Com esse propósito, vamos considerar a seguinte restrição \\(\\mathbf{u}_1^{\\rm T}\\mathbf{u}_1=\\|\\mathbf{u}_1\\|^2=1\\), o que leva ao critério \\[\n\\max_{\\mathbf{u}_1} \\mathbf{u}_1^{\\rm T} \\mathbf{S}\\mathbf{u}_1\\;\\;\\text{sujeito a}\\;\\;\\mathbf{u}_1^{\\rm T}\\mathbf{u}_1=1.\n\\]\nPara maximizar a variância levando em conta a restrição, pode-se considerar um multiplicador de Lagrange, denotado por \\(\\lambda_1\\), o que leva à maximização do seguinte critério sem restrição1\n\\[\nJ(\\mathbf{u}_1)=\\mathbf{u}_1^{\\rm T} \\mathbf{S}\\mathbf{u}_1+\\lambda_1(1-\\mathbf{u}_1^{\\rm T}\\mathbf{u}_1).\n\\]\nCalculando a derivando de \\(J(\\mathbf{u}_1)\\) em relação à \\(\\mathbf{u}_1\\), obtém-se \\[\n\\frac{d J(\\mathbf{u}_1)}{d\\mathbf{u}_1}=\\mathbf{S}\\mathbf{u}_1-\\lambda_1\\mathbf{u_1}.\n\\]\nIgualando essa derivada a zero, chega-se a\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{S}\\mathbf{u}_1=\\lambda_1\\mathbf{u}_1.\n$}\n\\end{equation*}\\]\nEssa igualdade ocorre apenas quando \\(\\mathbf{u}_1\\) for o autovetor de \\(\\mathbf{S}\\) associado ao autovalor \\(\\lambda_1\\). Para recordar isso, no Apêndice~A há uma recordação da teoria de Álgebra Linear relacionada a autovalores e autovetores.\nMultiplicando ambos os lados da relação anterior à esquerda por \\(\\mathbf{u}_1^{\\rm T}\\) e usando o fato de que \\(\\mathbf{u}_1^{\\rm T}\\mathbf{u}_1=1\\), obtemos \\[\n\\sigma_{p_1}^2=\\mathbf{u}_1^{\\rm T}\\mathbf{S}\\mathbf{u}_1=\\lambda_1.\n\\]\nEntão a variância \\(\\sigma_p^2\\) será máxima quando \\(\\mathbf{u}_1\\) for o autovetor de \\(\\mathbf{S}\\) relacionado ao maior autovalor \\(\\lambda_1\\). Esse autovetor é conhecido como o primeiro componente principal.\nPodemos adicionar componentes principais, escolhendo cada nova direção como aquela que maximiza a variância projetada entre todas as direções ortogonais possíveis às já consideradas. Dessa forma, é possível demonstrar por indução que ao considerar um espaço de dimensão \\(M\\), a projeção linear ótima para a qual a variância dos dados projetados é maximizada é definida pelos \\(M\\) autovetores \\(\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_M\\) da matriz de covariância dos dados \\(\\mathbf{S}\\), correspondentes aos seus \\(M\\) maiores autovalores \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_M\\).\nResumindo, a análise de componentes principais envolve o cálculo da média \\(\\overline{\\mathbf{x}}\\) e da matriz de covariância \\(\\mathbf{S}\\) do conjunto de dados. Em seguida, deve-se encontrar os \\(M\\) autovetores de \\(\\mathbf{S}\\) relacionados aos seus \\(M\\) maiores autovalores. Se levarmos em conta todos os \\(D\\) autovetores de \\(\\mathbf{S}\\), não haverá redução de dimensionalidade. Trata-se de uma transformação linear ortogonal que transforma os dados para um novo sistema de coordenadas de modo que a maior variância por qualquer projeção dos dados fica ao longo da primeira coordenada (primeiro componente principal), a segunda maior variância fica ao longo da segunda coordenada (segundo componente principal) e assim por diante. O primeiro componente principal é o mais importante porque explica a maior parcela da variância dos dados, o segundo componente é o segundo mais importante e assim sucessivamente. O objetivo é descrever a maxima variabilidade do conjunto de dados original com um conjunto menor de variáveis."
  },
  {
    "objectID": "t_pca.html#gerando-um-conjunto-de-dados-não-correlacionados",
    "href": "t_pca.html#gerando-um-conjunto-de-dados-não-correlacionados",
    "title": "Análise de Componentes Principais",
    "section": "Gerando um conjunto de dados não correlacionados",
    "text": "Gerando um conjunto de dados não correlacionados\nVimos que os \\(M\\) autovetores \\(\\mathbf{u}_k=[u_{k1}\\;u_{k2}\\;\\cdots\\;u_{kD}]^{\\rm T}\\), associados aos \\(M\\) maiores autovalores \\(\\lambda_k\\), \\(k=1, 2, \\cdots, M\\) da matriz de covariância dos dados \\(\\mathbf{S}\\) são os componentes principais. Os dados projetados em cada um desses componentes são dados por \\[\np_{kn}=\\mathbf{u}_k^{\\rm T}\\mathbf{x}_n=u_{k1}x_{1n}+u_{k2}x_{2n}+\\cdots u_{kD}x_{Dn}\n\\]\npara \\(k=1, 2, \\cdots, M\\) e \\(n=1, 2 \\cdots N\\). Assim, os dados projetados são combinações lineares de todas as variáveis presentes nos vetores do conjunto de dados e os elementos dos autovetores, chamados de loadings na literatura, são os pesos dessas combinações.\nOrganizando o conjunto original de dados na matriz \\[\n\\mathbf{X}=[\\mathbf{x}_1\\;\\mathbf{x}_2\\;\\cdots\\;\\mathbf{x}_N]\n\\]\ne os componentes principais na matriz \\[\n\\mathbf{U}=\\left[\\begin{array}{c}\n                    \\mathbf{u}_1^{\\rm T} \\\\\n                    \\mathbf{u}_2^{\\rm T} \\\\\n                    \\vdots \\\\\n                    \\mathbf{u}_M^{\\rm T}\n                  \\end{array}\n\\right],\n\\]\nobtemos a matriz dos dados transformados\n\\[\n\\mathbf{P}=[\\mathbf{p}_1\\;\\mathbf{p}_2\\;\\cdots\\;\\mathbf{p}_N]\n\\]\npor meio da transformação linear \\[\n\\mathbf{P}=\\mathbf{U}\\mathbf{X},\n\\]\nou seja, \\[\n\\left[\\begin{array}{cccc}\n            p_{11} &p_{12} & \\cdots & p_{1N} \\\\\n            p_{21} &p_{22} & \\cdots & p_{2N} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            p_{M1} &p_{M2} & \\cdots & p_{MN}\n          \\end{array}\\right]_{M\\times N}=\\left[\\begin{array}{cccc}\n            u_{11} &u_{12} & \\cdots & u_{1D} \\\\\n            u_{21} &u_{22} & \\cdots & u_{2D} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            u_{M1} &u_{M2} & \\cdots & u_{MD}\n          \\end{array}\n        \\right]_{M\\times D}\\left[\\begin{array}{cccc}\n            x_{11} &x_{12} & \\cdots & x_{1N} \\\\\n            x_{21} &x_{22} & \\cdots & x_{2N} \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            x_{D1} &x_{D2} & \\cdots & x_{DN}\n          \\end{array}\n        \\right]_{D\\times N}.\n\\]\nDevido à restrição do critério considerado para maximizar a variância dos dados projetados, os componentes principais, autovetores da matriz de covariância dos dados, possuem norma unitária. Além disso, de Álgebra Linear sabe-se que esses vetores são ortogonais entre si. Da teoria de Probabilidades, sabe-se que duas variáveis aleatórias são não correlacionadas quando sua covariância é nula2.\nConsidere os dados projetados por dois componentes principais distintos, ou seja, \\(\\mathbf{u}_k\\) e \\(\\mathbf{u}_{\\ell}\\) com \\(k\\neq \\ell\\) e \\(\\{k, \\ell\\} \\in \\{1, 2, \\ldots, M\\}\\). A covariância entre esses dados é calculada como \\[\n{\\rm cov}(k,\\ell)={\\rm E}\\{(\\mathbf{u}_k^{\\rm T}\\mathbf{x}_n-\\mathbf{u}_k^{\\rm T}\\overline{\\mathbf{x}})(\\mathbf{x}_n^{\\rm T}\\mathbf{u}_\\ell-\\overline{\\mathbf{x}}^{\\rm T}\\mathbf{u}_\\ell)\\}\n\\]\nem que \\(\\E\\{\\cdot\\}\\) representa a esperança matemática. Os vetores \\(\\mathbf{u}_k\\) e \\(\\mathbf{u}_{\\ell}\\) não são variáveis aleatórias e podem sair da esperança. Assim, \\[\n{\\rm cov}(k,\\ell)=\\mathbf{u}_k^{\\rm T}\\,\\E\\{\\mathbf{x}_n\\mathbf{x}_n^{\\rm T}-\\mathbf{x}_n\\overline{\\mathbf{x}}^{\\rm T}-\\overline{\\mathbf{x}}\\mathbf{x}_n^{\\rm T}+\\overline{\\mathbf{x}}\\,\\overline{\\mathbf{x}}^{\\rm T}\\}\\,\\mathbf{u}_\\ell.\n\\]\nA esperança que aparece nessa expressão é a matriz de covariância dos dados, o que leva a \\[\n{\\rm cov}(k,\\ell)=\\mathbf{u}_k^{\\rm T}\\,\\mathbf{S}\\,\\mathbf{u}_\\ell.\n\\]\nLembrando que \\(\\mathbf{S}\\,\\mathbf{u}_\\ell=\\lambda_\\ell\\) e que \\(\\mathbf{u}_k^{\\rm T}\\mathbf{u}_\\ell=0\\) (os autovetores são ortogonais), chega-se\n\\[\n{\\rm cov}(k,\\ell)=\\lambda_\\ell\\mathbf{u}_k^{\\rm T}\\mathbf{u}_\\ell=0,\n\\]\no que mostra que os dados transformados por componentes principais distintos são não correlacionados."
  },
  {
    "objectID": "t_pca.html#quantos-componentes-principais-usar",
    "href": "t_pca.html#quantos-componentes-principais-usar",
    "title": "Análise de Componentes Principais",
    "section": "Quantos componentes principais usar?",
    "text": "Quantos componentes principais usar?\nVimos que os autovalores \\(\\lambda_1, \\lambda_2, \\ldots, \\lambda_D\\) da matriz de covariância dos dados \\(\\mathbf{S}\\) são as variâncias dos dados transformados. Assim, a soma de todos os autovalores é a variância total explicada, ou seja, \\[\n\\sigma^2_{\\text{total}}=\\sum_{k=1}^{D}\\lambda_k.\n\\]\nConsequentemente, a proporção da variância explicada (em %) por cada componente principal \\(\\ell\\), \\(\\ell=1, 2, \\cdots M\\) é dada por \\[\n\\%{\\rm var}_\\ell=100\\displaystyle\\frac{\\lambda_\\ell}{\\displaystyle\\sum_{k=1}^{D}\\lambda_k}=100\\displaystyle\\frac{\\lambda_\\ell}{\\displaystyle\\sigma^2_{\\text{total}}}.\n\\]\nComo o PCA é um método usado para redução de dimensionalidade, deve-se considerar apenas os componentes que explicam a maior parte da variação dos dados. Não existe um ponto de corte absoluto para descartamos os componentes principais menos significativos. Em geral, considera-se o número de componentes principais para que a soma da proporção da variância explicada seja em torno de 80%."
  },
  {
    "objectID": "t_pca.html#normalizando-o-conjunto-de-dados",
    "href": "t_pca.html#normalizando-o-conjunto-de-dados",
    "title": "Análise de Componentes Principais",
    "section": "Normalizando o conjunto de dados",
    "text": "Normalizando o conjunto de dados\nEm geral, os elementos dos vetores de dados \\(\\mathbf{x}_n\\) podem representar variáveis com diferentes ordens de grandeza. Diante disso, é importante normalizar os dados antes de calcular o PCA. Suponha que cada vetor \\(\\mathbf{x}_n\\) do banco de dados seja dado por\n\\[\n\\mathbf{x}_n=[x_{1n}\\;x_{2n}\\;\\cdots\\; x_{Dn}]^{\\rm T},\n\\]\nem que cada variável \\(x_{nk}\\), \\(k=1, 2, \\ldots, D\\) representa uma grandeza. A normalização mais comum dos dados leva em conta a média\n\\[\n\\overline{x}_k=\\frac{1}{N}\\sum_{n=1}^{N}x_{kn},\n\\]\ne o desvio padrão de cada variável \\[\n\\sigma_{x_k}=\\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}(x_{kn}-\\overline{x}_k)^2},\n\\]\npara \\(k=1, 2, \\ldots, D\\). Assim, as variáveis são normalizadas como\n\\[\n\\widetilde{x}_{kn}=\\frac{x_{kn}-\\overline{x}_k}{\\sigma_{x_k}}\n\\]\ne os vetores de dados normalizados sobre os quais o PCA deve ser calculado é dado por\n\\[\n\\widetilde{\\mathbf{x}}_n=[\\widetilde{x}_{1n}\\;\\widetilde{x}_{2n}\\;\\cdots\\; \\widetilde{x}_{Dn}]^{\\rm T}\n\\]\npara \\(n=1, 2, \\ldots, N\\).\nNas seções anteriores, a formulação do PCA foi feita sobre o conjunto de dados não normalizados \\(\\{\\mathbf{x}_n\\}\\). No entanto, a normalização é aconselhável como forma de evitar enviesar a influência de certas variáveis quando as variáveis originais têm dispersões ou escalas significativamente diferentes. Quando as medidas originais já possuem dispersões semelhantes, a padronização tem pouco efeito."
  },
  {
    "objectID": "t_pca.html#exemplos",
    "href": "t_pca.html#exemplos",
    "title": "Análise de Componentes Principais",
    "section": "Exemplos",
    "text": "Exemplos\nConsidere que os pontos azuis indicados na Figura 1 pertencem a um conjunto de dados normalizados \\(\\{\\widetilde{\\mathbf{x}}_n\\}\\) com \\(N=10\\) e \\(D=2\\). Vamos considerar o subespaço gerado pelo primeiro componente principal, diminuindo a dimensão para \\(M=1\\). A matriz de covariância dos dados é dada por \\[\n\\mathbf{S}=\\left[\\begin{array}{cc}\n                     0,900 & 0,725 \\\\\n                     0,725 & 0,900\n                   \\end{array}\n\\right]\n\\]\ncujos autovalores são \\(\\lambda_1=1,625\\) e \\(\\lambda_2=0,1750\\) e os autovetores de norma unitária associados são \\(\\mathbf{u}_1=[1/\\sqrt{2}\\;\\;\\;\\; 1/\\sqrt{2}]^{\\rm T}\\) e \\(\\mathbf{u}_2=[-1/\\sqrt{2}\\;\\;\\;\\; 1/\\sqrt{2}]^{\\rm T}\\), respectivamente. Considerando o componente principal \\(\\mathbf{u}_1\\), autovetor associado ao maior autovalor, os dados projetados são calculados como\n\\[\np_n=\\frac{1}{\\sqrt{2}}\\widetilde{x}_{1n}+\\frac{1}{\\sqrt{2}}\\widetilde{x}_{2n}.\n\\]\nEsse componente explica \\(\\%{\\rm var}_1=100(1,625)/(1,625+0.1750)=90,28\\%\\) da variância total. O PCA busca um espaço de dimensão \\(M=1\\), denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.\n\n\n\n\n\n\nFigura 1: O PCA busca um espaço de dimensão menor conhecido como subespaço principal, denotado pela linha vermelha tal que a projeção ortogonal dos dados originais (pontos azuis) neste subespaço maximiza a variância dos pontos projetados (pontos pretos). Uma formulação alternativa do PCA é baseada na minimização dos erros de projeção, indicados pelas linhas verdes.\n\n\n\nO PCA pode ser usado para compressão de imagens. Para ilustrar isso, na Figura 2(a), consideramos imagem média (\\(\\overline{\\mathbf{x}}\\)) e os quatro primeiros componentes principais (\\(\\mathbf{u}_1,\\cdots,\\mathbf{u}_4\\)) com os seus autovalores correspondentes baseados em imagens do dígito três da base de dados MNIST. Na Figura 2(b), são mostradas a imagem original e as imagens reconstruídas considerando 1, 10, 50 e 250 componentes principais. A medida que se aumenta o valor de \\(M\\), a reconstrução se torna mais precisa e é perfeita para \\(M=D=784\\).\n\n\n\n\n\n\nFigura 2: (a) Imagem média e os quatro primeiros componentes principais baseados em imagens do dígito três da base de dados MNIST; (b) Imagem original e reconstrução da imagem considerando 1, 10, 50 e 250 componentes principais. Fonte: (Bishop 2011)"
  },
  {
    "objectID": "t_pca.html#leitura-adicional",
    "href": "t_pca.html#leitura-adicional",
    "title": "Análise de Componentes Principais",
    "section": "Leitura adicional",
    "text": "Leitura adicional\nO livro (Jolliffe 2002) é uma referência recomendada para quem quiser se aprofundar no assunto."
  },
  {
    "objectID": "t_pca.html#autovalores-e-autovetores",
    "href": "t_pca.html#autovalores-e-autovetores",
    "title": "Análise de Componentes Principais",
    "section": "Autovalores e Autovetores",
    "text": "Autovalores e Autovetores\n\nSeja \\(\\AM\\) uma matriz \\(M\\times M\\) com elementos constantes. Essa matriz, quando aplicada a um vetor \\(\\xM\\) com dimensão \\(M\\times 1\\), resulta em um vetor \\(\\yM\\) com dimensão \\(M\\times 1\\), ou seja,\n\\[\\begin{equation}\\label{eq:TL}\n\\yM=\\AM\\, \\xM.\n\\end{equation}\\]\nNota-se que a matriz \\(\\AM\\) representa uma transformação linear que transforma um vetor \\(\\xM\\) no vetor \\(\\yM\\). O vetor \\(\\yM\\) pode ser interpretado como resultado da projeção do vetor \\(\\xM\\) nas colunas da matriz \\(\\AM\\). De modo geral, essa transformação muda o módulo e a direção do vetor \\(\\xM\\).\nUm caso particular de grande interesse prático é aquele em que \\(\\xM\\) é um vetor não nulo e \\(\\AM\\,\\xM\\) é um múltiplo escalar de \\(\\xM\\). Para destacar esse vetor \\(\\xM\\) dos demais vamos denotá-lo como \\(\\vM\\), assim,\n\\[\n\\begin{equation*}\n\\fbox{$\\displaystyle\n\\AM\\, \\vM=\\lambda\\vM\n$}\n\\end{equation*}\n\\tag{1}\\]\nem que \\(\\lambda\\) é uma constante real ou complexa. Nesse caso, a transformação linear aplicada em \\(\\vM\\) resulta em um múltiplo escalar dele mesmo. O vetor particular \\(\\xM=\\vM\\) representa uma direção privilegiada no espaço formado pelas colunas da matriz \\(\\AM\\), tal que a ação da transformação \\(\\AM\\) sobre o vetor \\(\\vM\\) age apenas sobre o módulo desse vetor mantendo a sua direção. O escalar \\(\\lambda\\) é chamado de autovalor de \\(\\AM\\) e \\(\\vM\\) de autovetor associado a \\(\\lambda\\). Cabe observar que para um dado autovalor \\(\\lambda\\) podem existir vários vetores não nulos \\(\\vM\\) que satisfazem a Equação 1, como veremos a seguir. Além disso, o autovetor \\(\\vM\\) não pode ser nulo, porém, o autovalor \\(\\lambda\\) pode ser nulo."
  },
  {
    "objectID": "t_pca.html#a-obtenção-dos-autovalores-e-autovetores",
    "href": "t_pca.html#a-obtenção-dos-autovalores-e-autovetores",
    "title": "Análise de Componentes Principais",
    "section": "A obtenção dos autovalores e autovetores",
    "text": "A obtenção dos autovalores e autovetores\n\nPor conveniência vamos reescrever a Equação 1 da seguinte forma,\n\\[\n\\begin{equation}\n% \\AM\\, \\xM-\\lambda\\IM\\xM=\n\\left( \\AM-\\lambda\\IM\\right)\\vM=\\mathbf{0},\n\\end{equation}\n\\tag{2}\\]\nem que \\(\\IM\\) denota a matriz identidade de dimensão \\(M\\times M\\) e \\(**0**\\) um vetor de zeros de dimensão \\(M\\times 1\\). Nota-se que \\(\\lambda\\) é um autovalor da matriz \\(\\AM\\) se e somente se a Equação 2 possui uma solução não trivial.\nA partir da Equação 2, usando conceitos de solução de sistemas de equações e particularizando para o caso de interesse, seguem as afirmações:\n\nA Equação 2 terá uma solução não trivial se e somente se \\(\\left( \\AM-\\lambda\\IM\\right)\\) for singular, ou seja, \\[\n\\begin{equation*}\n\\fbox{$\\displaystyle\n\\det\\left( \\AM-\\lambda\\IM\\right)=0.\n$}\n\\end{equation*}\n\\tag{3}\\]\nAo aplicar a operação de determinante em \\(\\left( \\AM-\\lambda\\IM\\right)\\) obtemos um polinômio em \\(\\lambda\\), que representamos como \\[\n\\begin{equation}\\label{eq:sol2}\np(\\lambda)= \\det\\left( \\AM-\\lambda\\IM\\right)=\\lambda^M+c_1\\lambda^{M-1}+c_2\\lambda^{M-2}\\cdots c_M.\n\\end{equation}\n\\tag{4}\\]\nO polinômio \\(p(\\lambda)\\) é chamado de polinômio característico e \\(p(\\lambda)=0\\) é chamada de equação característica.\nNota-se que grau de \\(p(\\lambda)\\) é \\(M\\), portanto, { \\(p(\\lambda)=0\\)} possui \\(M\\) soluções. Essas soluções podem ser distintas, repetidas, reais ou complexas. Os valores de \\(\\lambda\\) que satisfazem a Equação 4 são os autovalores da matriz \\(\\AM\\). Portanto, \\(\\AM\\) possui \\(M\\) autovalores que podem ser distintos, repetidos, reais ou complexos.\nO conjunto de todas as soluções da Equação 2, aqui denotada como \\(\\cal{N}\\left( \\AM-\\lambda\\IM\\right)\\) é o espaço nulo de \\(\\AM-\\lambda\\IM\\) e todos os autovetores da matriz \\(\\AM\\) estão nesse espaço nulo. Assim, se \\(\\lambda\\) é um autovalor de \\(\\AM\\), então, \\({\\cal{N}}\\left( \\AM-\\lambda\\IM\\right)\\neq \\{ 0 \\}\\) e qualquer subespaço não nulo em \\(\\cal{N}\\left( \\AM-\\lambda\\IM\\right)\\) é um autovetor associado a \\(\\lambda\\). O subespaço \\(\\cal{N}\\left( \\AM-\\lambda\\IM\\right)\\) é chamado de autoespaço de \\(\\lambda\\)."
  },
  {
    "objectID": "t_pca.html#footnotes",
    "href": "t_pca.html#footnotes",
    "title": "Análise de Componentes Principais",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\n{O método dos multiplicadores de Lagrange é uma estratégia usada para encontrar mínimos e máximos locais de uma função sujeita a restrições. Para mais detalhes, ver por exemplo, https://en.wikipedia.org/wiki/Lagrange_multiplier.↩︎\nLembre que se duas variáveis aleatórias são independentes, elas são não correlacionadas, mas o contrário nem sempre é verdade, ou seja, podemos ter duas variáveis aleatórias não correlacionadas que são dependentes. No entanto, se as variáveis aleatórias forem gaussianas, a não correlação implica independência.↩︎"
  },
  {
    "objectID": "t_neuronio.html",
    "href": "t_neuronio.html",
    "title": "O modelo do neurônio",
    "section": "",
    "text": "Para introduzir o perceptron de Rosenblatt, vamos voltar ao exemplo das meias-luas, em que o algoritmo LMS foi utilizado para classificar os dados como pertencentes à Região A ou Região B, como mostrado na Figura 1, repetida aqui por conveniência.\n\n\n\n\n\n\nFigura 1: O problema de classificação das meias-luas (Haykin 2009).\n\n\n\nPara \\(r_1=10\\), \\(r_2=1\\), \\(r_3=6\\), \\(\\mu=10^{-3}\\) e \\(M=2\\), a saída do LMS no modo estocástico (\\(N_t=5000\\), \\(N_b=1\\) e \\(N_e=1\\)) está mostrada na Figura Figura 2. Nesta aplicação, considerou-se como sinal desejado \\(d=+1\\) para dados pertencentes à Região A e \\(d=-1\\) para os pertencentes à Região B. Com os pesos e bias da última iteração, o LMS consegue classificar os dados com uma taxa de erros de aproximadamente 0,6% por meio de uma separação linear entre as regiões. Apesar disso, a saída do algoritmo fica espalhada no intervalo \\([-1,\\!5\\;\\; 1,\\!5],\\) não havendo uma clara separação em torno do zero.\n\n\n\n\n\n\nFigura 2: Saída do algoritmo LMS (\\(\\eta=10^{-3}\\) e \\(M=2\\)) durante o treinamento no modo estocástico (\\(N_t=5000\\), \\(N_b=1\\) e \\(N_e=1\\)) utilizado no problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)).\n\n\n\nDiferente do LMS, o perceptron de Rosenblatt força a saída \\(y(n)\\) a assumir valores do conjunto \\(\\{-1,\\; +1\\}.\\) Para isso, ele introduz uma função não linear \\(\\varphi(\\cdot)\\) à saída do combinador. No caso, a função \\(\\varphi(\\cdot)\\) é um limitador abrupto (hard limiter), dado por\n\\[\n\\varphi(v)=\\text{sgn}(v)=\\left\\{\\begin{array}{cc}\n                      +1, & v\\geq 0 \\\\\n                      -1, & v&lt;0\n                    \\end{array},\n\\right.\n\\]\nem que \\(\\text{sgn}(\\cdot)\\) representa a função sinal, como mostrado na Figura 3.\n\n\n\n\n\n\nFigura 3: Função sinal.\n\n\n\nConsiderando o \\(n\\)-ésimo vetor dos dados de treinamento\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}\n\\]\ne o vetor de pesos e bias com dimensão \\(M+1\\)\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T},\n\\]\na saída do combinador linear pode ser escrita como\n\\[\nv(n) = \\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\n\\]\ne a saída do perceptron de Rosenblatt é dada por\n\\[\ny(n)=\\varphi(v(n))=\\text{sgn}(v(n)).\n\\]\nObserve que devido à função sinal, \\(y(n)\\in \\{-1,\\;+1\\}\\). O diagrama de fluxo de sinal do perceptron de Rosenblatt está mostrado na Figura 4.\n\n\n\n\n\n\nFigura 4: Fluxo de sinal do perceptron de Rosenblatt.\n\n\n\nComo no algoritmo LMS, os pesos são atualizados para minimizar o erro quadrático \\(e^2(n)\\), em que\n\\[\ne(n)=d(n)-\\varphi(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))=d(n)-{\\text{sgn}}(v(n))=d(n)-y(n).\n\\]\nObserve que \\(e(n)\\) assume agora três valores possíveis: \\(-2\\) ou \\(+2\\) quando \\(d(n)\\neq y(n)\\) e \\(0\\) quando \\(d(n)=y(n)\\). Como a função sinal não é derivável em todos os pontos, não é possível obter o algoritmo de maneira formal, como feito na dedução do algoritmo LMS. Além disso, note que\n\\[\n\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-\\frac{\\partial \\text{sgn}(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))}{\\partial \\mathbf{w}(n-1)}=-\\mathbf{x}(n)\\,{\\text{sgn}'}(v(n))=\\left\\{\\begin{array}{cc}\n                                         \\boldsymbol{0}, & v(n)\\neq 0 \\\\\n                                         \\nexists, & v(n)=0. \\\\\n                                       \\end{array}\n\\right.\n\\]\nIgnorando o fato da derivada não existir para \\(v(n)=0\\), os pesos não seriam atualizados se utilizássemos esse resultado, uma vez que o vetor gradiente é nulo para \\(v(n)\\neq 0\\). Por isso, utiliza-se a equação de atualização\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n).\n\\]\nAssim, os pesos são atualizados apenas quando \\(e(n)\\neq 0\\), ou seja, quando \\(y(n)\\neq d(n)\\). Caso contrário, \\(\\mathbf{w}(n)=\\mathbf{w}(n-1)\\). O passo de adaptação \\(\\eta\\), também chamado de taxa de aprendizado, é uma constante positiva que deve ser escolhida no intervalo \\(0&lt;\\eta\\leq 1\\). Como no caso do LMS, a escolha desse passo deve sempre levar em conta o compromisso entre estimativas mais precisas dos pesos e velocidade de aprendizado. A prova de convergência desse algoritmo para \\(\\eta=1\\) pode ser encontrada, por exemplo, em (Haykin 2009). Considerando a formulação matricial e o modo de treinamento mini-batch1, o pseudocódigo do algoritmo de treinamento do perceptron de Rosenblatt é mostrado no Algoritmo 1.\n\n\nExemplo 1 Sumário do algoritmo de treinamento do perceptron de Rosenblatt no modo mini-batch. \\(N_e\\) é o número de épocas, \\(N_b\\) o tamanho do mini-batch, \\(N_t\\) o número de dados de treinamento e \\(N_{mb}= \\lfloor N_t/N_b \\rfloor\\) o número de mini-batches por época.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(k=1,2,\\ldots, N_e\\), calcule:      Misture os dados de treinamento      Organize os dados na matriz \\(\\mathbf{X}(\\ell)\\) e no vetor \\(\\mathbf{d}(\\ell)\\) para \\(\\ell=0, 1,2,\\ldots, N_{mb}-1\\)      Para \\(\\ell=0, 1,2,\\ldots, N_{mb} - 1\\) calcule:       \\(m=(k-1)N_{mb}+\\ell+1\\)       \\(\\mathbf{v}_{m-1}(\\ell)=\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\\)       \\(\\mathbf{y}_{m-1}(\\ell)=\\text{sgn}(\\mathbf{v}_{m-1}(\\ell))\\)       \\(\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-{\\mathbf y}_{m-1}(\\ell)\\)       \\(\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\displaystyle\\frac{\\eta}{N_b}\\mathbf{X}^{\\rm T}(\\ell)\\mathbf{e}_{m-1}(\\ell)\\)      Fim    Fim\n\n\nVoltando ao exemplo das meias-luas, vamos considerar agora a solução obtida pelo perceptron de Rosenblatt no modo de treinamento batch com \\(M=2\\) e considerando \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\). Na Figura 5 são mostrados a função custo ao longo das épocas, os dados de teste e a separação linear das regiões no caso de \\(r_2=1\\). Observa-se neste caso que a função custo converge para zero e a separação obtida proporciona uma solução com taxa de erro nula. Para \\(r_2=-4\\), uma condição que viola a separabilidade linear, os resultados estão mostrados na Figura 6. Neste caso, observa-se que a função custo não converge mais para zero. Ela varia continuamente, indicando o “colapso” do algoritmo. Isso faz com que parte dos pontos da Região A sejam classificados erroneamente como pertencentes à Região B e vice-versa ao se utilizar os pesos e bias da última iteração, o que leva a uma taxa de erro de aproximadamente \\(12\\%\\).\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com o perceptron de Rosenblatt treinado em batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\)).\n\n\n\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com o perceptron de Rosenblatt treinado em batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\)).\n\n\n\nComparando o perceptron de Rosenblatt com o algoritmo LMS, percebe-se que ambos podem ser descritos pelo modelo da Figura 4. A única diferença é a função utilizada na saída do combinador linear. Como vimos, no perceptron de Rosenblatt utiliza-se \\(\\varphi(v)={\\rm sgn}(v)\\), enquanto no LMS considera-se \\(\\varphi(v)=v\\). Em termos de convergência, o sinal de erro utilizado no perceptron de Rosenblatt é limitado, pois \\(e(n)\\in \\{-2,\\; 0,\\; 2\\}\\), o que não ocorre no algoritmo LMS. Isso faz com que o perceptron de Rosenblatt não sofra divergência desde que as entradas sejam limitadas. O mesmo não se pode afirmar sobre o algoritmo LMS, pois o sinal de erro não é limitado. Dependendo do valor do passo de adaptação \\(\\eta\\), o LMS pode divergir. Apesar dessa diferença, ambos levam a fronteiras de separação que são retas (ou hiperplanos no caso em que \\(M\\geq 2\\)). Essas soluções são boas apenas quando há separabilidade linear, o que no exemplo das meias-luas ocorre para \\(r_2=1\\), mas não ocorre para \\(r_2=-4\\). Para gerar uma fronteira não linear, podemos usar uma rede neural, como será visto posteriormente. Antes de vermos que o perceptron de Rosenblatt é um dos primeiros modelos de neurônio, unidade básica de uma rede neural, vamos tratar a seguir da regressão logística.\n\n\n\nA adaptação dos pesos do perceptron de Rosenblatt não levam em conta a derivada da função \\(\\varphi(v)\\), uma vez que essa função tem derivada nula para \\(v\\neq 0\\) e não é definida para \\(v=0\\). Em vez de usar a função sinal no perceptron de Rosenblatt, poderíamos considerar uma função \\(\\varphi(v)\\) com derivada definida e não nula para todo \\(v\\). Neste caso, teríamos \\[\ne(n)=d(n)-\\varphi({\\mathbf{x}^{\\rm T}}(n)\\mathbf{w}(n-1))=d(n)-\\varphi(v(n))=d(n)-y(n),\n\\] cuja derivada em relação a \\(\\mathbf{w}(n-1)\\) é \\[\n\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-\\frac{\\partial \\varphi(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))}{\\partial \\mathbf{w}(n-1)}=-\\mathbf{x}(n)\\,{\\varphi'}(v(n)).\n\\] Considerando a minimização do erro quadrático instantâneo como no LMS, temos o vetor gradiente dado por \\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\text{MSE}}=2e(n)\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-2e(n)\\,{\\varphi'}(v(n))\\,\\mathbf{x}(n).\n\\] Assim, o vetor de pesos no modo estocástico deve ser adaptado como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\, e(n)\\,{\\varphi'}(v(n))\\,\\mathbf{x}(n).\n$}\n\\end{equation*}\\]\nA minimização do erro quadrático pode levar o algoritmo a ficar parado em mínimos locais. Uma alternativa é usar a entropia cruzada, definida como \\[  \nJ_{\\rm EC} = -  \\left[ d(n) \\ln\\left({y(n)}\\right) + [1 - d(n)] \\ln{\\left(1 -y(n)\\right)}\\right].\n\\] Lembrando que \\(v(n)=\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\\) e que \\(y(n)=\\varphi(v(n))\\), chega-se ao seguinte gradiente \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\text{EC}}=&-\\frac{d(n)}{y(n)}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}+\\frac{1-d(n)}{1-y(n)}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}\\nonumber\\\\\n=&-\\frac{e(n)}{y(n)(1-y(n))}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}\\nonumber\\\\\n=&-\\frac{e(n)}{y(n)(1-y(n))}\\,\\varphi'(v(n))\\,\\mathbf{x}(n).\\nonumber\n\\end{align*}\n\\] Assim, o vetor de pesos no modo estocástico deve ser adaptado como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\, \\frac{e(n)\\varphi'(v(n))}{y(n)(1-y(n))}\\mathbf{x}(n).\n$}\n\\end{equation*}\\]\nEm um problema de classificação binária com rótulos iguais a 0 ou 1, uma possível candidata para a função \\(\\varphi(\\cdot)\\) que apresenta derivada não nula e definida em todos os pontos é a função sigmoidal, também conhecida como função logística. Essa função é definida como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi(v)={\\rm sgm}(v)=\\displaystyle\\frac{1}{1+e^{-v}}\n$}\n\\end{equation*}\\] e tem derivada dada por \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'(v)=\\frac{\\rm d}{{\\rm d}v}{\\rm sgm}(v)=\\displaystyle \\frac{e^{-v}}{\\left[1+e^{-v}\\right]^2}=  \\varphi(v)[1-\\varphi(v)]=y(1-y),\n$}\n\\end{equation*}\\] em que se usou o fato da saída do neurônio ser \\(y=\\varphi(v)\\). Na Figura 7 são mostrados gráficos da função sigmoidal e de sua derivada. Pode-se observar que a saída do neurônio com função sigmoidal fica no intervalo \\([0,\\; 1]\\). Caso os rótulos sejam iguais a \\(-1\\) e \\(1\\), pode-se considerar a função tangente hiperbólica, dada por \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi(v)={\\rm tanh}(v)=\\frac{e^{v}-e^{-v}}{e^{v}+e^{-v}},\n$}\n\\end{equation*}\\] cuja derivada é \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'(v)=\\frac{\\rm d}{{\\rm d}v}{\\rm tanh}(v)=\\left[1-{\\rm tanh}^2(v)\\right]=(1-y)(1+y),\n$}\n\\end{equation*}\\] em que se usou o fato da saída do neurônio ser igual a \\(y={\\rm tanh}(v)\\). Na Figura 8 são mostrados gráficos da tangente hiperbólica e de sua derivada.\n\n\n\n\n\n\nFigura 7: Função logística e sua derivada.\n\n\n\n\n\n\n\n\n\nFigura 8: Função tangente hiperbólica e sua derivada.\n\n\n\nVamos agora obter quatro formas de adaptar os pesos levando em conta as funções logística (sgm) e tangente hiperbólica (tanh) e as funções custo do erro quadrático médio (MSE) e da entropia cruzada (EC): \\[\n\\begin{align*}\n&\\text{MSE, sgm:}\\;\\;\\; \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)y(n)[1-y(n)]\\mathbf{x}(n)\\\\\n&\\text{EC, sgm:}\\;\\;\\;\\;\\;\\, \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n)\\\\\n&\\text{MSE, tanh:}\\;\\; \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)[1+y(n)][1-y(n)]\\mathbf{x}(n)\\\\\n&\\text{EC, tanh:}\\;\\;\\;\\;\\, \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\!\\left(1+\\frac{1}{y(n)}\\right)\\mathbf{x}(n).\n\\end{align*}\n\\] A equação obtida com EC e sgm é conhecida na literatura como regressão logística para o caso binário. As demais podem ser interpretadas como variantes. Cabe observar que na equação obtida com EC e tanh aparece o termo \\(1/y(n)\\), o que pode levar a divisão por \\(0\\). Para evitar isso, pode-se somar uma constante pequena em \\(y(n),\\) o que acaba alterando a dinâmica do algoritmo. Por isso, essa equação não é muito utilizada e o que se faz é mapear os rótulos iguais a \\(-1\\) em \\(0\\) para se utilizar as versões do algoritmo obtidas com a função logística.\nPara exemplificar, vamos considerar novamente o problema de classificação binária com as meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), mas agora os rótulos da Região B foram mapeados de \\(-1\\) para \\(0\\) a fim de se utilizar as equações obtidas anteriormente com a função logística. Os resultados utilizando o modo de treinamento mini-batch podem ser vistos na Figura 9 e na Figura 10, considerando a minimização do MSE e da EC, respectivamente. Os pesos e bias obtidos na minimização de cada função custo são diferentes, mas a separação das regiões são similares e levam a uma taxa de erro de aproximadamente \\(8\\%\\). Apesar dessa taxa ser um pouco menor que a obtida com o neurônio de Rosenblatt, a separação continua linear.\n\n\n\n\n\n\nFigura 9: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Pesos e bias ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com a regressão logística que minimiza o MSE; modo mini-batch (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_b=10\\) e \\(N_e=100\\)); Taxa de erro de \\(8,15\\%\\).\n\n\n\n\n\n\n\n\n\nFigura 10: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Pesos e ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com a regressão logística que minimiza a EC; modo (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_b=10\\) e \\(N_e=100\\)); Taxa de erro de \\(8,25\\%\\).\n\n\n\nNa literatura a regressão logística é abordada sob o ponto de vista probabilístico como, por exemplo, em (Bishop 2006). Nessa referência, também é abordado o caso multiclasse.\n\n\n\nNo início do século passado, o médico e histologista espanhol Ramón y Cajál foi o primeiro a introduzir a ideia dos neurônios como unidades básicas do sistema nervoso. Os neurônios são células altamente especializadas na transmissão de informações na forma de pulsos nervosos. As ligações entre os neurônios são chamadas de sinapses, que tem por função enviar sinais por transmissões sinápticas para ocorrer ações específicas no corpo. A taxa dessas transmissões é considerada baixa quando comparada com portas lógicas de silício. Eventos em um chip de silício acontecem na faixa de nanossegundos, enquanto os eventos neurais acontecem na faixa de milissegundos. No entanto, essa taxa “baixa” é compensada pelo impressionante número de neurônios existentes no sistema nervoso humano, estimado em mais de 86 bilhões. Em termos de sinapses, esse número aumenta para mais de 60 trilhões. O resultado final é que o cérebro é uma estrutura extremamente eficiente.\nO neurônio biológico está esquematizado na Figura 11. A atividade do neurônio é caracterizada por pulsos elétricos da ordem de milivolts e duração da ordem de milissegundos. Ele recebe esses pulsos de outros neurônios pelos seus dendritos. Se o sinal acumulado exceder um certo valor, um pulso é enviado via axônio aos seus terminais, que por sua vez, se acoplam a outros neurônios. Grosso modo, a computação realizada por um neurônio na sua saída (no seu axônio) pode ser resumida na frequência dos pulsos. Se houver poucos pulsos por unidade de tempo, o neurônio é considerado pouco ativo. Em contrapartida, se ele tiver muitos pulsos por unidade de tempo, haverá mais estímulos sinápticos e o músculo que o neurônio controla, por exemplo, é forçado a uma atividade maior.\n\n\n\n\n\n\nFigura 11: Neurônio biológico. Fonte: adaptado de (Dürr e Sick 2020).\n\n\n\nRedes neurais surgiram para buscar modelar o cérebro humano. Nos anos de surgimento das redes neurais (1943-1960), vários pesquisadores se destacam por suas contribuições pioneiras (Haykin 2009):\n\nMcCulloch e Pitts (1943) por introduzirem a ideia de redes neurais como máquinas de computação;\nHebb (1949) por postular a primeira regra de aprendizagem auto-organizada;\nRosenblatt (1958) por propor o perceptron como o primeiro modelo de aprendizagem supervisionada;\nWidrow e Hoff (1960) por propor o Adaline (adaptive linear element), que deu origem ao algoritmo LMS.\n\nInspirado no funcionamento do neurônio biológico, Rosenblatt propôs o modelo de neurônio artificial, chamado de perceptron, como ilustrado na Figura 4. O neurônio biológico recebe vários estímulos de outros neurônios que chegam por seus dendritos, esses estímulos são então acumulados e se exceder um limiar, o neurônio gera um estímulo no seu axônio que são transmitidos a outros neurônios. No modelo matemático de Rosenblatt, esses estímulos são representados pelo vetor de entrada \\(\\mathbf{x}(n)\\) e o acúmulo dos estímulos pela soma ponderada da entrada com os pesos, gerando o sinal \\(v(n)\\). Se \\(v(n)&lt;0\\), o neurônio estará em repouso. Caso contrário, estará ativo e um novo estímulo, representado por \\(y(n)\\), é gerado. Aqui cabe uma observação: para representar o neurônio em repouso, talvez fosse mais adequado considerar a função degrau (função de Heaviside) em vez da função sinal. Assim, \\(y(n)=0\\) para \\(v(n)&lt;0\\). No entanto, pensando na implementação do modelo com um circuito analógico, pode ser mais adequado considerar uma tensão negativa em vez de uma tensão nula para representar o repouso e para isso, a função sinal se mostrou mais adequada.\nEm julho de 1958, o escritório de Pesquisa Naval dos EUA revelou uma invenção notável. Um IBM 704, um computador de 5 toneladas que ocupava uma sala, foi alimentado com uma série de cartões perfurados. Após 50 tentativas, o computador aprendeu a distinguir os cartões marcados à esquerda dos cartões marcados à direita. Foi uma demonstração do perceptron de Rosenblatt, a primeira máquina capaz de ter uma ideia original. Na época, Rosenblatt era psicólogo pesquisador e engenheiro de projetos no Laboratório Aeronáutico da Cornell em Buffalo, Nova York. “As histórias sobre a criação de máquinas com qualidades humanas têm sido fascinantes em ficção científica. No entanto, estamos prestes a testemunhar o nascimento de tal máquina - uma máquina capaz de perceber, reconhecer e identificar seus arredores sem qualquer treinamento ou controle humano”, escreveu Rosenblatt em 1958. Ele estava certo, mas levou aproximadamente meio século para vermos isso acontecer. Na Figura 12, são mostradas uma imagem do título da publicação de Rosenblatt de 1958 e uma foto de Rosenblatt em 1960 com seu perceptron chamado de Mark I, uma máquina eletromecânica implementava os pesos adaptativos por meio de potenciômetros que eram ajustados por atuadores.\n\n\n\n\n\n\nFigura 12: Publicação de Rosenblatt de 1958 (à esquerda) e foto de Rosenblatt e seu perceptron chamado de Mark I em 1960 (à direita) [Fonte].\n\n\n\nDesde 1960, muita pesquisa foi feita com o objetivo de melhorar o modelo do cérebro humano. Apesar dos inúmeros avanços, ainda estamos longe de termos um sistema que consiga modelar de maneira precisa o cérebro, devido à sua alta complexidade e eficiência. Apesar das redes neurais artificiais serem inspiradas no funcionamento do cérebro, vamos encará-las como sistemas não lineares que podem ser aplicados como soluções eficientes em problemas de regressão e classificação.\nUma sugestão de vídeo sobre o surgimento das redes neurais é o The man who forever changed artificial intelligence.\nQuem quiser se aprofundar em modelos de neurônios e do cérebro humano já que esse assunto está fora do escopo deste curso, sugerimos o livro (Gerstner et al. 2014)."
  },
  {
    "objectID": "t_neuronio.html#o-perceptron-de-rosenblatt",
    "href": "t_neuronio.html#o-perceptron-de-rosenblatt",
    "title": "O modelo do neurônio",
    "section": "",
    "text": "Para introduzir o perceptron de Rosenblatt, vamos voltar ao exemplo das meias-luas, em que o algoritmo LMS foi utilizado para classificar os dados como pertencentes à Região A ou Região B, como mostrado na Figura 1, repetida aqui por conveniência.\n\n\n\n\n\n\nFigura 1: O problema de classificação das meias-luas (Haykin 2009).\n\n\n\nPara \\(r_1=10\\), \\(r_2=1\\), \\(r_3=6\\), \\(\\mu=10^{-3}\\) e \\(M=2\\), a saída do LMS no modo estocástico (\\(N_t=5000\\), \\(N_b=1\\) e \\(N_e=1\\)) está mostrada na Figura Figura 2. Nesta aplicação, considerou-se como sinal desejado \\(d=+1\\) para dados pertencentes à Região A e \\(d=-1\\) para os pertencentes à Região B. Com os pesos e bias da última iteração, o LMS consegue classificar os dados com uma taxa de erros de aproximadamente 0,6% por meio de uma separação linear entre as regiões. Apesar disso, a saída do algoritmo fica espalhada no intervalo \\([-1,\\!5\\;\\; 1,\\!5],\\) não havendo uma clara separação em torno do zero.\n\n\n\n\n\n\nFigura 2: Saída do algoritmo LMS (\\(\\eta=10^{-3}\\) e \\(M=2\\)) durante o treinamento no modo estocástico (\\(N_t=5000\\), \\(N_b=1\\) e \\(N_e=1\\)) utilizado no problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)).\n\n\n\nDiferente do LMS, o perceptron de Rosenblatt força a saída \\(y(n)\\) a assumir valores do conjunto \\(\\{-1,\\; +1\\}.\\) Para isso, ele introduz uma função não linear \\(\\varphi(\\cdot)\\) à saída do combinador. No caso, a função \\(\\varphi(\\cdot)\\) é um limitador abrupto (hard limiter), dado por\n\\[\n\\varphi(v)=\\text{sgn}(v)=\\left\\{\\begin{array}{cc}\n                      +1, & v\\geq 0 \\\\\n                      -1, & v&lt;0\n                    \\end{array},\n\\right.\n\\]\nem que \\(\\text{sgn}(\\cdot)\\) representa a função sinal, como mostrado na Figura 3.\n\n\n\n\n\n\nFigura 3: Função sinal.\n\n\n\nConsiderando o \\(n\\)-ésimo vetor dos dados de treinamento\n\\[\n\\mathbf{x}(n)=[\\,1\\;x_{1n}\\; x_{2n}\\; \\cdots\\; x_{Mn}\\,]^{\\rm T}\n\\]\ne o vetor de pesos e bias com dimensão \\(M+1\\)\n\\[\n\\mathbf{w}(n) = [\\,b(n)\\;w_1(n)\\;\\cdots\\;w_M(n)\\,]^{\\rm T},\n\\]\na saída do combinador linear pode ser escrita como\n\\[\nv(n) = \\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\n\\]\ne a saída do perceptron de Rosenblatt é dada por\n\\[\ny(n)=\\varphi(v(n))=\\text{sgn}(v(n)).\n\\]\nObserve que devido à função sinal, \\(y(n)\\in \\{-1,\\;+1\\}\\). O diagrama de fluxo de sinal do perceptron de Rosenblatt está mostrado na Figura 4.\n\n\n\n\n\n\nFigura 4: Fluxo de sinal do perceptron de Rosenblatt.\n\n\n\nComo no algoritmo LMS, os pesos são atualizados para minimizar o erro quadrático \\(e^2(n)\\), em que\n\\[\ne(n)=d(n)-\\varphi(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))=d(n)-{\\text{sgn}}(v(n))=d(n)-y(n).\n\\]\nObserve que \\(e(n)\\) assume agora três valores possíveis: \\(-2\\) ou \\(+2\\) quando \\(d(n)\\neq y(n)\\) e \\(0\\) quando \\(d(n)=y(n)\\). Como a função sinal não é derivável em todos os pontos, não é possível obter o algoritmo de maneira formal, como feito na dedução do algoritmo LMS. Além disso, note que\n\\[\n\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-\\frac{\\partial \\text{sgn}(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))}{\\partial \\mathbf{w}(n-1)}=-\\mathbf{x}(n)\\,{\\text{sgn}'}(v(n))=\\left\\{\\begin{array}{cc}\n                                         \\boldsymbol{0}, & v(n)\\neq 0 \\\\\n                                         \\nexists, & v(n)=0. \\\\\n                                       \\end{array}\n\\right.\n\\]\nIgnorando o fato da derivada não existir para \\(v(n)=0\\), os pesos não seriam atualizados se utilizássemos esse resultado, uma vez que o vetor gradiente é nulo para \\(v(n)\\neq 0\\). Por isso, utiliza-se a equação de atualização\n\\[\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n).\n\\]\nAssim, os pesos são atualizados apenas quando \\(e(n)\\neq 0\\), ou seja, quando \\(y(n)\\neq d(n)\\). Caso contrário, \\(\\mathbf{w}(n)=\\mathbf{w}(n-1)\\). O passo de adaptação \\(\\eta\\), também chamado de taxa de aprendizado, é uma constante positiva que deve ser escolhida no intervalo \\(0&lt;\\eta\\leq 1\\). Como no caso do LMS, a escolha desse passo deve sempre levar em conta o compromisso entre estimativas mais precisas dos pesos e velocidade de aprendizado. A prova de convergência desse algoritmo para \\(\\eta=1\\) pode ser encontrada, por exemplo, em (Haykin 2009). Considerando a formulação matricial e o modo de treinamento mini-batch1, o pseudocódigo do algoritmo de treinamento do perceptron de Rosenblatt é mostrado no Algoritmo 1.\n\n\nExemplo 1 Sumário do algoritmo de treinamento do perceptron de Rosenblatt no modo mini-batch. \\(N_e\\) é o número de épocas, \\(N_b\\) o tamanho do mini-batch, \\(N_t\\) o número de dados de treinamento e \\(N_{mb}= \\lfloor N_t/N_b \\rfloor\\) o número de mini-batches por época.\nInicialização: \\(\\mathbf{w}(0)=\\boldsymbol{0}\\)    Para \\(k=1,2,\\ldots, N_e\\), calcule:      Misture os dados de treinamento      Organize os dados na matriz \\(\\mathbf{X}(\\ell)\\) e no vetor \\(\\mathbf{d}(\\ell)\\) para \\(\\ell=0, 1,2,\\ldots, N_{mb}-1\\)      Para \\(\\ell=0, 1,2,\\ldots, N_{mb} - 1\\) calcule:       \\(m=(k-1)N_{mb}+\\ell+1\\)       \\(\\mathbf{v}_{m-1}(\\ell)=\\mathbf{X}(\\ell)\\mathbf{w}(m-1)\\)       \\(\\mathbf{y}_{m-1}(\\ell)=\\text{sgn}(\\mathbf{v}_{m-1}(\\ell))\\)       \\(\\mathbf{e}_{m-1}(\\ell)=\\mathbf{d}(\\ell)-{\\mathbf y}_{m-1}(\\ell)\\)       \\(\\mathbf{w}(m)=\\mathbf{w}(m-1)+\\displaystyle\\frac{\\eta}{N_b}\\mathbf{X}^{\\rm T}(\\ell)\\mathbf{e}_{m-1}(\\ell)\\)      Fim    Fim\n\n\nVoltando ao exemplo das meias-luas, vamos considerar agora a solução obtida pelo perceptron de Rosenblatt no modo de treinamento batch com \\(M=2\\) e considerando \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\). Na Figura 5 são mostrados a função custo ao longo das épocas, os dados de teste e a separação linear das regiões no caso de \\(r_2=1\\). Observa-se neste caso que a função custo converge para zero e a separação obtida proporciona uma solução com taxa de erro nula. Para \\(r_2=-4\\), uma condição que viola a separabilidade linear, os resultados estão mostrados na Figura 6. Neste caso, observa-se que a função custo não converge mais para zero. Ela varia continuamente, indicando o “colapso” do algoritmo. Isso faz com que parte dos pontos da Região A sejam classificados erroneamente como pertencentes à Região B e vice-versa ao se utilizar os pesos e bias da última iteração, o que leva a uma taxa de erro de aproximadamente \\(12\\%\\).\n\n\n\n\n\n\nFigura 5: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com o perceptron de Rosenblatt treinado em batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\)).\n\n\n\n\n\n\n\n\n\nFigura 6: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\)). Função custo ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com o perceptron de Rosenblatt treinado em batch (\\(M=2\\), \\(\\eta=10^{-3}\\), \\(N_t=5000\\), \\(N_b=N_t\\) e \\(N_e=50\\)).\n\n\n\nComparando o perceptron de Rosenblatt com o algoritmo LMS, percebe-se que ambos podem ser descritos pelo modelo da Figura 4. A única diferença é a função utilizada na saída do combinador linear. Como vimos, no perceptron de Rosenblatt utiliza-se \\(\\varphi(v)={\\rm sgn}(v)\\), enquanto no LMS considera-se \\(\\varphi(v)=v\\). Em termos de convergência, o sinal de erro utilizado no perceptron de Rosenblatt é limitado, pois \\(e(n)\\in \\{-2,\\; 0,\\; 2\\}\\), o que não ocorre no algoritmo LMS. Isso faz com que o perceptron de Rosenblatt não sofra divergência desde que as entradas sejam limitadas. O mesmo não se pode afirmar sobre o algoritmo LMS, pois o sinal de erro não é limitado. Dependendo do valor do passo de adaptação \\(\\eta\\), o LMS pode divergir. Apesar dessa diferença, ambos levam a fronteiras de separação que são retas (ou hiperplanos no caso em que \\(M\\geq 2\\)). Essas soluções são boas apenas quando há separabilidade linear, o que no exemplo das meias-luas ocorre para \\(r_2=1\\), mas não ocorre para \\(r_2=-4\\). Para gerar uma fronteira não linear, podemos usar uma rede neural, como será visto posteriormente. Antes de vermos que o perceptron de Rosenblatt é um dos primeiros modelos de neurônio, unidade básica de uma rede neural, vamos tratar a seguir da regressão logística."
  },
  {
    "objectID": "t_neuronio.html#regressão-logística-para-classificação-binária",
    "href": "t_neuronio.html#regressão-logística-para-classificação-binária",
    "title": "O modelo do neurônio",
    "section": "",
    "text": "A adaptação dos pesos do perceptron de Rosenblatt não levam em conta a derivada da função \\(\\varphi(v)\\), uma vez que essa função tem derivada nula para \\(v\\neq 0\\) e não é definida para \\(v=0\\). Em vez de usar a função sinal no perceptron de Rosenblatt, poderíamos considerar uma função \\(\\varphi(v)\\) com derivada definida e não nula para todo \\(v\\). Neste caso, teríamos \\[\ne(n)=d(n)-\\varphi({\\mathbf{x}^{\\rm T}}(n)\\mathbf{w}(n-1))=d(n)-\\varphi(v(n))=d(n)-y(n),\n\\] cuja derivada em relação a \\(\\mathbf{w}(n-1)\\) é \\[\n\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-\\frac{\\partial \\varphi(\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1))}{\\partial \\mathbf{w}(n-1)}=-\\mathbf{x}(n)\\,{\\varphi'}(v(n)).\n\\] Considerando a minimização do erro quadrático instantâneo como no LMS, temos o vetor gradiente dado por \\[\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\text{MSE}}=2e(n)\\frac{\\partial e(n)}{\\partial \\mathbf{w}(n-1)}=-2e(n)\\,{\\varphi'}(v(n))\\,\\mathbf{x}(n).\n\\] Assim, o vetor de pesos no modo estocástico deve ser adaptado como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\, e(n)\\,{\\varphi'}(v(n))\\,\\mathbf{x}(n).\n$}\n\\end{equation*}\\]\nA minimização do erro quadrático pode levar o algoritmo a ficar parado em mínimos locais. Uma alternativa é usar a entropia cruzada, definida como \\[  \nJ_{\\rm EC} = -  \\left[ d(n) \\ln\\left({y(n)}\\right) + [1 - d(n)] \\ln{\\left(1 -y(n)\\right)}\\right].\n\\] Lembrando que \\(v(n)=\\mathbf{x}^{\\rm T}(n)\\mathbf{w}(n-1)\\) e que \\(y(n)=\\varphi(v(n))\\), chega-se ao seguinte gradiente \\[\n\\begin{align*}\n\\widehat{\\boldsymbol{\\nabla}}_{\\mathbf{w}}J_{\\text{EC}}=&-\\frac{d(n)}{y(n)}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}+\\frac{1-d(n)}{1-y(n)}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}\\nonumber\\\\\n=&-\\frac{e(n)}{y(n)(1-y(n))}\\frac{\\partial y(n)}{\\partial \\mathbf{w}(n-1)}\\nonumber\\\\\n=&-\\frac{e(n)}{y(n)(1-y(n))}\\,\\varphi'(v(n))\\,\\mathbf{x}(n).\\nonumber\n\\end{align*}\n\\] Assim, o vetor de pesos no modo estocástico deve ser adaptado como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta\\, \\frac{e(n)\\varphi'(v(n))}{y(n)(1-y(n))}\\mathbf{x}(n).\n$}\n\\end{equation*}\\]\nEm um problema de classificação binária com rótulos iguais a 0 ou 1, uma possível candidata para a função \\(\\varphi(\\cdot)\\) que apresenta derivada não nula e definida em todos os pontos é a função sigmoidal, também conhecida como função logística. Essa função é definida como \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi(v)={\\rm sgm}(v)=\\displaystyle\\frac{1}{1+e^{-v}}\n$}\n\\end{equation*}\\] e tem derivada dada por \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'(v)=\\frac{\\rm d}{{\\rm d}v}{\\rm sgm}(v)=\\displaystyle \\frac{e^{-v}}{\\left[1+e^{-v}\\right]^2}=  \\varphi(v)[1-\\varphi(v)]=y(1-y),\n$}\n\\end{equation*}\\] em que se usou o fato da saída do neurônio ser \\(y=\\varphi(v)\\). Na Figura 7 são mostrados gráficos da função sigmoidal e de sua derivada. Pode-se observar que a saída do neurônio com função sigmoidal fica no intervalo \\([0,\\; 1]\\). Caso os rótulos sejam iguais a \\(-1\\) e \\(1\\), pode-se considerar a função tangente hiperbólica, dada por \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi(v)={\\rm tanh}(v)=\\frac{e^{v}-e^{-v}}{e^{v}+e^{-v}},\n$}\n\\end{equation*}\\] cuja derivada é \\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\varphi'(v)=\\frac{\\rm d}{{\\rm d}v}{\\rm tanh}(v)=\\left[1-{\\rm tanh}^2(v)\\right]=(1-y)(1+y),\n$}\n\\end{equation*}\\] em que se usou o fato da saída do neurônio ser igual a \\(y={\\rm tanh}(v)\\). Na Figura 8 são mostrados gráficos da tangente hiperbólica e de sua derivada.\n\n\n\n\n\n\nFigura 7: Função logística e sua derivada.\n\n\n\n\n\n\n\n\n\nFigura 8: Função tangente hiperbólica e sua derivada.\n\n\n\nVamos agora obter quatro formas de adaptar os pesos levando em conta as funções logística (sgm) e tangente hiperbólica (tanh) e as funções custo do erro quadrático médio (MSE) e da entropia cruzada (EC): \\[\n\\begin{align*}\n&\\text{MSE, sgm:}\\;\\;\\; \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)y(n)[1-y(n)]\\mathbf{x}(n)\\\\\n&\\text{EC, sgm:}\\;\\;\\;\\;\\;\\, \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\mathbf{x}(n)\\\\\n&\\text{MSE, tanh:}\\;\\; \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)[1+y(n)][1-y(n)]\\mathbf{x}(n)\\\\\n&\\text{EC, tanh:}\\;\\;\\;\\;\\, \\mathbf{w}(n)=\\mathbf{w}(n-1)+\\eta e(n)\\!\\left(1+\\frac{1}{y(n)}\\right)\\mathbf{x}(n).\n\\end{align*}\n\\] A equação obtida com EC e sgm é conhecida na literatura como regressão logística para o caso binário. As demais podem ser interpretadas como variantes. Cabe observar que na equação obtida com EC e tanh aparece o termo \\(1/y(n)\\), o que pode levar a divisão por \\(0\\). Para evitar isso, pode-se somar uma constante pequena em \\(y(n),\\) o que acaba alterando a dinâmica do algoritmo. Por isso, essa equação não é muito utilizada e o que se faz é mapear os rótulos iguais a \\(-1\\) em \\(0\\) para se utilizar as versões do algoritmo obtidas com a função logística.\nPara exemplificar, vamos considerar novamente o problema de classificação binária com as meias-luas com \\(r_1=10\\), \\(r_2=-4\\) e \\(r_3=6\\), mas agora os rótulos da Região B foram mapeados de \\(-1\\) para \\(0\\) a fim de se utilizar as equações obtidas anteriormente com a função logística. Os resultados utilizando o modo de treinamento mini-batch podem ser vistos na Figura 9 e na Figura 10, considerando a minimização do MSE e da EC, respectivamente. Os pesos e bias obtidos na minimização de cada função custo são diferentes, mas a separação das regiões são similares e levam a uma taxa de erro de aproximadamente \\(8\\%\\). Apesar dessa taxa ser um pouco menor que a obtida com o neurônio de Rosenblatt, a separação continua linear.\n\n\n\n\n\n\nFigura 9: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Pesos e bias ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com a regressão logística que minimiza o MSE; modo mini-batch (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_b=10\\) e \\(N_e=100\\)); Taxa de erro de \\(8,15\\%\\).\n\n\n\n\n\n\n\n\n\nFigura 10: O problema de classificação das meias-luas (\\(r_1=10\\), \\(r_2=1\\) e \\(r_3=6\\)). Pesos e ao longo das épocas de treinamento (figura à esquerda); Dados de teste (\\(N_{\\text{teste}}=2000\\)) e separação das regiões (figura à direita) obtida com a regressão logística que minimiza a EC; modo (\\(M=2\\), \\(\\eta=10^{-2}\\), \\(N_t=5000\\), \\(N_b=10\\) e \\(N_e=100\\)); Taxa de erro de \\(8,25\\%\\).\n\n\n\nNa literatura a regressão logística é abordada sob o ponto de vista probabilístico como, por exemplo, em (Bishop 2006). Nessa referência, também é abordado o caso multiclasse."
  },
  {
    "objectID": "t_neuronio.html#o-neurônio-biológico-e-um-pouco-de-história",
    "href": "t_neuronio.html#o-neurônio-biológico-e-um-pouco-de-história",
    "title": "O modelo do neurônio",
    "section": "",
    "text": "No início do século passado, o médico e histologista espanhol Ramón y Cajál foi o primeiro a introduzir a ideia dos neurônios como unidades básicas do sistema nervoso. Os neurônios são células altamente especializadas na transmissão de informações na forma de pulsos nervosos. As ligações entre os neurônios são chamadas de sinapses, que tem por função enviar sinais por transmissões sinápticas para ocorrer ações específicas no corpo. A taxa dessas transmissões é considerada baixa quando comparada com portas lógicas de silício. Eventos em um chip de silício acontecem na faixa de nanossegundos, enquanto os eventos neurais acontecem na faixa de milissegundos. No entanto, essa taxa “baixa” é compensada pelo impressionante número de neurônios existentes no sistema nervoso humano, estimado em mais de 86 bilhões. Em termos de sinapses, esse número aumenta para mais de 60 trilhões. O resultado final é que o cérebro é uma estrutura extremamente eficiente.\nO neurônio biológico está esquematizado na Figura 11. A atividade do neurônio é caracterizada por pulsos elétricos da ordem de milivolts e duração da ordem de milissegundos. Ele recebe esses pulsos de outros neurônios pelos seus dendritos. Se o sinal acumulado exceder um certo valor, um pulso é enviado via axônio aos seus terminais, que por sua vez, se acoplam a outros neurônios. Grosso modo, a computação realizada por um neurônio na sua saída (no seu axônio) pode ser resumida na frequência dos pulsos. Se houver poucos pulsos por unidade de tempo, o neurônio é considerado pouco ativo. Em contrapartida, se ele tiver muitos pulsos por unidade de tempo, haverá mais estímulos sinápticos e o músculo que o neurônio controla, por exemplo, é forçado a uma atividade maior.\n\n\n\n\n\n\nFigura 11: Neurônio biológico. Fonte: adaptado de (Dürr e Sick 2020).\n\n\n\nRedes neurais surgiram para buscar modelar o cérebro humano. Nos anos de surgimento das redes neurais (1943-1960), vários pesquisadores se destacam por suas contribuições pioneiras (Haykin 2009):\n\nMcCulloch e Pitts (1943) por introduzirem a ideia de redes neurais como máquinas de computação;\nHebb (1949) por postular a primeira regra de aprendizagem auto-organizada;\nRosenblatt (1958) por propor o perceptron como o primeiro modelo de aprendizagem supervisionada;\nWidrow e Hoff (1960) por propor o Adaline (adaptive linear element), que deu origem ao algoritmo LMS.\n\nInspirado no funcionamento do neurônio biológico, Rosenblatt propôs o modelo de neurônio artificial, chamado de perceptron, como ilustrado na Figura 4. O neurônio biológico recebe vários estímulos de outros neurônios que chegam por seus dendritos, esses estímulos são então acumulados e se exceder um limiar, o neurônio gera um estímulo no seu axônio que são transmitidos a outros neurônios. No modelo matemático de Rosenblatt, esses estímulos são representados pelo vetor de entrada \\(\\mathbf{x}(n)\\) e o acúmulo dos estímulos pela soma ponderada da entrada com os pesos, gerando o sinal \\(v(n)\\). Se \\(v(n)&lt;0\\), o neurônio estará em repouso. Caso contrário, estará ativo e um novo estímulo, representado por \\(y(n)\\), é gerado. Aqui cabe uma observação: para representar o neurônio em repouso, talvez fosse mais adequado considerar a função degrau (função de Heaviside) em vez da função sinal. Assim, \\(y(n)=0\\) para \\(v(n)&lt;0\\). No entanto, pensando na implementação do modelo com um circuito analógico, pode ser mais adequado considerar uma tensão negativa em vez de uma tensão nula para representar o repouso e para isso, a função sinal se mostrou mais adequada.\nEm julho de 1958, o escritório de Pesquisa Naval dos EUA revelou uma invenção notável. Um IBM 704, um computador de 5 toneladas que ocupava uma sala, foi alimentado com uma série de cartões perfurados. Após 50 tentativas, o computador aprendeu a distinguir os cartões marcados à esquerda dos cartões marcados à direita. Foi uma demonstração do perceptron de Rosenblatt, a primeira máquina capaz de ter uma ideia original. Na época, Rosenblatt era psicólogo pesquisador e engenheiro de projetos no Laboratório Aeronáutico da Cornell em Buffalo, Nova York. “As histórias sobre a criação de máquinas com qualidades humanas têm sido fascinantes em ficção científica. No entanto, estamos prestes a testemunhar o nascimento de tal máquina - uma máquina capaz de perceber, reconhecer e identificar seus arredores sem qualquer treinamento ou controle humano”, escreveu Rosenblatt em 1958. Ele estava certo, mas levou aproximadamente meio século para vermos isso acontecer. Na Figura 12, são mostradas uma imagem do título da publicação de Rosenblatt de 1958 e uma foto de Rosenblatt em 1960 com seu perceptron chamado de Mark I, uma máquina eletromecânica implementava os pesos adaptativos por meio de potenciômetros que eram ajustados por atuadores.\n\n\n\n\n\n\nFigura 12: Publicação de Rosenblatt de 1958 (à esquerda) e foto de Rosenblatt e seu perceptron chamado de Mark I em 1960 (à direita) [Fonte].\n\n\n\nDesde 1960, muita pesquisa foi feita com o objetivo de melhorar o modelo do cérebro humano. Apesar dos inúmeros avanços, ainda estamos longe de termos um sistema que consiga modelar de maneira precisa o cérebro, devido à sua alta complexidade e eficiência. Apesar das redes neurais artificiais serem inspiradas no funcionamento do cérebro, vamos encará-las como sistemas não lineares que podem ser aplicados como soluções eficientes em problemas de regressão e classificação.\nUma sugestão de vídeo sobre o surgimento das redes neurais é o The man who forever changed artificial intelligence.\nQuem quiser se aprofundar em modelos de neurônios e do cérebro humano já que esse assunto está fora do escopo deste curso, sugerimos o livro (Gerstner et al. 2014)."
  },
  {
    "objectID": "t_neuronio.html#footnotes",
    "href": "t_neuronio.html#footnotes",
    "title": "O modelo do neurônio",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nComo no caso do algoritmo LMS no modo mini-batch, inserimos aqui o índice \\(m-1\\) aos vetores que foram calculados com os dados da posição \\(\\ell\\) e pesos \\(\\mathbf{w}(m-1)\\).↩︎"
  },
  {
    "objectID": "t_cnn.html",
    "href": "t_cnn.html",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "A rede neural convolucional (convolutional neural network - CNN) é um tipo de rede neural especializada no processamento e classificação de dados organizados em matrizes. Ela pode ser interpretada como uma rede MLP, modificada para trabalhar com dados bidimensionais. Seu nome deriva de uma operação semelhante à convolução, que é usada no lugar da simples multiplicação de matrizes em pelo menos uma de suas camadas.\nDa mesma forma que a MLP, a CNN atualiza os pesos e biases por meio do algoritmo backpropagation. Ela é composta por camadas convolucionais para a extração de características, que servem como entrada para uma rede MLP totalmente conectada, responsável pela classificação. As camadas convolucionais também possuem estágios com operações de subamostragem, conhecidas como agrupamento (pooling). A subamostragem reduz a resolução do mapa de características e diminui a sensibilidade da saída a translações e outras formas de distorção.\nA seguir, vamos detalhar essa rede.\n\n\nSeja \\(w(n)\\) a resposta impulsiva de um filtro de tempo discreto. A relação entrada-saída desse filtro é dada pela operação de convolução entre a sequência de entrada \\(x(n)\\) e sua resposta impulsiva \\(w(n)\\), ou seja, \\[\ny(n)=x(n)\\ast w(n)=\\sum_{k=-\\infty}^{\\infty}x(k)w(n-k)=\\sum_{k=-\\infty}^{\\infty}w(k)x(n-k)\n\\]\npara qualquer inteiro \\(n\\). A última igualdade vem do fato da convolução ser comutativa. Se o filtro for causal e com resposta impulsiva finita (FIR – finite impulse response) com \\(M\\) coeficientes, \\(\\{w(n)\\}_{n=0}^{M-1}\\), e a sequência de entrada tiver \\(N\\) amostras, \\(\\{x(n)\\}_{n=0}^{N-1}\\), o resultado da convolução se reduz a \\[\ny(n)=\\sum_{k=0}^{M-1}w(k)x(n-k).\n\\]\nNeste caso, a sequência de saída \\(y(n)\\) terá \\(N+M-1\\) amostras.\nNo caso bidimensional, a convolução entre uma imagem \\(\\mathbf{I}(i,j)\\) e um filtro \\(\\mathbf{K}(i,j)\\) é dada por \\[\n\\mathbf{S}(i,j)=\\mathbf{I}(i,j)\\ast \\mathbf{K}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(m,n)\\mathbf{K}(i-m,j-n).\n\\]\nNovamente, devido à propriedade comutativa, podemos escrever \\[\n\\mathbf{S}(i,j)=\\mathbf{K}(i,j)\\ast \\mathbf{I}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(i-m,j-n)\\mathbf{K}(m,n).\n\\]\nEm aprendizado de máquina, é comum considerar a correlação cruzada no lugar da convolução, o que leva a \\[\n\\mathbf{S}(i,j)=\\mathbf{K}(i,j)\\star \\mathbf{I}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(i+m,j+n)\\mathbf{K}(m,n),\n\\]\nem que usamos o símbolo \\(\\star\\) em vez do símbolo \\(\\ast\\) para indicar essa operação. Esse cálculo é semelhante ao da convolução, a menos do fato de que não se considera o rebatimento da entrada (ou do filtro).\nNa Figura 1, é mostrado um exemplo de cálculo da convolução e da correlação cruzada para uma dimensão, considerando duas funções de tempo contínuo \\(f\\) e \\(g\\). Muitas bibliotecas de aprendizado de máquina implementam a correlação cruzada e chamam essa operação de convolução sem rebatimento ou simplesmente de convolução. Diante disso, também vamos chamar essa operação aqui de convolução.\n\n\n\n\n\n\nFigura 1: Diferença entre convolução e correlação cruzada em uma dimensão [Fonte].\n\n\n\nNa Figura 2, é exemplificada a convolução da imagem \\(\\mathbf{I}\\) de dimensões \\((n_1 \\times n_2)=(3\\times 3)\\) com o filtro \\(\\mathbf{K}\\) de dimensões \\((f_1 \\times f_2)=(2\\times 2)\\), que gera a saída \\(\\mathbf{S}\\) de dimensões \\((o_1 \\times o_2)=(2\\times 2)\\). De forma geral, as dimensões de \\(\\mathbf{S}\\) são iguais a \\[\n(o_1 \\times o_2) = (n_1-f_1+1\\;\\times\\; n_2-f_2+1).\n\\]\n\n\n\n\n\n\nFigura 2: Operação de convolução (sem rebatimento) da imagem \\(\\mathbf{I}\\) de dimensões \\((n_1 \\times n_2)=(3\\times 3)\\) com o filtro \\(\\mathbf{K}\\) de dimensões \\((f_1 \\times f_2)=(2\\times 2)\\), que gera a saída \\(\\mathbf{S}\\) de dimensões \\((o_1 \\times o_2)=(2\\times 2)\\).\n\n\n\nA convolução pode ser implementada por meio da multiplicação de matrizes. Para isso, vamos definir a matriz de convolução do filtro denotada por \\(\\mathbf{C}\\), como ilustrado na Figura 3. A convolução de uma entrada \\(4\\times 4\\) com um filtro \\(3\\times 3\\) leva à saída \\(2\\times 2\\). Essa convolução pode ser implementada pela multiplicação da matriz de convolução \\(\\mathbf{C}\\) de dimensões \\(4\\times 16\\) pelo vetor \\(16\\times 1\\), o que leva ao vetor de saída \\(4\\times 1\\). Na sequência, o vetor resultante \\(4\\times 1\\) deve então ser transformado de volta para uma saída \\(2\\times 2\\).\n\n\n\n\n\n\nFigura 3: Exemplo da convolução realizada por meio da multiplicação de matrizes: entrada \\(4\\times 4\\), filtro \\(3\\times 3\\) e saída \\(2\\times 2\\). [Fonte].\n\n\n\nEm cada estágio de convolução de uma camada convolucional, é comum considerar \\(k\\) filtros, com \\(k\\geq 1\\). Para cada filtro, calcula-se a convolução entre a entrada e o filtro e a esse resultado soma-se o bias. Os elementos da matriz resultante entram em uma função de ativação não linear e uma das matrizes da saída é obtida. A saída terá uma dimensão a mais, igual ao número de filtros da camada (\\(k\\)). É comum considerar filtros quadrados (\\(f_1=f_2=f\\)) e de mesmo tamanho para uma dada camada. A consequência é que as dimensões da saída de uma camada convolucional serão dadas por\n\\[\n(n_1-f+1)\\times(n_2-f+1)\\times k.\n\\]\nNa Figura 4-(a), esse processo é ilustrado considerando uma entrada de dimensões \\(4 \\times 4\\) e uma camada convolucional com quatro filtros de dimensões \\(3\\times 3\\), o que gera a saída de dimensões \\(2\\times 2\\times 4\\). Na Figura 4-(b), a soma do bias e a aplicação da função linear são ilustradas para uma das matrizes resultantes da convolução. Os filtros das camadas convolucionais são matrizes de pesos. Assim, considerando os quatro filtros de dimensões \\(3\\times 3\\) da Figura 4-(a), há \\(4 \\times 3 \\times  3  = 36\\) pesos e \\(4\\) biases (um bias por filtro).\n\n\n\n\n\n\nFigura 4: (a) Entrada e saída de um estágio de convolução da camada convolucional: entrada de dimensões \\(4 \\times 4\\), quatro filtros de dimensões \\(3\\times 3\\) e saída de dimensões \\(2\\times 2\\times 4\\); (b) Soma do bias e aplicação da função de ativação à uma das matrizes resultantes da convolução.\n\n\n\nCaso a entrada de um estágio de convolução da camada convolucional seja um tensor, como a saída de dimensões \\(2\\times 2 \\times 4\\) da Figura 4, os filtros dessa camada devem ter sua terceira dimensão igual à terceira dimensão da entrada, denotada por \\(n_3\\). Neste caso, a convolução deve levar em conta as \\(n_3\\) matrizes que compõem o tensor de entrada e as \\(n_3\\) matrizes que compõem cada filtro, como ilustrado na Figura 5 para \\(n_3=3\\). No exemplo dessa figura, note que a entrada tem dimensões \\((4\\times 4 \\times 3)\\), há apenas um filtro de dimensões \\((2\\times 2\\times 3)\\), o que leva à saída da convolução com dimensões \\((3\\times 3 \\times 1)\\). Para uma entrada de dimensões \\(n_1\\times n_2\\times n_3\\) e \\(k\\) filtros de dimensões \\(f\\times f\\times n_3\\), a saída terá dimensões \\((n_1-f+1)\\times (n_2-f+1) \\times k\\). Na Figura 6, é ilustrado um estágio de convolução com entrada de dimensões \\((4 \\times 4 \\times 4)\\) e seis filtros de dimensões \\((3\\times 3 \\times 4)\\), que levam à saída de dimensões \\((2\\times 2\\times 6)\\). Há \\(6\\times 3 \\times 3 \\times 4 =216\\) pesos e \\(6\\) biases nesta camada.\n\n\n\n\n\n\nFigura 5: Exemplo de convolução considerando um tensor de entrada com \\(n_3=3\\).\n\n\n\n\n\n\n\n\n\nFigura 6: Entrada e saída de um estágio de convolução de uma camada convolucional: entrada de dimensões \\(4 \\times 4 \\times 4\\), seis filtros de dimensões \\(3\\times 3 \\times 4\\) e saída de dimensões \\(2\\times 2\\times 6\\).\n\n\n\n\n\n\nAo se calcular a convolução da entrada \\(\\mathbf{I}\\) com o filtro \\(\\mathbf{K}\\), a matriz de saída \\(\\mathbf{S}\\) fica com dimensões menores que as da entrada. Se houver uma cascata de filtros, as dimensões da matriz de saída de cada filtro vão diminuindo ao longo da cascata. Isso faz com a matriz de saída se torne quase sem sentido depois de várias convoluções devido às suas dimensões muito reduzidas. Além disso, pode-se perder informações valiosas ao se descartar completamente as bordas da entrada.\nPara que a matriz \\(\\mathbf{S}\\) fique com as mesmas dimensões da entrada \\(\\mathbf{I}\\), adicionam-se bordas preenchidas com zeros à matriz \\(\\mathbf{I}\\). Essa prática é conhecida como zero padding. Para exemplificar, considere a matriz de entrada \\(\\mathbf{I}\\) com dimensões \\((n_1 \\times n_2)=(9\\times 9)\\) indicada no quadrado vermelho da Figura 7. Preenchendo a matriz \\(\\mathbf{I}\\) com \\(p\\) bordas de zeros, suas dimensões se tornam iguais a\n\\[\n(n_1+2p\\times n_2+2p).\n\\]\nNo exemplo da Figura 7, \\(p=2\\) e as dimensões de \\(\\mathbf{I}\\) ficam \\((n_1+2p\\times n_2+2p)=(13\\times 13)\\). As dimensões da matriz \\(\\mathbf{S}\\), por sua vez, ficam iguais a\n\\[\n(o_1 \\times o_2) = (n_1+2p-f_1+1 \\;\\times\\; n_2+2p-f_2+1).\n\\]\n\n\n\n\n\n\nFigura 7: Exemplo de zero-padding: a matriz \\(\\mathbf{I}\\) está indicada no quadrado vermelho e tem dimensões \\((n_1 \\times n_2)=(9\\times 9)\\) e neste exemplo \\(p=2\\). Fonte: adaptado de https://salfade.com/tutorials/layers-of-a-convolutional-neural-network-part-1.\n\n\n\nPara que as dimensões da saída sejam as mesmas da entrada, deve-se considerar\n\\[\np=\\frac{f_1-1}{2}=\\frac{f_2-1}{2}.\n\\]\nDessa forma, conclui-se que para manter as dimensões da saída iguais às da entrada, o filtro deve ser quadrado, ou seja \\(f_1=f_2=f\\), o que leva a \\(p=({f-1})/{2}\\). Note que para que \\(p\\) seja inteiro, \\(f\\) deve ser ímpar. Se \\(f\\) for par, uma possibilidade é adicionar \\(\\lceil (f-1)/2 \\rceil\\) linhas de zeros no topo e \\(\\lfloor (f-1)/2 \\rfloor\\) linhas de zeros na parte inferior da matriz \\(\\mathbf{I}\\). Devemos fazer o mesmo para as colunas à esquerda e à direita. Geralmente, os filtros usados nas CNNs têm dimensões ímpares, o que facilita o zero padding. Apesar da entrada na Figura 7 ser quadrada, não existe restrições para as dimensões dessa matriz, ou seja, podemos ter \\(n_1\\neq n_2\\).\nNo cálculo da convolução, começamos com a janela do filtro no canto superior esquerdo da matriz de entrada e deslocamos essa janela para baixo e para a direita. No exemplo da Figura 2, deslocamos uma linha ou coluna por vez. No entanto, podemos deslocar o filtro sobre a matriz de entrada mais de uma linha ou coluna por vez. Isso é vantajoso em termos computacionais e reduz a resolução, o que é desejável em alguns casos. Esse método é conhecido como stride. A quantidade de linhas ou colunas percorridas de uma só vez pelo filtro é denotada por \\(s\\). Ao considerar zero padding \\(p\\) e stride \\(s\\), as dimensões da matriz \\(\\mathbf{S}\\) ficam iguais a\n\\[\n(o_1 \\times o_2) = \\left(\\left\\lfloor \\frac{n_1+2p-f_1}{s}+1\\right\\rfloor \\;\\times\\; \\left\\lfloor \\frac{n_2+2p-f_2}{s}+1\\right\\rfloor \\right).\n\\]\nNa Figura 8, é mostrado um exemplo de convolução com stride e sem zero padding (\\(p=0\\)), considerando \\(s=1\\) e \\(s=2\\). Neste exemplo, a matriz de entrada \\(\\mathbf{I}\\) tem dimensões \\((n_1 \\times n_2)=(5\\times 5)\\) e o filtro \\(\\mathbf{K}\\) tem dimensões \\((f_1 \\times f_2)=(3\\times 3)\\). O resultado com \\(s=1\\) é uma matriz de saída com dimensões \\((o_1 \\times o_2)=(3\\times 3)\\). Já com \\(s=2\\), a matriz de saída passa a ter dimensões \\((o_1 \\times o_2)=(2\\times 2)\\).\n\n\n\n\n\n\nFigura 8: Exemplo de convolução com stride, considerando \\(s=1\\) e \\(s=2\\); entrada \\(\\mathbf{I}\\) com dimensões \\((n_1 \\times n_2)=(5\\times 5)\\) e filtro \\(\\mathbf{K}\\) com dimensões \\((f \\times f)=(3\\times 3)\\). Fonte: adaptado de https://zephyrnet.com/basics-of-cnn-in-deep-learning/.\n\n\n\n\n\n\nAs camadas convolucionais são os blocos básicos de uma CNN. Uma camada convolucional desloca um filtro sobre a imagem e extrai características, resultando em um mapa de características. Esse mapa de características, por sua vez, pode ser alimentado para a próxima camada convolucional para extrair características em um nível superior e assim sucessivamente. Empilhar várias camadas convolucionais permite que uma CNN reconheça estruturas e objetos cada vez mais complexos em uma imagem.\nUm problema que surge é que o mapa de características produzido pelo filtro é dependente da localização. Isso significa que durante o treinamento, a CNN aprende a associar a presença de uma determinada característica a um local específico na imagem de entrada, o que pode prejudicar seu desempenho. Em vez disso, deseja-se que o mapa de características e a rede sejam invariantes à translação para que a localização da característica deixe de importar.\nUma das técnicas usadas para tornar as CNNs invariantes à translação é o agrupamento (pooling), também chamado de subamostragem. Existem diferentes abordagens para o agrupamento, sendo o agrupamento máximo (max-pooling) e o agrupamento médio (average pooling) os mais utilizados. Além de tornar as CNNs invariantes à translação, o pooling reduz as dimensões das matrizes. Na Figura 9, é mostrado um exemplo numérico de pooling, considerando (a) max-pooling e (b) average pooling. Uma imagem de dimensões \\(6\\times 6\\) foi transformada em uma imagem \\(3\\times 3\\). No caso do max-pooling (Figura 9-(a)), os quatro pixels da região azul foram substituídos pelo valor máximo, \\(9\\) no caso. O mesmo aconteceu com os pixels da região laranja, que foram substituídos pelo valor \\(7\\). No caso do average pooling (Figura 9-(b)), a média dos pixels da região azul dá 4,5, que arredondando leva ao valor \\(5\\). Já a média dos pixels da região laranja dá \\(4\\).\n\n\n\n\n\n\nFigura 9: Exemplo numérico de (a) max-pooling e (b) average pooling. [Fonte].\n\n\n\nSe a rede neural dependesse apenas do mapa de características original, sua capacidade de detectar uma característica dependeria da localização no mapa. Por exemplo, se o valor \\(9\\) fosse encontrado apenas no quadrante superior esquerdo, a rede aprenderia a associar a característica relativa ao número 9 com o quadrante superior esquerdo. Ao aplicar o agrupamento, essa característica é extraída em um mapa menor e mais geral que indica apenas se uma característica está presente naquele quadrante específico ou não. A cada camada adicional, o mapa se torna menor, preservando apenas as informações importantes sobre a presença das características de interesse. À medida que o mapa se torna menor, torna-se cada vez mais independente da localização da característica. O pooling é especialmente útil para classificação de imagem em que se deseja detectar a presença de um determinado objeto em uma imagem, mas não se importa onde exatamente ele está localizado.\n\n\n\nEm algumas aplicações, deseja-se fazer transformações que vão na direção oposta de uma convolução tradicional, ou seja, deseja-se realizar uma sobreamostragem. Este é o caso, por exemplo, de redes usadas para geração de imagens de alta resolução e redes usadas para extração de características por meio de autoencoders.\nTradicionalmente, a sobreamostragem é obtida aplicando-se esquemas de interpolação ou criando regras manualmente. No entanto, as redes neurais podem aprender a transformação adequada automaticamente por meio da convolução transposta. Essa operação também é chamada na literatura de desconvolução. No entanto, esse termo não é adequado uma vez que desconvolução em processamento de sinais é a operação inversa da convolução, cujo objetivo é desfazer a convolução, ou seja obter a imagem original a partir de uma imagem que é fruto da convolução da original com um filtro.\nÉ possível implementar uma convolução transposta utilizando a convolução. Para exemplificar, é mostrado na Figura 10 uma convolução transposta considerando um filtro \\(3\\times 3\\) sobre uma entrada \\(2\\times 2\\) modificada com zero padding de \\(p=2\\) e stride de passo unitário, \\(s=1\\). A saída sobreamostrada tem dimensões \\(4 \\times 4\\). Neste caso, as dimensões da saída são maiores que as da entrada devido ao zero padding. Cabe observar que a convolução transposta gera o mesmo resultado da convolução tradicional ao considerarmos a entrada aumentada com zeros. Pode-se mapear a mesma imagem de dimensões \\(2\\times 2\\) em uma imagem de dimensões ainda maiores, aplicando um zero padding mais sofisticado. Na Figura 11, a convolução transposta é aplicada sobre a mesma entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\), além de um zero inserido entre os pixels. Neste caso, a saída fica com dimensões \\(5 \\times 5\\).\n\n\n\n\n\n\nFigura 10: Exemplo da convolução transposta: entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\), filtro \\(3\\times 3\\) e saída \\(4\\times 4.\\) [Fonte].\n\n\n\n\n\n\n\n\n\nFigura 11: Exemplo da convolução transposta: entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\) e com preenchimento de um zero entre os pixels, filtro \\(3\\times 3\\) e saída \\(5\\times 5\\). [Fonte].\n\n\n\nOutra forma de implementar a convolução transposta é seguindo os passos exemplificados a seguir [Fonte]:\n\nConsidere um mapa de características \\(2\\times 2\\) que precisa ser sobreamostrado para um mapa de características \\(3\\times 3\\).\n\n\n\n\n\n\n\nConsidere o filtro \\(2\\times 2\\)\n\n\n\n\n\n\n\nMultiplique o elemento superior esquerdo do mapa de características de entrada por cada elemento do filtro:\n\n\n\n\n\n\n\nRepita o Item 3 para cada elemento do mapa de características de entrada:\n\n\n\n\n\n\n\nAlguns dos elementos dos mapas de características resultantes da sobreamostragem ficam sobrepostos. Para resolver esse problema, os elementos das posições sobrepostas devem ser adicionados. Assim, obtém-se a saída final.\n\n\n\n\n\n\nPara finalizar, considere a implementação da convolução por meio da multiplicação de matrizes, exemplificada na Figura 3. Multiplicando a matriz do filtro transposta de dimensões \\(16\\times 4\\) pelo vetor de entrada \\(4\\times 1\\), que inclusive pode ser a saída vetorizada de uma camada convolucional anterior, obtém-se um vetor de dimensões \\(16\\times 1\\). Comparando com a entrada que continha apenas 4 pixels, esse procedimento levou a uma saída aumentada com dimensões \\(4\\times 4\\), como mostrado na Figura 12. Não podemos esquecer que os pesos devem ser atualizados para que a sobreamostragem se adéque à aplicação. O fato de usarmos a matriz do filtro transposta \\(\\mathbf{C}^{\\rm T}\\) justifica o nome dessa operação.\n\n\n\n\n\n\nFigura 12: Exemplo da convolução transposta realizada por meio da multiplicação de matrizes: entrada \\(2\\times 2\\) transformada no vetor \\(4\\times 1\\) e filtro \\(3\\times 3\\) com elementos reorganizados na matriz transposta \\(\\mathbf{C}^{\\rm T}\\), o que leva à saída aumentada (sobreamostrada) \\(4\\times 4\\). [Fonte].\n\n\n\n\n\n\nDepois de aprendermos como é feita a convolução nas camadas convolucionais em conjunto com os métodos de zero padding (preenchimento com zeros), stride (passo largo) e pooling (subamostragem), podemos agora analisar a estrutura completa de uma CNN típica, como ilustrado na Figura 13. Uma CNN típica é composta de camadas convolucionais para extração de características, seguida de uma MLP totalmente conectada para classificação. Suponha que se deseja identificar se a imagem de entrada da rede corresponde ao Pato Donald, ao Pateta (Goofy) ou ao Piu-Piu (Tweety). Nas camadas convolucionais é comum alternar estágios de convolução e subamostragem (pooling). Como vimos, a subamostragem é importante para que o mapa de características se torne invariante à translação. O mapa de características subamostrado da última camada convolucional deve ser vetorizado. Em seguida, esse vetor entra na MLP. No exemplo da Figura 13, a MLP é composta de uma camada oculta e uma camada de saída. No entanto, em muitos problemas é comum considerar uma MLP profunda, com várias camadas ocultas. Para classificação multiclasse, considera-se a entropia cruzada como função custo e a função de ativação softmax, que fornece a probabilidade da classificação. No caso da Figura 13, há três neurônios de saída e com os valores obtidos pela softmax que estão indicados na figura, decide-se que a imagem é do Piu-Piu.\n\n\n\n\n\n\nFigura 13: Estrutura de uma CNN. [Fonte].\n\n\n\nOs pesos dos filtros das camadas convolucionais e dos neurônios da MLP são atualizados com o algoritmo backpropagation\n\n\n\nEsse exemplo foi explorado em https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/. Nesse exemplo, uma rede MLP e uma CNN foram treinadas para classificação do conjunto de dados MNIST, disponível em http://yann.lecun.com/exdb/mnist/. Trata-se de um banco de dados de dígitos manuscritos, que possui um conjunto de treinamento de 60000 exemplos e um conjunto de teste de 10000 exemplos.\nAs redes MLP e CNN treinadas têm o mesmo número de parâmetros. Na Figura 14, são mostrados exemplos de treinamento do banco de dados MNIST original. Antes do treinamento, os dados foram normalizados para que a inicialização da rede corresponda à distribuição de dados. Além disso, as cinco etapas foram usadas no treinamento:\n\nAlimentação dos dados no modelo;\nCálculo da função custo;\nLimpeza do cache de gradientes acumulados;\nCálculo dos gradientes;\nExecução do método de otimização.\n\n\n\n\n\n\n\nFigura 14: Exemplos de treinamento do banco de dados MNIST original. [Fonte].\n\n\n\nPrimeiramente, ambas as redes foram treinadas com os dados MNIST normalizados. A MLP atingiu uma acurácia de 87% enquanto a acurácia da CNN ficou igual a 95%. Dado o mesmo número de parâmetros, a CNN conseguiu treinar muito mais filtros. Na rede MLP, os neurônios tentam obter algumas dependências entre pixels mais distantes com pixels próximos, o que acaba sendo desperdiçado. Em vez disso, na CNN, os parâmetros se concentram no relacionamento entre os pixels vizinhos.\nEm seguida, foi feita uma permutação aleatória de todos os pixels em todas as imagens do conjunto de dados MNIST. Isso transforma a Figura 14 na Figura 15. Ambas as redes foram treinadas com esse conjunto de dados modificado.\n\n\n\n\n\n\nFigura 15: Exemplos de treinamento do banco de dados MNIST permutado. [Fonte].\n\n\n\nO desempenho da rede MLP quase se manteve inalterado (85%), mas a acurácia da CNN caiu para 83%. Isso ocorre porque, após uma permutação aleatória, as imagens não possuem mais as três propriedades de localidade, estacionariedade e composicionalidade, que são exploradas por uma CNN."
  },
  {
    "objectID": "t_cnn.html#convolução-e-camadas-convolucionais",
    "href": "t_cnn.html#convolução-e-camadas-convolucionais",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "Seja \\(w(n)\\) a resposta impulsiva de um filtro de tempo discreto. A relação entrada-saída desse filtro é dada pela operação de convolução entre a sequência de entrada \\(x(n)\\) e sua resposta impulsiva \\(w(n)\\), ou seja, \\[\ny(n)=x(n)\\ast w(n)=\\sum_{k=-\\infty}^{\\infty}x(k)w(n-k)=\\sum_{k=-\\infty}^{\\infty}w(k)x(n-k)\n\\]\npara qualquer inteiro \\(n\\). A última igualdade vem do fato da convolução ser comutativa. Se o filtro for causal e com resposta impulsiva finita (FIR – finite impulse response) com \\(M\\) coeficientes, \\(\\{w(n)\\}_{n=0}^{M-1}\\), e a sequência de entrada tiver \\(N\\) amostras, \\(\\{x(n)\\}_{n=0}^{N-1}\\), o resultado da convolução se reduz a \\[\ny(n)=\\sum_{k=0}^{M-1}w(k)x(n-k).\n\\]\nNeste caso, a sequência de saída \\(y(n)\\) terá \\(N+M-1\\) amostras.\nNo caso bidimensional, a convolução entre uma imagem \\(\\mathbf{I}(i,j)\\) e um filtro \\(\\mathbf{K}(i,j)\\) é dada por \\[\n\\mathbf{S}(i,j)=\\mathbf{I}(i,j)\\ast \\mathbf{K}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(m,n)\\mathbf{K}(i-m,j-n).\n\\]\nNovamente, devido à propriedade comutativa, podemos escrever \\[\n\\mathbf{S}(i,j)=\\mathbf{K}(i,j)\\ast \\mathbf{I}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(i-m,j-n)\\mathbf{K}(m,n).\n\\]\nEm aprendizado de máquina, é comum considerar a correlação cruzada no lugar da convolução, o que leva a \\[\n\\mathbf{S}(i,j)=\\mathbf{K}(i,j)\\star \\mathbf{I}(i,j)=\\sum_{m}\\sum_{n}\\mathbf{I}(i+m,j+n)\\mathbf{K}(m,n),\n\\]\nem que usamos o símbolo \\(\\star\\) em vez do símbolo \\(\\ast\\) para indicar essa operação. Esse cálculo é semelhante ao da convolução, a menos do fato de que não se considera o rebatimento da entrada (ou do filtro).\nNa Figura 1, é mostrado um exemplo de cálculo da convolução e da correlação cruzada para uma dimensão, considerando duas funções de tempo contínuo \\(f\\) e \\(g\\). Muitas bibliotecas de aprendizado de máquina implementam a correlação cruzada e chamam essa operação de convolução sem rebatimento ou simplesmente de convolução. Diante disso, também vamos chamar essa operação aqui de convolução.\n\n\n\n\n\n\nFigura 1: Diferença entre convolução e correlação cruzada em uma dimensão [Fonte].\n\n\n\nNa Figura 2, é exemplificada a convolução da imagem \\(\\mathbf{I}\\) de dimensões \\((n_1 \\times n_2)=(3\\times 3)\\) com o filtro \\(\\mathbf{K}\\) de dimensões \\((f_1 \\times f_2)=(2\\times 2)\\), que gera a saída \\(\\mathbf{S}\\) de dimensões \\((o_1 \\times o_2)=(2\\times 2)\\). De forma geral, as dimensões de \\(\\mathbf{S}\\) são iguais a \\[\n(o_1 \\times o_2) = (n_1-f_1+1\\;\\times\\; n_2-f_2+1).\n\\]\n\n\n\n\n\n\nFigura 2: Operação de convolução (sem rebatimento) da imagem \\(\\mathbf{I}\\) de dimensões \\((n_1 \\times n_2)=(3\\times 3)\\) com o filtro \\(\\mathbf{K}\\) de dimensões \\((f_1 \\times f_2)=(2\\times 2)\\), que gera a saída \\(\\mathbf{S}\\) de dimensões \\((o_1 \\times o_2)=(2\\times 2)\\).\n\n\n\nA convolução pode ser implementada por meio da multiplicação de matrizes. Para isso, vamos definir a matriz de convolução do filtro denotada por \\(\\mathbf{C}\\), como ilustrado na Figura 3. A convolução de uma entrada \\(4\\times 4\\) com um filtro \\(3\\times 3\\) leva à saída \\(2\\times 2\\). Essa convolução pode ser implementada pela multiplicação da matriz de convolução \\(\\mathbf{C}\\) de dimensões \\(4\\times 16\\) pelo vetor \\(16\\times 1\\), o que leva ao vetor de saída \\(4\\times 1\\). Na sequência, o vetor resultante \\(4\\times 1\\) deve então ser transformado de volta para uma saída \\(2\\times 2\\).\n\n\n\n\n\n\nFigura 3: Exemplo da convolução realizada por meio da multiplicação de matrizes: entrada \\(4\\times 4\\), filtro \\(3\\times 3\\) e saída \\(2\\times 2\\). [Fonte].\n\n\n\nEm cada estágio de convolução de uma camada convolucional, é comum considerar \\(k\\) filtros, com \\(k\\geq 1\\). Para cada filtro, calcula-se a convolução entre a entrada e o filtro e a esse resultado soma-se o bias. Os elementos da matriz resultante entram em uma função de ativação não linear e uma das matrizes da saída é obtida. A saída terá uma dimensão a mais, igual ao número de filtros da camada (\\(k\\)). É comum considerar filtros quadrados (\\(f_1=f_2=f\\)) e de mesmo tamanho para uma dada camada. A consequência é que as dimensões da saída de uma camada convolucional serão dadas por\n\\[\n(n_1-f+1)\\times(n_2-f+1)\\times k.\n\\]\nNa Figura 4-(a), esse processo é ilustrado considerando uma entrada de dimensões \\(4 \\times 4\\) e uma camada convolucional com quatro filtros de dimensões \\(3\\times 3\\), o que gera a saída de dimensões \\(2\\times 2\\times 4\\). Na Figura 4-(b), a soma do bias e a aplicação da função linear são ilustradas para uma das matrizes resultantes da convolução. Os filtros das camadas convolucionais são matrizes de pesos. Assim, considerando os quatro filtros de dimensões \\(3\\times 3\\) da Figura 4-(a), há \\(4 \\times 3 \\times  3  = 36\\) pesos e \\(4\\) biases (um bias por filtro).\n\n\n\n\n\n\nFigura 4: (a) Entrada e saída de um estágio de convolução da camada convolucional: entrada de dimensões \\(4 \\times 4\\), quatro filtros de dimensões \\(3\\times 3\\) e saída de dimensões \\(2\\times 2\\times 4\\); (b) Soma do bias e aplicação da função de ativação à uma das matrizes resultantes da convolução.\n\n\n\nCaso a entrada de um estágio de convolução da camada convolucional seja um tensor, como a saída de dimensões \\(2\\times 2 \\times 4\\) da Figura 4, os filtros dessa camada devem ter sua terceira dimensão igual à terceira dimensão da entrada, denotada por \\(n_3\\). Neste caso, a convolução deve levar em conta as \\(n_3\\) matrizes que compõem o tensor de entrada e as \\(n_3\\) matrizes que compõem cada filtro, como ilustrado na Figura 5 para \\(n_3=3\\). No exemplo dessa figura, note que a entrada tem dimensões \\((4\\times 4 \\times 3)\\), há apenas um filtro de dimensões \\((2\\times 2\\times 3)\\), o que leva à saída da convolução com dimensões \\((3\\times 3 \\times 1)\\). Para uma entrada de dimensões \\(n_1\\times n_2\\times n_3\\) e \\(k\\) filtros de dimensões \\(f\\times f\\times n_3\\), a saída terá dimensões \\((n_1-f+1)\\times (n_2-f+1) \\times k\\). Na Figura 6, é ilustrado um estágio de convolução com entrada de dimensões \\((4 \\times 4 \\times 4)\\) e seis filtros de dimensões \\((3\\times 3 \\times 4)\\), que levam à saída de dimensões \\((2\\times 2\\times 6)\\). Há \\(6\\times 3 \\times 3 \\times 4 =216\\) pesos e \\(6\\) biases nesta camada.\n\n\n\n\n\n\nFigura 5: Exemplo de convolução considerando um tensor de entrada com \\(n_3=3\\).\n\n\n\n\n\n\n\n\n\nFigura 6: Entrada e saída de um estágio de convolução de uma camada convolucional: entrada de dimensões \\(4 \\times 4 \\times 4\\), seis filtros de dimensões \\(3\\times 3 \\times 4\\) e saída de dimensões \\(2\\times 2\\times 6\\)."
  },
  {
    "objectID": "t_cnn.html#zero-padding-e-stride",
    "href": "t_cnn.html#zero-padding-e-stride",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "Ao se calcular a convolução da entrada \\(\\mathbf{I}\\) com o filtro \\(\\mathbf{K}\\), a matriz de saída \\(\\mathbf{S}\\) fica com dimensões menores que as da entrada. Se houver uma cascata de filtros, as dimensões da matriz de saída de cada filtro vão diminuindo ao longo da cascata. Isso faz com a matriz de saída se torne quase sem sentido depois de várias convoluções devido às suas dimensões muito reduzidas. Além disso, pode-se perder informações valiosas ao se descartar completamente as bordas da entrada.\nPara que a matriz \\(\\mathbf{S}\\) fique com as mesmas dimensões da entrada \\(\\mathbf{I}\\), adicionam-se bordas preenchidas com zeros à matriz \\(\\mathbf{I}\\). Essa prática é conhecida como zero padding. Para exemplificar, considere a matriz de entrada \\(\\mathbf{I}\\) com dimensões \\((n_1 \\times n_2)=(9\\times 9)\\) indicada no quadrado vermelho da Figura 7. Preenchendo a matriz \\(\\mathbf{I}\\) com \\(p\\) bordas de zeros, suas dimensões se tornam iguais a\n\\[\n(n_1+2p\\times n_2+2p).\n\\]\nNo exemplo da Figura 7, \\(p=2\\) e as dimensões de \\(\\mathbf{I}\\) ficam \\((n_1+2p\\times n_2+2p)=(13\\times 13)\\). As dimensões da matriz \\(\\mathbf{S}\\), por sua vez, ficam iguais a\n\\[\n(o_1 \\times o_2) = (n_1+2p-f_1+1 \\;\\times\\; n_2+2p-f_2+1).\n\\]\n\n\n\n\n\n\nFigura 7: Exemplo de zero-padding: a matriz \\(\\mathbf{I}\\) está indicada no quadrado vermelho e tem dimensões \\((n_1 \\times n_2)=(9\\times 9)\\) e neste exemplo \\(p=2\\). Fonte: adaptado de https://salfade.com/tutorials/layers-of-a-convolutional-neural-network-part-1.\n\n\n\nPara que as dimensões da saída sejam as mesmas da entrada, deve-se considerar\n\\[\np=\\frac{f_1-1}{2}=\\frac{f_2-1}{2}.\n\\]\nDessa forma, conclui-se que para manter as dimensões da saída iguais às da entrada, o filtro deve ser quadrado, ou seja \\(f_1=f_2=f\\), o que leva a \\(p=({f-1})/{2}\\). Note que para que \\(p\\) seja inteiro, \\(f\\) deve ser ímpar. Se \\(f\\) for par, uma possibilidade é adicionar \\(\\lceil (f-1)/2 \\rceil\\) linhas de zeros no topo e \\(\\lfloor (f-1)/2 \\rfloor\\) linhas de zeros na parte inferior da matriz \\(\\mathbf{I}\\). Devemos fazer o mesmo para as colunas à esquerda e à direita. Geralmente, os filtros usados nas CNNs têm dimensões ímpares, o que facilita o zero padding. Apesar da entrada na Figura 7 ser quadrada, não existe restrições para as dimensões dessa matriz, ou seja, podemos ter \\(n_1\\neq n_2\\).\nNo cálculo da convolução, começamos com a janela do filtro no canto superior esquerdo da matriz de entrada e deslocamos essa janela para baixo e para a direita. No exemplo da Figura 2, deslocamos uma linha ou coluna por vez. No entanto, podemos deslocar o filtro sobre a matriz de entrada mais de uma linha ou coluna por vez. Isso é vantajoso em termos computacionais e reduz a resolução, o que é desejável em alguns casos. Esse método é conhecido como stride. A quantidade de linhas ou colunas percorridas de uma só vez pelo filtro é denotada por \\(s\\). Ao considerar zero padding \\(p\\) e stride \\(s\\), as dimensões da matriz \\(\\mathbf{S}\\) ficam iguais a\n\\[\n(o_1 \\times o_2) = \\left(\\left\\lfloor \\frac{n_1+2p-f_1}{s}+1\\right\\rfloor \\;\\times\\; \\left\\lfloor \\frac{n_2+2p-f_2}{s}+1\\right\\rfloor \\right).\n\\]\nNa Figura 8, é mostrado um exemplo de convolução com stride e sem zero padding (\\(p=0\\)), considerando \\(s=1\\) e \\(s=2\\). Neste exemplo, a matriz de entrada \\(\\mathbf{I}\\) tem dimensões \\((n_1 \\times n_2)=(5\\times 5)\\) e o filtro \\(\\mathbf{K}\\) tem dimensões \\((f_1 \\times f_2)=(3\\times 3)\\). O resultado com \\(s=1\\) é uma matriz de saída com dimensões \\((o_1 \\times o_2)=(3\\times 3)\\). Já com \\(s=2\\), a matriz de saída passa a ter dimensões \\((o_1 \\times o_2)=(2\\times 2)\\).\n\n\n\n\n\n\nFigura 8: Exemplo de convolução com stride, considerando \\(s=1\\) e \\(s=2\\); entrada \\(\\mathbf{I}\\) com dimensões \\((n_1 \\times n_2)=(5\\times 5)\\) e filtro \\(\\mathbf{K}\\) com dimensões \\((f \\times f)=(3\\times 3)\\). Fonte: adaptado de https://zephyrnet.com/basics-of-cnn-in-deep-learning/."
  },
  {
    "objectID": "t_cnn.html#pooling",
    "href": "t_cnn.html#pooling",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "As camadas convolucionais são os blocos básicos de uma CNN. Uma camada convolucional desloca um filtro sobre a imagem e extrai características, resultando em um mapa de características. Esse mapa de características, por sua vez, pode ser alimentado para a próxima camada convolucional para extrair características em um nível superior e assim sucessivamente. Empilhar várias camadas convolucionais permite que uma CNN reconheça estruturas e objetos cada vez mais complexos em uma imagem.\nUm problema que surge é que o mapa de características produzido pelo filtro é dependente da localização. Isso significa que durante o treinamento, a CNN aprende a associar a presença de uma determinada característica a um local específico na imagem de entrada, o que pode prejudicar seu desempenho. Em vez disso, deseja-se que o mapa de características e a rede sejam invariantes à translação para que a localização da característica deixe de importar.\nUma das técnicas usadas para tornar as CNNs invariantes à translação é o agrupamento (pooling), também chamado de subamostragem. Existem diferentes abordagens para o agrupamento, sendo o agrupamento máximo (max-pooling) e o agrupamento médio (average pooling) os mais utilizados. Além de tornar as CNNs invariantes à translação, o pooling reduz as dimensões das matrizes. Na Figura 9, é mostrado um exemplo numérico de pooling, considerando (a) max-pooling e (b) average pooling. Uma imagem de dimensões \\(6\\times 6\\) foi transformada em uma imagem \\(3\\times 3\\). No caso do max-pooling (Figura 9-(a)), os quatro pixels da região azul foram substituídos pelo valor máximo, \\(9\\) no caso. O mesmo aconteceu com os pixels da região laranja, que foram substituídos pelo valor \\(7\\). No caso do average pooling (Figura 9-(b)), a média dos pixels da região azul dá 4,5, que arredondando leva ao valor \\(5\\). Já a média dos pixels da região laranja dá \\(4\\).\n\n\n\n\n\n\nFigura 9: Exemplo numérico de (a) max-pooling e (b) average pooling. [Fonte].\n\n\n\nSe a rede neural dependesse apenas do mapa de características original, sua capacidade de detectar uma característica dependeria da localização no mapa. Por exemplo, se o valor \\(9\\) fosse encontrado apenas no quadrante superior esquerdo, a rede aprenderia a associar a característica relativa ao número 9 com o quadrante superior esquerdo. Ao aplicar o agrupamento, essa característica é extraída em um mapa menor e mais geral que indica apenas se uma característica está presente naquele quadrante específico ou não. A cada camada adicional, o mapa se torna menor, preservando apenas as informações importantes sobre a presença das características de interesse. À medida que o mapa se torna menor, torna-se cada vez mais independente da localização da característica. O pooling é especialmente útil para classificação de imagem em que se deseja detectar a presença de um determinado objeto em uma imagem, mas não se importa onde exatamente ele está localizado."
  },
  {
    "objectID": "t_cnn.html#convolução-transposta",
    "href": "t_cnn.html#convolução-transposta",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "Em algumas aplicações, deseja-se fazer transformações que vão na direção oposta de uma convolução tradicional, ou seja, deseja-se realizar uma sobreamostragem. Este é o caso, por exemplo, de redes usadas para geração de imagens de alta resolução e redes usadas para extração de características por meio de autoencoders.\nTradicionalmente, a sobreamostragem é obtida aplicando-se esquemas de interpolação ou criando regras manualmente. No entanto, as redes neurais podem aprender a transformação adequada automaticamente por meio da convolução transposta. Essa operação também é chamada na literatura de desconvolução. No entanto, esse termo não é adequado uma vez que desconvolução em processamento de sinais é a operação inversa da convolução, cujo objetivo é desfazer a convolução, ou seja obter a imagem original a partir de uma imagem que é fruto da convolução da original com um filtro.\nÉ possível implementar uma convolução transposta utilizando a convolução. Para exemplificar, é mostrado na Figura 10 uma convolução transposta considerando um filtro \\(3\\times 3\\) sobre uma entrada \\(2\\times 2\\) modificada com zero padding de \\(p=2\\) e stride de passo unitário, \\(s=1\\). A saída sobreamostrada tem dimensões \\(4 \\times 4\\). Neste caso, as dimensões da saída são maiores que as da entrada devido ao zero padding. Cabe observar que a convolução transposta gera o mesmo resultado da convolução tradicional ao considerarmos a entrada aumentada com zeros. Pode-se mapear a mesma imagem de dimensões \\(2\\times 2\\) em uma imagem de dimensões ainda maiores, aplicando um zero padding mais sofisticado. Na Figura 11, a convolução transposta é aplicada sobre a mesma entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\), além de um zero inserido entre os pixels. Neste caso, a saída fica com dimensões \\(5 \\times 5\\).\n\n\n\n\n\n\nFigura 10: Exemplo da convolução transposta: entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\), filtro \\(3\\times 3\\) e saída \\(4\\times 4.\\) [Fonte].\n\n\n\n\n\n\n\n\n\nFigura 11: Exemplo da convolução transposta: entrada \\(2\\times 2\\) com \\(p=2\\) e \\(s=1\\) e com preenchimento de um zero entre os pixels, filtro \\(3\\times 3\\) e saída \\(5\\times 5\\). [Fonte].\n\n\n\nOutra forma de implementar a convolução transposta é seguindo os passos exemplificados a seguir [Fonte]:\n\nConsidere um mapa de características \\(2\\times 2\\) que precisa ser sobreamostrado para um mapa de características \\(3\\times 3\\).\n\n\n\n\n\n\n\nConsidere o filtro \\(2\\times 2\\)\n\n\n\n\n\n\n\nMultiplique o elemento superior esquerdo do mapa de características de entrada por cada elemento do filtro:\n\n\n\n\n\n\n\nRepita o Item 3 para cada elemento do mapa de características de entrada:\n\n\n\n\n\n\n\nAlguns dos elementos dos mapas de características resultantes da sobreamostragem ficam sobrepostos. Para resolver esse problema, os elementos das posições sobrepostas devem ser adicionados. Assim, obtém-se a saída final.\n\n\n\n\n\n\nPara finalizar, considere a implementação da convolução por meio da multiplicação de matrizes, exemplificada na Figura 3. Multiplicando a matriz do filtro transposta de dimensões \\(16\\times 4\\) pelo vetor de entrada \\(4\\times 1\\), que inclusive pode ser a saída vetorizada de uma camada convolucional anterior, obtém-se um vetor de dimensões \\(16\\times 1\\). Comparando com a entrada que continha apenas 4 pixels, esse procedimento levou a uma saída aumentada com dimensões \\(4\\times 4\\), como mostrado na Figura 12. Não podemos esquecer que os pesos devem ser atualizados para que a sobreamostragem se adéque à aplicação. O fato de usarmos a matriz do filtro transposta \\(\\mathbf{C}^{\\rm T}\\) justifica o nome dessa operação.\n\n\n\n\n\n\nFigura 12: Exemplo da convolução transposta realizada por meio da multiplicação de matrizes: entrada \\(2\\times 2\\) transformada no vetor \\(4\\times 1\\) e filtro \\(3\\times 3\\) com elementos reorganizados na matriz transposta \\(\\mathbf{C}^{\\rm T}\\), o que leva à saída aumentada (sobreamostrada) \\(4\\times 4\\). [Fonte]."
  },
  {
    "objectID": "t_cnn.html#estrutura-de-uma-cnn",
    "href": "t_cnn.html#estrutura-de-uma-cnn",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "Depois de aprendermos como é feita a convolução nas camadas convolucionais em conjunto com os métodos de zero padding (preenchimento com zeros), stride (passo largo) e pooling (subamostragem), podemos agora analisar a estrutura completa de uma CNN típica, como ilustrado na Figura 13. Uma CNN típica é composta de camadas convolucionais para extração de características, seguida de uma MLP totalmente conectada para classificação. Suponha que se deseja identificar se a imagem de entrada da rede corresponde ao Pato Donald, ao Pateta (Goofy) ou ao Piu-Piu (Tweety). Nas camadas convolucionais é comum alternar estágios de convolução e subamostragem (pooling). Como vimos, a subamostragem é importante para que o mapa de características se torne invariante à translação. O mapa de características subamostrado da última camada convolucional deve ser vetorizado. Em seguida, esse vetor entra na MLP. No exemplo da Figura 13, a MLP é composta de uma camada oculta e uma camada de saída. No entanto, em muitos problemas é comum considerar uma MLP profunda, com várias camadas ocultas. Para classificação multiclasse, considera-se a entropia cruzada como função custo e a função de ativação softmax, que fornece a probabilidade da classificação. No caso da Figura 13, há três neurônios de saída e com os valores obtidos pela softmax que estão indicados na figura, decide-se que a imagem é do Piu-Piu.\n\n\n\n\n\n\nFigura 13: Estrutura de uma CNN. [Fonte].\n\n\n\nOs pesos dos filtros das camadas convolucionais e dos neurônios da MLP são atualizados com o algoritmo backpropagation"
  },
  {
    "objectID": "t_cnn.html#exemplo-de-aplicação",
    "href": "t_cnn.html#exemplo-de-aplicação",
    "title": "Rede neural convolucional",
    "section": "",
    "text": "Esse exemplo foi explorado em https://atcold.github.io/pytorch-Deep-Learning/en/week03/03-3/. Nesse exemplo, uma rede MLP e uma CNN foram treinadas para classificação do conjunto de dados MNIST, disponível em http://yann.lecun.com/exdb/mnist/. Trata-se de um banco de dados de dígitos manuscritos, que possui um conjunto de treinamento de 60000 exemplos e um conjunto de teste de 10000 exemplos.\nAs redes MLP e CNN treinadas têm o mesmo número de parâmetros. Na Figura 14, são mostrados exemplos de treinamento do banco de dados MNIST original. Antes do treinamento, os dados foram normalizados para que a inicialização da rede corresponda à distribuição de dados. Além disso, as cinco etapas foram usadas no treinamento:\n\nAlimentação dos dados no modelo;\nCálculo da função custo;\nLimpeza do cache de gradientes acumulados;\nCálculo dos gradientes;\nExecução do método de otimização.\n\n\n\n\n\n\n\nFigura 14: Exemplos de treinamento do banco de dados MNIST original. [Fonte].\n\n\n\nPrimeiramente, ambas as redes foram treinadas com os dados MNIST normalizados. A MLP atingiu uma acurácia de 87% enquanto a acurácia da CNN ficou igual a 95%. Dado o mesmo número de parâmetros, a CNN conseguiu treinar muito mais filtros. Na rede MLP, os neurônios tentam obter algumas dependências entre pixels mais distantes com pixels próximos, o que acaba sendo desperdiçado. Em vez disso, na CNN, os parâmetros se concentram no relacionamento entre os pixels vizinhos.\nEm seguida, foi feita uma permutação aleatória de todos os pixels em todas as imagens do conjunto de dados MNIST. Isso transforma a Figura 14 na Figura 15. Ambas as redes foram treinadas com esse conjunto de dados modificado.\n\n\n\n\n\n\nFigura 15: Exemplos de treinamento do banco de dados MNIST permutado. [Fonte].\n\n\n\nO desempenho da rede MLP quase se manteve inalterado (85%), mas a acurácia da CNN caiu para 83%. Isso ocorre porque, após uma permutação aleatória, as imagens não possuem mais as três propriedades de localidade, estacionariedade e composicionalidade, que são exploradas por uma CNN."
  },
  {
    "objectID": "ex_aula_lms.html",
    "href": "ex_aula_lms.html",
    "title": "Exercício - Filtro Adaptativo LMS",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_autoencoders.html",
    "href": "t_autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "Autoencoders são redes neurais que tem sido usadas para redução de ruído em imagens (denoising) e também para redução de dimensionalidade. Eles são redes neurais em que a saída é uma estimativa da própria entrada. Eles comprimem a entrada em um código de menor dimensão e, em seguida, reconstroem a saída a partir dessa representação. O código gerado também é chamado de representação do espaço latente.\nUm autoencoder é composto por três componentes: codificador, código e decodificador, como mostrado na Figura 1. O codificador comprime a entrada e produz o código, o decodificador então reconstrói a entrada usando apenas esse código.\n\n\n\n\n\n\nFigura 1: Componentes de um autoencoder: codificador, código e decodificador. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nPara construir um autoencoder, precisamos de um método de codificação, um método de decodificação e uma função de perda para comparar a saída com a entrada. Autoencoders são usados principalmente para redução de dimensionalidade (ou compressão) com algumas propriedades importantes:\n\nDados específicos: Autoencoders só são capazes de compactar significativamente dados semelhantes aos que foram treinados. Eles aprendem recursos específicos para os dados de treinamento fornecidos. Portanto, não podemos esperar que um autoencoder treinado em dígitos manuscritos comprima fotos de paisagens.\nCom perdas: A saída do autoencoder não será exatamente igual à entrada, será uma representação próxima, mas degradada. Se você deseja compactação sem perdas, essa não é a melhor solução.\nNão supervisionado: Para treinar um autoencoder, precisamos apenas lançar os dados brutos de entrada nele. Alguns autores interpretam autoencoders como uma técnica de aprendizagem não supervisionada, uma vez que eles não precisam de rótulos explícitos para treinar. No entanto, o mais correto é dizer que sua aprendizagem é auto supervisionada porque eles geram seus próprios rótulos a partir dos dados de treinamento.\n\n\n\nTanto o codificador quanto o decodificador são redes neurais totalmente conectadas, como uma MLP ou CNN. O código é uma única camada cujo número de neurônios no caso de uma MLP ou de filtros no caso de uma CNN deve ser escolhido. Esse número também chamado de tamanho do código é um hiperparâmetro que deve ser definido antes de treinar o autoencoder. Na Figura 2, é mostrada uma visualização mais detalhada de um autoencoder. Primeiro, a entrada passa pelo codificador, que é uma rede neural totalmente conectada, para produzir o código. O decodificador, que possui estrutura de uma rede neural semelhante, produz a saída apenas usando o código. O objetivo é obter uma saída idêntica à entrada. Observe que a arquitetura do decodificador é a imagem espelhada do codificador. No entanto, isso não é um requisito, mas normalmente é o caso. O único requisito é que a dimensionalidade da entrada e da saída seja a mesma.\n\n\n\n\n\n\nFigura 2: Visualização mais detalhada de um autoencoder. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nExistem quatro hiperparâmetros que precisamos definir antes de treinar um autoencoder:\n\nTamanho do código: número de nós (ou filtros) na camada intermediária. Quanto menor o esse número maior a compressão.\nNúmero de camadas: o autoencoder pode ser tão profundo quanto quisermos. Na Figura 2 há duas camadas tanto no codificador quanto no decodificador.\nNúmero de neurônios (ou filtros) por camada: a arquitetura do autoencoder da Figura~\\(\\ref{Fig:auto2}\\) é chamada de autoencoder empilhado, que parece um sanduíche. O número de neurônios (ou filtros) por camada diminui a cada camada subsequente do codificador e aumenta novamente no decodificador. Além disso, o decodificador é simétrico ao codificador em termos de estrutura de camada (mas isso não é necessário).\nFunção de perda: Deve-se considerar as funções custo comumente usadas para regressão (erro quadrático médio) ou classificação (entropia cruzada)\n\nAutoencoders devem ser treinados da mesma forma que as redes MLP ou CNN, isto é, com o algoritmo backpropagation.\n\n\n\nUm autoencoder foi construído com uma MLP para estimar imagens de dígitos do banco de dados MNIST. As imagens são vetorizadas em um vetor com 784 elementos. A primeira camada oculta tem 128 neurônios e o código tem 32 neurônios. O decodificador é espelhado, ou seja, possui uma camada oculta com 128 neurônios e uma camada de saída com 784 neurônios. Nas camadas ocultas utilizou-se a ReLU como função de ativação e a função sigmoidal foi usada na camada de saída. A entropia cruzada binária foi utilizada como função custo.\nNa Figura 3 são mostradas as imagens originais e as imagens reconstruídas com o autoencoder treinado. As imagens reconstruídas são realmente muito parecidas com as originais, mas não exatamente iguais. Podemos notar isso mais claramente no dígito “4”.\n\n\n\n\n\n\nFigura 3: Imagens originais e reconstruídas de dígitos do banco de dados MNIST com um autoencoder MLP descrito no texto. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nTemos total controle sobre a arquitetura do autoencoder. Podemos torná-lo muito poderoso aumentando o número de camadas, neurônios por camada e, mais importante, o tamanho do código. Aumentar esses hiperparâmetros permitirá que o autoencoder aprenda codificações mais complexas. Mas devemos ter cuidado para não torná-lo muito poderoso. Caso contrário, o autoencoder simplesmente aprenderá a copiar suas entradas para a saída, sem aprender nenhuma representação significativa. Ele apenas imitará a função de identidade. O autoencoder reconstruirá os dados de treinamento perfeitamente, mas observaremos overfitting e uma baixa capacidade de generalização.\nÉ por isso que é comum considerar uma arquitetura do tipo “sanduíche” e o tamanho do código pequeno. Uma vez que a camada de codificação tem uma dimensionalidade menor do que os dados de entrada, o autoencoder é dito incompleto . Ele não poderá copiar diretamente suas entradas para a saída e será forçado a aprender recursos inteligentes. Se os dados de entrada tiverem um padrão, por exemplo, o dígito “1” geralmente contém uma linha reta e o dígito “0” é circular, ele aprenderá esse fato e o codificará de uma forma mais compacta. Se os dados de entrada forem completamente aleatórios sem qualquer correlação ou dependência interna, um autoencoder incompleto não poderá recuperá-los perfeitamente. Felizmente no mundo real há muita dependência.\n\n\n\nManter a camada de código pequena força o autoencoder a aprender uma representação inteligente dos dados. Existe outra maneira de forçar o autoencoder a aprender recursos úteis, que é adicionar ruído aleatório às suas entradas e fazer com que ele recupere os dados originais sem ruído. Dessa forma, o autoencoder não pode simplesmente copiar a entrada para sua saída porque a entrada também contém ruído aleatório. O objetivo é eliminar o ruído e produzir os dados significativos subjacentes. Isso é chamado de denoising autoencoder. Na Figura 4, é mostrado um exemplo desse esquema.\n\n\n\n\n\n\nFigura 4: Esquema do denoising autoencoder. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nAdicionando ruído gaussiano aos dados de entrada e treinando o autoencoder anterior com os dados ruidosos, obtém-se o resultado da Figura 5, o que mostra um desempenho muito bom. O resultado pode ser ainda melhor considerando redes convolucionais.\n\n\n\n\n\n\nFigura 5: Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nPara forçar o autoencoder a aprender recursos úteis vimos que é importante manter o tamanho do código pequeno e/ou adicionar ruído à entrada. Outra maneira de forçar isso é usar a regularização. Podemos regularizar o autoencoder usando uma restrição de esparsidade tal que apenas uma fração dos neurônios fique ativa a iteração do algoritmo de treinamento, o que pode ser feito adicionando um termo de penalidade à função custo. Isso força o autoencoder a representar cada entrada como uma combinação de um pequeno número de neurônios e exige que ele descubra uma estrutura interessante nos dados. Esse método é chamado de sparse autoencoders e funciona mesmo se o tamanho do código for grande, pois apenas um pequeno subconjunto dos neurônios estará ativo."
  },
  {
    "objectID": "t_autoencoders.html#arquitetura",
    "href": "t_autoencoders.html#arquitetura",
    "title": "Autoencoders",
    "section": "",
    "text": "Tanto o codificador quanto o decodificador são redes neurais totalmente conectadas, como uma MLP ou CNN. O código é uma única camada cujo número de neurônios no caso de uma MLP ou de filtros no caso de uma CNN deve ser escolhido. Esse número também chamado de tamanho do código é um hiperparâmetro que deve ser definido antes de treinar o autoencoder. Na Figura 2, é mostrada uma visualização mais detalhada de um autoencoder. Primeiro, a entrada passa pelo codificador, que é uma rede neural totalmente conectada, para produzir o código. O decodificador, que possui estrutura de uma rede neural semelhante, produz a saída apenas usando o código. O objetivo é obter uma saída idêntica à entrada. Observe que a arquitetura do decodificador é a imagem espelhada do codificador. No entanto, isso não é um requisito, mas normalmente é o caso. O único requisito é que a dimensionalidade da entrada e da saída seja a mesma.\n\n\n\n\n\n\nFigura 2: Visualização mais detalhada de um autoencoder. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nExistem quatro hiperparâmetros que precisamos definir antes de treinar um autoencoder:\n\nTamanho do código: número de nós (ou filtros) na camada intermediária. Quanto menor o esse número maior a compressão.\nNúmero de camadas: o autoencoder pode ser tão profundo quanto quisermos. Na Figura 2 há duas camadas tanto no codificador quanto no decodificador.\nNúmero de neurônios (ou filtros) por camada: a arquitetura do autoencoder da Figura~\\(\\ref{Fig:auto2}\\) é chamada de autoencoder empilhado, que parece um sanduíche. O número de neurônios (ou filtros) por camada diminui a cada camada subsequente do codificador e aumenta novamente no decodificador. Além disso, o decodificador é simétrico ao codificador em termos de estrutura de camada (mas isso não é necessário).\nFunção de perda: Deve-se considerar as funções custo comumente usadas para regressão (erro quadrático médio) ou classificação (entropia cruzada)\n\nAutoencoders devem ser treinados da mesma forma que as redes MLP ou CNN, isto é, com o algoritmo backpropagation."
  },
  {
    "objectID": "t_autoencoders.html#exemplos",
    "href": "t_autoencoders.html#exemplos",
    "title": "Autoencoders",
    "section": "",
    "text": "Um autoencoder foi construído com uma MLP para estimar imagens de dígitos do banco de dados MNIST. As imagens são vetorizadas em um vetor com 784 elementos. A primeira camada oculta tem 128 neurônios e o código tem 32 neurônios. O decodificador é espelhado, ou seja, possui uma camada oculta com 128 neurônios e uma camada de saída com 784 neurônios. Nas camadas ocultas utilizou-se a ReLU como função de ativação e a função sigmoidal foi usada na camada de saída. A entropia cruzada binária foi utilizada como função custo.\nNa Figura 3 são mostradas as imagens originais e as imagens reconstruídas com o autoencoder treinado. As imagens reconstruídas são realmente muito parecidas com as originais, mas não exatamente iguais. Podemos notar isso mais claramente no dígito “4”.\n\n\n\n\n\n\nFigura 3: Imagens originais e reconstruídas de dígitos do banco de dados MNIST com um autoencoder MLP descrito no texto. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nTemos total controle sobre a arquitetura do autoencoder. Podemos torná-lo muito poderoso aumentando o número de camadas, neurônios por camada e, mais importante, o tamanho do código. Aumentar esses hiperparâmetros permitirá que o autoencoder aprenda codificações mais complexas. Mas devemos ter cuidado para não torná-lo muito poderoso. Caso contrário, o autoencoder simplesmente aprenderá a copiar suas entradas para a saída, sem aprender nenhuma representação significativa. Ele apenas imitará a função de identidade. O autoencoder reconstruirá os dados de treinamento perfeitamente, mas observaremos overfitting e uma baixa capacidade de generalização.\nÉ por isso que é comum considerar uma arquitetura do tipo “sanduíche” e o tamanho do código pequeno. Uma vez que a camada de codificação tem uma dimensionalidade menor do que os dados de entrada, o autoencoder é dito incompleto . Ele não poderá copiar diretamente suas entradas para a saída e será forçado a aprender recursos inteligentes. Se os dados de entrada tiverem um padrão, por exemplo, o dígito “1” geralmente contém uma linha reta e o dígito “0” é circular, ele aprenderá esse fato e o codificará de uma forma mais compacta. Se os dados de entrada forem completamente aleatórios sem qualquer correlação ou dependência interna, um autoencoder incompleto não poderá recuperá-los perfeitamente. Felizmente no mundo real há muita dependência."
  },
  {
    "objectID": "t_autoencoders.html#denoising-autoencoders",
    "href": "t_autoencoders.html#denoising-autoencoders",
    "title": "Autoencoders",
    "section": "",
    "text": "Manter a camada de código pequena força o autoencoder a aprender uma representação inteligente dos dados. Existe outra maneira de forçar o autoencoder a aprender recursos úteis, que é adicionar ruído aleatório às suas entradas e fazer com que ele recupere os dados originais sem ruído. Dessa forma, o autoencoder não pode simplesmente copiar a entrada para sua saída porque a entrada também contém ruído aleatório. O objetivo é eliminar o ruído e produzir os dados significativos subjacentes. Isso é chamado de denoising autoencoder. Na Figura 4, é mostrado um exemplo desse esquema.\n\n\n\n\n\n\nFigura 4: Esquema do denoising autoencoder. Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nAdicionando ruído gaussiano aos dados de entrada e treinando o autoencoder anterior com os dados ruidosos, obtém-se o resultado da Figura 5, o que mostra um desempenho muito bom. O resultado pode ser ainda melhor considerando redes convolucionais.\n\n\n\n\n\n\nFigura 5: Fonte: https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n\n\n\nPara forçar o autoencoder a aprender recursos úteis vimos que é importante manter o tamanho do código pequeno e/ou adicionar ruído à entrada. Outra maneira de forçar isso é usar a regularização. Podemos regularizar o autoencoder usando uma restrição de esparsidade tal que apenas uma fração dos neurônios fique ativa a iteração do algoritmo de treinamento, o que pode ser feito adicionando um termo de penalidade à função custo. Isso força o autoencoder a representar cada entrada como uma combinação de um pequeno número de neurônios e exige que ele descubra uma estrutura interessante nos dados. Esse método é chamado de sparse autoencoders e funciona mesmo se o tamanho do código for grande, pois apenas um pequeno subconjunto dos neurônios estará ativo."
  },
  {
    "objectID": "ex_aula_mlp_2.html",
    "href": "ex_aula_mlp_2.html",
    "title": "Exercício - MLP 2",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "ex_aula_perceptron_rosenblatt_e_reg_logistica.html",
    "href": "ex_aula_perceptron_rosenblatt_e_reg_logistica.html",
    "title": "Exercício - Perceptron de Rosenblatt e Regressão Logística",
    "section": "",
    "text": "Jupyter Notebook do exercício"
  },
  {
    "objectID": "t_regressao_linear.html",
    "href": "t_regressao_linear.html",
    "title": "Regressão linear",
    "section": "",
    "text": "Em engenharia, é muito comum ajustar um modelo a dados experimentais previamente observados. Utilizando o modelo, é possíver prever valores de dados não observados, o que caracteriza um problema de regressão. Um dos modelos mais simples é o ajuste de uma reta a dados conhecidos, o que leva à solução conhecida como regressão linear univariada.\nSeja\n\\[\n\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\},\n\\]\num conjunto de \\(N\\) pontos conhecidos previamente que podem ser fruto de um experimento. Vamos obter a melhor reta que se ajusta a esses pontos. Quando dizemos melhor, precisamos especificar em que sentido. Como, em geral, os pontos são experimentais, raramente se consegue obter uma reta que se ajuste exatamente aos pontos. Por isso, vamos buscar uma aproximação que melhor se ajusta aos dados, considerando o critério dos mínimos quadrados. Assim, deseja-se obter uma relação matemática do tipo\n\\[\nd=b+wx\n\\]\nentre as variáveis \\(x\\) e \\(d\\), em que \\(w\\) e \\(b\\) são constantes que se deseja determinar. É comum chamar \\(d\\) de sinal desejado ou rótulo, \\(x\\) de entrada, \\(w\\) de peso e \\(b\\) de viés (ou bias).\nQuando os pontos experimentais são colineares, a reta passa exatamente por todos os \\(n\\) pontos e as constantes desconhecidas \\(w\\) e \\(b\\) satisfazem\n\\[\n\\begin{array}{c}\n  d_1=b+w\\;x_1 \\\\\n  d_2=b+w\\;x_2 \\\\\n  \\vdots \\\\\n  d_N=b+w\\;x_N.\n\\end{array}\n\\]\nPodemos reescrever esse sistema de equações na forma matricial, ou seja,\n\\[\n\\underbrace{\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n   d_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{d}}=\n\\underbrace{\\left[\n  \\begin{array}{cc}\n    1&x_1 \\\\\n    1&x_2 \\\\\n    \\vdots&\\vdots \\\\\n    1&x_N\n  \\end{array}\n\\right]}_{\\mathbf{X}}\n\\underbrace{\\left[\n  \\begin{array}{c}\n    b \\\\\n    w   \\end{array}\n\\right]}_{\\mathbf{w}}.\n\\]\nNesse caso, como os pontos experimentais são colineares, vale \\(\\mathbf{d}-\\mathbf{X}\\mathbf{w}=\\mathbf{0}\\).\nSe os pontos não forem colineares, o que acontece na maior parte dos casos, \\(\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\neq\\mathbf{0}\\). Dessa forma, para encontrar a reta que melhor se ajusta aos dados, vamos representar a diferença entre os vetores \\(\\mathbf{d}\\) e \\(\\mathbf{Xw}\\) por meio do vetor de erros, ou seja,\n\\[\n\\mathbf{e}=\\mathbf{d}-\\mathbf{X}\\mathbf{w}.\n\\]\nOs elementos desse vetor de erros, \\(e_i=d_i-b-wx_i\\) para \\(i=1,\\cdots,N\\), representam as distâncias verticais da reta \\(wx+b\\) aos pontos experimentais \\((x_i,d_i)\\), como ilustrado na Figura 1, considerando \\(N=3\\).\n\n\n\n\n\n\nFigura 1: Distância de um conjunto de pontos a uma determinada reta, considerando \\(N=3\\).\n\n\n\nA melhor reta segundo o critério dos mínimos quadrados deve minimizar o quadrado da norma Euclidiana do vetor de erros, ou seja\n\\[\n\\|\\mathbf{e}\\|^2=\\sum_{i=1}^N e_i^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2=\\sum_{i=1}^N(d_i-b-wx_i)^2.\n\\]\nPara minimizar essa norma quadrática, devemos derivá-la em relação às constantes \\(w\\) e \\(b\\) que se deseja determinar e igualar essas derivadas a zero. Assim, obtemos as seguintes derivadas:\n\\[\n\\begin{array}{cccc}\n   \\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial w} & = & 2\\displaystyle\\sum_{i=1}^N e_i\\displaystyle\\frac{\\partial e_i}{\\partial w} &\n  = -2\\displaystyle\\sum_{i=1}^N e_i x_i\\\\\n   \\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial b} & = & 2\\displaystyle\\sum_{i=1}^N e_i\\displaystyle\\frac{\\partial e_i}{\\partial b} &\n  = - 2\\displaystyle\\sum_{i=1}^N e_i,\n\\end{array}\n\\]\nque podem ser escritas de forma compacta como\n\\[\n\\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial \\mathbf{w}}=\n-2\\displaystyle\\sum_{i=1}^N \\left[\n                                  \\begin{array}{c}\n                                    1 \\\\\n                                    x_i \\\\\n                                  \\end{array}\n                                \\right] e_i=-2\\mathbf{X}^{{\\rm T}}\\mathbf{e}=-2\\mathbf{X}^{{\\rm T}}(\\mathbf{d}-\\mathbf{X}\\mathbf{w}),\n\\]\nem que \\((\\cdot)^{{\\rm T}}\\) representa a operação de transposição da matriz \\(\\mathbf{X}\\). Igualando essa derivada ao vetor nulo, obtemos\n\\[\n-2\\mathbf{X}^{{\\rm T}}(\\mathbf{d}-\\mathbf{X}\\mathbf{w}^{\\rm o})=\\mathbf{0},\n\\]\nou ainda\n\\[\n\\mathbf{X}^{{\\rm T}}\\mathbf{X}\\mathbf{w}^{\\rm o}=\\mathbf{X}^{{\\rm T}}\\mathbf{d}.\n\\]\nPortanto, o vetor de coeficientes \\(\\mathbf{w}\\) que satisfaz essa equação, denotado como \\(\\mathbf{w}^{\\rm o}=[\\,b^{\\rm o}\\;\\;w^{\\rm o}\\,]^{{\\rm T}}\\), minimiza a norma quadrática do vetor de erros e\n\\[\ny=w^{\\rm o}x+b^{\\rm o}\\approx d\n\\]\né a melhor reta que se ajusta aos pontos previamente conhecidos, segundo o critério dos mínimos quadrados.\nSe \\(\\mathbf{X}^{{\\rm T}}\\mathbf{X}\\) for invertível,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{{\\rm T}}\\mathbf{X})^{-1}\\mathbf{X}^{{\\rm T}}\\mathbf{d}.\n$}\n\\end{equation*}\\]\nEssa equação expressa a unicidade da solução. Assim, existe uma única reta que se ajusta a esses pontos segundo o critério dos mínimos quadrados.\nObservações importantes:\n\nO modelo \\(y=w^{\\rm o}x+b^{\\rm o}\\) é de fato linear apenas quando \\(b^{\\rm o}=0\\), pois neste caso \\(x=0\\) leva a \\(y=0\\). No entanto, o termo linear é frequentemente usado na literatura neste caso para se referir ao modelo dado por uma reta.\nOs dados \\(\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\}\\) conhecidos previamente foram totalmente usados aqui para se obter o modelo da reta. Neste caso, eles podem ser chamados de dados de treinamento do modelo.\nA matriz \\((\\mathbf{X}^{{\\rm T}}\\mathbf{X})^{-1}\\mathbf{X}^{{\\rm T}}\\) é conhecida na literatura como a pseudoinversa de \\(\\mathbf{X}\\).\n\n\n\n\nSuponha agora que os dados não sejam mais compostos por duplas do tipo \\((x_i, d_i)\\), mas por uma sequência de \\(M\\) valores de \\(x\\), seguida do valor de \\(d\\), ou seja,\n\\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1} ,d_1), (x_{12}, x_{22}, \\cdots, x_{M2} ,d_2),\\cdots, (x_{1N}, x_{2N}, \\cdots, x_{MN} ,d_N)\\}.\n\\]\nConsiderando que esses \\(N\\) conjuntos de dados sejam previamente conhecidos, deseja-se agora obter a melhor função linear segundo o critério dos mínimos quadrados, que se ajusta a esses dados. Trata-se de uma generalização do resultado anterior. Em vez de se obter a melhor reta, vamos encontrar o melhor hiperplano que se ajusta aos dados, levando à solução conhecida como regressão linear multivariada.\nAssim, o modelo se torna\n\\[\ny=b+w_1x_1+w_2x_2+\\cdots+w_Mx_M\\approx d.\n\\]\nConsiderando os \\(N\\) conjuntos de dados, obtemos o seguinte vetor de erros\n\\[\n\\underbrace{\\left[\n  \\begin{array}{c}\n    e_1 \\\\\n    e_2 \\\\\n    \\vdots \\\\\n    e_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{e}}\n=\\underbrace{\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n    d_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{d}}\n-\n\\underbrace{\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N} & x_{2N} & \\cdots & x_{MN} \\\\\n  \\end{array}\n\\right]}_{\\mathbf{X}}\n\\underbrace{\\left[\n  \\begin{array}{c}\n    b \\\\\n    w_1 \\\\\n    \\vdots \\\\\n    w_M \\\\\n  \\end{array}\n\\right]}_{\\mathbf{w}}\n\\]\nComo no caso da reta, o melhor hiperplano que se ajusta aos dados segundo o critério dos mínimos quadrados é o que minimiza o quadrado da norma Euclidiana do vetor de erros, dada por\n\\[\n\\|\\mathbf{e}\\|^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2.\n\\]\nGeneralizando os passos para obtenção da reta que se ajusta aos dados, chega-se a\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1}\\mathbf{X}^{\\rm T}\\mathbf{d}\n$}\n\\end{equation*}\\]\nem que \\(\\mathbf{w}^{\\rm o}=[\\,b^{\\rm o}\\;\\;w_1^{\\rm o}\\;\\;w_2^{\\rm o}\\;\\;\\cdots\\;\\;w_M^{\\rm o}\\,]^{\\rm T}\\) é o vetor que contém o bias e pesos ótimos que minimizam \\(\\|\\mathbf{e}\\|^2\\).\nObservações importantes:\n\nCalcular a inversa da matriz \\(\\mathbf{X}^{\\rm T}\\mathbf{X}\\) diretamente pode levar a problemas numéricos, dependendo do valor de \\(M\\). Isso ocorre, por exemplo, quando se utiliza a função inv.m no Matlab. Algo semelhante também ocorre em Python e é pior ao se considerar precisão de 32 bits em ponto flutuante. Procure evitar isso, resolvendo o sistema linear\n\\[\n\\mathbf{X}^{\\rm T}\\mathbf{X}\\mathbf{w}^{\\rm o}=\\mathbf{X}^{\\rm T}\\mathbf{d}\n\\]\npara encontrar \\(\\mathbf{w}^{\\rm o}\\). No Matlab, basta fazer \\((\\mathbf{X}^{\\rm T}\\mathbf{X})\\backslash(\\mathbf{X}^{\\rm T}\\mathbf{d})\\). Em Python, pode-se, por exemplo, usar a função np.linalg.solve do NumPy.\nA matriz \\(\\mathbf{X}^{\\rm T}\\mathbf{X}\\) é uma estimativa da matriz de autocorrelação dos dados de entrada \\(x\\).\nO vetor \\(\\mathbf{X}^{\\rm T}\\mathbf{d}\\) é uma estimativa da correlação cruzada entre os dados de entrada \\(x\\) e o sinal desejado \\(d\\).\nQuando se deseja ajustar um polinômio de grau \\(M\\) aos dados\n\\[\n\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\},\n\\]\nbasta usar o resultado do caso multivariado, considerando\n\\[\n\\{(x_{1}, x_{1}^2, \\cdots, x_{1}^M ,d_1), (x_{2}, x_{2}^2, \\cdots, x_{2}^M ,d_2),\\cdots, (x_{N}, x_{N}^2, \\cdots, x_{N}^M ,d_N)\\}.\n\\]\nIsso leva à seguinte aproximação\n\\[\ny=b+w_1x+w_2x^2+\\cdots+w_Mx^M\\approx d.\n\\]\nNeste caso a matrix \\(\\mathbf{X}\\) se torna\n\\[\\mathbf{X}=\\left[\n   \\begin{array}{ccccc}\n     1      & x_{1} & x_{1}^2 & \\cdots & x_{1}^M \\\\\n     1      & x_{2} & x_{2}^2 & \\cdots & x_{2}^M \\\\\n     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n     1      & x_{N} & x_{N}^2 & \\cdots & x_{N}^M \\\\\n   \\end{array}\n\\right].\n\\]\nO resultado do item anterior pode ser usado para aproximar os dados não só por polinômios, mas também por outras funções. Por exemplo, poderíamos calcular as seguintes aproximações para os dados\n\\[\ny=b+w_1\\ln(x+5)+w_2\\exp(x-2)\\approx d\n\\]\nou\n\\[\ny=b+w_1\\cos(2\\pi f_0 x)+w_2{\\rm sen}(2\\pi f_0 x)\\approx d\n\\]\nem que \\(f_0\\) é uma frequência pré-determinada. Um outro exemplo útil em Engenharia Elétrica é aproximar uma função \\(f(t)\\) periódica com período \\(T_0=1/f_0\\) por uma soma de senos e cossenos, ou seja,\n\\[\n\\begin{align}\n{f}(t)\\approx \\;b&+w_{11}\\cos(2\\pi f_0 t)+ w_{12}{\\rm sen}\\,(2\\pi f_0 t)\\nonumber\\\\\n&+w_{21}\\cos(2\\pi 2 f_0 t)+ w_{22}{\\rm sen}\\,(2\\pi 2 f_0 t)\\nonumber\\\\\n&+\\cdots\\nonumber\\\\\n&+w_{M1}\\cos(2\\pi f_0 M t)+ w_{M2}{\\rm sen}\\,(2\\pi f_0 M t).\\nonumber\n\\end{align}\n\\] Os coeficientes \\(b, w_{11}, w_{12}, w_{21}, w_{22}, \\cdots, w_{M1}, w_{M2}\\) são conhecidos como coeficientes da série de Fourier e os dados usados para obter essa aproximação são obtidos a partir da amostragem da função \\(f(t)\\).\n\n\n\n\nUm conceito que aparece de forma recorrente em redes neurais é o chamado overfitting. Apesar de nem termos falado em redes neurais ainda, é possível já introduzir esse conceito considerando regressão linear. Antes de falar de overfitting, precisamos fazer algumas considerações importantes.\nSuponha que se deseja criar um modelo de regressão para prever o valor de venda de um automóvel usado. Dispomos de um banco de dados que contém várias informações sobre diferentes automóveis usados como ano de fabricação, modelo, estado de conservação, valor da tabela Fipe, valor médio de venda no mercado, etc. Utilizando esse banco de dados, podemos obter um modelo de regressão linear, que neste caso será multivariada, pois dispomos de muitas variáveis. Poderíamos usar todos os dados disponíveis para gerar o modelo. Se fizéssemos isso, como conseguiríamos avaliar se o modelo obtido é bom? Como saber se o modelo é capaz de prever adequadamente o valor de venda de um determinado carro que não aparece no banco de dados? Por isso, é importante reservar uma parte dos dados para avaliar a qualidade do modelo. Assim, é uma prática comum separar os dados de forma aleatória em dois conjuntos disjuntos: (1) conjunto de treinamento (ou aprendizado) e (2) conjunto de teste1. Os dados do conjunto de treinamento são efetivamente usados para gerar o modelo. Os dados do conjunto de teste são então usados para avaliar a qualidade do modelo gerado. Os dados usados para avaliação não devem aparecer no treinamento e vice-versa. Se o modelo se sair bem no teste, costuma-se dizer que ele tem uma boa capacidade de generalização.\nEm qualquer problema de regressão, deseja-se que o modelo tenha uma boa capacidade de generalização. No exemplo do automóvel usado, é importante que o modelo consiga prever com o menor erro possível o valor de venda de um carro que não constava no banco de dados. No entanto, um modelo com muitos parâmetros pode ter um ótimo desempenho no treinamento, mas uma baixa capacidade de generalização, o que leva a um erro elevado na fase de teste. Isso é chamado de overfitting. Modelos com baixa capacidade de generalização não são desejáveis, uma vez que na prática serão apresentados a dados que não foram usados no treinamento e deveriam ser capazes de realizar uma predição ou classificação de maneira adequada. Diante isso, existem várias técnicas em aprendizado de máquina que foram propostas para evitar o overfitting. Por ora, vamos apenas entender melhor esse conceito com um exemplo.\nConsidere que dispomos de apenas dez valores igualmente espaçados de \\(x\\) no intervalo \\([0,\\!1;\\;1,\\!5]\\). Os valores de \\(d\\) são gerados utilizando a função\n\\[\nd=0,\\!5+0,\\!25\\cos(2\\pi x)+v,\n\\]\nem que \\(v\\) é um ruído branco gaussiano com média zero e desvio padrão \\(0,\\!06\\). Assim, por exemplo, poderíamos ter o seguinte conjunto de treinamento\n\\[\n\\{(0,\\!1000,\\;0,\\!7055),\\;\\;(0,\\!2556,\\;0,\\!4357),\\;\\;(0,\\!4111,\\;0,\\!3264),\\;\\;\\cdots,\\;\\;(1,\\!5000,\\;0,\\!2514)\\}.\n\\]\nComo o valor de \\(d\\) depende do ruído, se não fixarmos uma semente, cada vez que gerarmos os dados teremos valores distintos. O objetivo é encontrar uma função (um modelo) polinomial de grau \\(M\\) que melhor se aproxima dos pontos do conjunto de treinamento, levando em conta a forma da cossenóide sem ruído. Na Figura 2, são mostrados os pontos disponíveis no treinamento (em vermelho), as curvas pretas representam o sinal senoidal sem ruído e as azuis os polinômios obtidos com a regressão. Foram considerados polinômios com graus \\(M=1\\) (reta), \\(M=2\\) (parábola) até \\(M=9\\). É possível ver que para \\(M=1\\) e \\(M=2\\) ocorre o underfitting, ou seja, as distâncias dos pontos de treinamento aos pontos gerados pelos polinômios dos modelos são elevadas, o que indica que eles não são adequados. À medida em que o valor do grau do polinômio aumenta, observa-se um melhor ajuste entre os pontos vermelhos e as curvas azuis, até o caso extremo de \\(M=9\\). Neste caso, o polinômio obtido passa exatamente em todos os pontos do treinamento, mas claramente a curva azul fica distante da cossenóide sem ruído em alguns trechos como pode ser visto pelas flutuações indesejadas. Isso indica que pode ter ocorrido overfitting devido ao número excessivo de parâmetros do modelo.\n\n\n\n\n\n\n\n\n\nFigura 2: Regressão linear polinomial; \\(M\\) representa o grau do polinômio, os pontos do conjunto de treinamento estão representados em vermelho; as curvas pretas representam o sinal senoidal sem ruído e as azuis os polinômios obtidos com a regressão.\n\n\n\n\n\n\nPara analisar o overfitting, geramos um conjunto de teste com 1401 valores de \\(x\\) igualmente espaçados no intervalo \\([0,\\!1;\\;1,\\!5]\\) e calculamos o valor de \\(d\\). Como há ruído na geração de \\(d\\), os pontos gerados no teste são diferentes dos de treinamento. Para cada valor de \\(M\\), medimos o valor absoluto médio do erro de predição, levando em conta o conjunto de treinamento e de teste. Na Figura 3, são mostrados os valores desses erros em função do grau do polinômio. Como esperado, o erro de aprendizagem (com os dados do treinamento) diminuem monotonicamente, chegando a zero para \\(M=9\\). Esse comportamento é típico sempre que o modelo ajustado varia do mais simples para o mais complexo. Em contrapartida, o erro do teste diminui até \\(M=5\\) e depois aumenta, indicando que modelos com muitos parâmetros têm baixas capacidades de generalização.\n\n\n\n\n\n\n\n\n\nFigura 3: Regressão linear polinomial; valor médio do módulo do erro de predição levando em conta o conjunto de treinamento e o conjunto de teste.\n\n\n\n\n\n\n\n[Miranda et al. (2015)](Goodfellow, Bengio, e Courville 2016)[Bishop (2006)](Izenman 2008)(Haykin 2009)"
  },
  {
    "objectID": "t_regressao_linear.html#regressão-linear-univariada",
    "href": "t_regressao_linear.html#regressão-linear-univariada",
    "title": "Regressão linear",
    "section": "",
    "text": "Em engenharia, é muito comum ajustar um modelo a dados experimentais previamente observados. Utilizando o modelo, é possíver prever valores de dados não observados, o que caracteriza um problema de regressão. Um dos modelos mais simples é o ajuste de uma reta a dados conhecidos, o que leva à solução conhecida como regressão linear univariada.\nSeja\n\\[\n\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\},\n\\]\num conjunto de \\(N\\) pontos conhecidos previamente que podem ser fruto de um experimento. Vamos obter a melhor reta que se ajusta a esses pontos. Quando dizemos melhor, precisamos especificar em que sentido. Como, em geral, os pontos são experimentais, raramente se consegue obter uma reta que se ajuste exatamente aos pontos. Por isso, vamos buscar uma aproximação que melhor se ajusta aos dados, considerando o critério dos mínimos quadrados. Assim, deseja-se obter uma relação matemática do tipo\n\\[\nd=b+wx\n\\]\nentre as variáveis \\(x\\) e \\(d\\), em que \\(w\\) e \\(b\\) são constantes que se deseja determinar. É comum chamar \\(d\\) de sinal desejado ou rótulo, \\(x\\) de entrada, \\(w\\) de peso e \\(b\\) de viés (ou bias).\nQuando os pontos experimentais são colineares, a reta passa exatamente por todos os \\(n\\) pontos e as constantes desconhecidas \\(w\\) e \\(b\\) satisfazem\n\\[\n\\begin{array}{c}\n  d_1=b+w\\;x_1 \\\\\n  d_2=b+w\\;x_2 \\\\\n  \\vdots \\\\\n  d_N=b+w\\;x_N.\n\\end{array}\n\\]\nPodemos reescrever esse sistema de equações na forma matricial, ou seja,\n\\[\n\\underbrace{\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n   d_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{d}}=\n\\underbrace{\\left[\n  \\begin{array}{cc}\n    1&x_1 \\\\\n    1&x_2 \\\\\n    \\vdots&\\vdots \\\\\n    1&x_N\n  \\end{array}\n\\right]}_{\\mathbf{X}}\n\\underbrace{\\left[\n  \\begin{array}{c}\n    b \\\\\n    w   \\end{array}\n\\right]}_{\\mathbf{w}}.\n\\]\nNesse caso, como os pontos experimentais são colineares, vale \\(\\mathbf{d}-\\mathbf{X}\\mathbf{w}=\\mathbf{0}\\).\nSe os pontos não forem colineares, o que acontece na maior parte dos casos, \\(\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\neq\\mathbf{0}\\). Dessa forma, para encontrar a reta que melhor se ajusta aos dados, vamos representar a diferença entre os vetores \\(\\mathbf{d}\\) e \\(\\mathbf{Xw}\\) por meio do vetor de erros, ou seja,\n\\[\n\\mathbf{e}=\\mathbf{d}-\\mathbf{X}\\mathbf{w}.\n\\]\nOs elementos desse vetor de erros, \\(e_i=d_i-b-wx_i\\) para \\(i=1,\\cdots,N\\), representam as distâncias verticais da reta \\(wx+b\\) aos pontos experimentais \\((x_i,d_i)\\), como ilustrado na Figura 1, considerando \\(N=3\\).\n\n\n\n\n\n\nFigura 1: Distância de um conjunto de pontos a uma determinada reta, considerando \\(N=3\\).\n\n\n\nA melhor reta segundo o critério dos mínimos quadrados deve minimizar o quadrado da norma Euclidiana do vetor de erros, ou seja\n\\[\n\\|\\mathbf{e}\\|^2=\\sum_{i=1}^N e_i^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2=\\sum_{i=1}^N(d_i-b-wx_i)^2.\n\\]\nPara minimizar essa norma quadrática, devemos derivá-la em relação às constantes \\(w\\) e \\(b\\) que se deseja determinar e igualar essas derivadas a zero. Assim, obtemos as seguintes derivadas:\n\\[\n\\begin{array}{cccc}\n   \\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial w} & = & 2\\displaystyle\\sum_{i=1}^N e_i\\displaystyle\\frac{\\partial e_i}{\\partial w} &\n  = -2\\displaystyle\\sum_{i=1}^N e_i x_i\\\\\n   \\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial b} & = & 2\\displaystyle\\sum_{i=1}^N e_i\\displaystyle\\frac{\\partial e_i}{\\partial b} &\n  = - 2\\displaystyle\\sum_{i=1}^N e_i,\n\\end{array}\n\\]\nque podem ser escritas de forma compacta como\n\\[\n\\displaystyle\\frac{\\displaystyle\\partial\\sum_{i=1}^N e_i^2}{\\partial \\mathbf{w}}=\n-2\\displaystyle\\sum_{i=1}^N \\left[\n                                  \\begin{array}{c}\n                                    1 \\\\\n                                    x_i \\\\\n                                  \\end{array}\n                                \\right] e_i=-2\\mathbf{X}^{{\\rm T}}\\mathbf{e}=-2\\mathbf{X}^{{\\rm T}}(\\mathbf{d}-\\mathbf{X}\\mathbf{w}),\n\\]\nem que \\((\\cdot)^{{\\rm T}}\\) representa a operação de transposição da matriz \\(\\mathbf{X}\\). Igualando essa derivada ao vetor nulo, obtemos\n\\[\n-2\\mathbf{X}^{{\\rm T}}(\\mathbf{d}-\\mathbf{X}\\mathbf{w}^{\\rm o})=\\mathbf{0},\n\\]\nou ainda\n\\[\n\\mathbf{X}^{{\\rm T}}\\mathbf{X}\\mathbf{w}^{\\rm o}=\\mathbf{X}^{{\\rm T}}\\mathbf{d}.\n\\]\nPortanto, o vetor de coeficientes \\(\\mathbf{w}\\) que satisfaz essa equação, denotado como \\(\\mathbf{w}^{\\rm o}=[\\,b^{\\rm o}\\;\\;w^{\\rm o}\\,]^{{\\rm T}}\\), minimiza a norma quadrática do vetor de erros e\n\\[\ny=w^{\\rm o}x+b^{\\rm o}\\approx d\n\\]\né a melhor reta que se ajusta aos pontos previamente conhecidos, segundo o critério dos mínimos quadrados.\nSe \\(\\mathbf{X}^{{\\rm T}}\\mathbf{X}\\) for invertível,\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{{\\rm T}}\\mathbf{X})^{-1}\\mathbf{X}^{{\\rm T}}\\mathbf{d}.\n$}\n\\end{equation*}\\]\nEssa equação expressa a unicidade da solução. Assim, existe uma única reta que se ajusta a esses pontos segundo o critério dos mínimos quadrados.\nObservações importantes:\n\nO modelo \\(y=w^{\\rm o}x+b^{\\rm o}\\) é de fato linear apenas quando \\(b^{\\rm o}=0\\), pois neste caso \\(x=0\\) leva a \\(y=0\\). No entanto, o termo linear é frequentemente usado na literatura neste caso para se referir ao modelo dado por uma reta.\nOs dados \\(\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\}\\) conhecidos previamente foram totalmente usados aqui para se obter o modelo da reta. Neste caso, eles podem ser chamados de dados de treinamento do modelo.\nA matriz \\((\\mathbf{X}^{{\\rm T}}\\mathbf{X})^{-1}\\mathbf{X}^{{\\rm T}}\\) é conhecida na literatura como a pseudoinversa de \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "t_regressao_linear.html#regressão-linear-multivariada",
    "href": "t_regressao_linear.html#regressão-linear-multivariada",
    "title": "Regressão linear",
    "section": "",
    "text": "Suponha agora que os dados não sejam mais compostos por duplas do tipo \\((x_i, d_i)\\), mas por uma sequência de \\(M\\) valores de \\(x\\), seguida do valor de \\(d\\), ou seja,\n\\[\n\\{(x_{11}, x_{21}, \\cdots, x_{M1} ,d_1), (x_{12}, x_{22}, \\cdots, x_{M2} ,d_2),\\cdots, (x_{1N}, x_{2N}, \\cdots, x_{MN} ,d_N)\\}.\n\\]\nConsiderando que esses \\(N\\) conjuntos de dados sejam previamente conhecidos, deseja-se agora obter a melhor função linear segundo o critério dos mínimos quadrados, que se ajusta a esses dados. Trata-se de uma generalização do resultado anterior. Em vez de se obter a melhor reta, vamos encontrar o melhor hiperplano que se ajusta aos dados, levando à solução conhecida como regressão linear multivariada.\nAssim, o modelo se torna\n\\[\ny=b+w_1x_1+w_2x_2+\\cdots+w_Mx_M\\approx d.\n\\]\nConsiderando os \\(N\\) conjuntos de dados, obtemos o seguinte vetor de erros\n\\[\n\\underbrace{\\left[\n  \\begin{array}{c}\n    e_1 \\\\\n    e_2 \\\\\n    \\vdots \\\\\n    e_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{e}}\n=\\underbrace{\\left[\n  \\begin{array}{c}\n    d_1 \\\\\n    d_2 \\\\\n    \\vdots \\\\\n    d_N \\\\\n  \\end{array}\n\\right]}_{\\mathbf{d}}\n-\n\\underbrace{\\left[\n  \\begin{array}{ccccc}\n    1      & x_{11} & x_{21} & \\cdots & x_{M1} \\\\\n    1      & x_{12} & x_{22} & \\cdots & x_{M2} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & x_{1N} & x_{2N} & \\cdots & x_{MN} \\\\\n  \\end{array}\n\\right]}_{\\mathbf{X}}\n\\underbrace{\\left[\n  \\begin{array}{c}\n    b \\\\\n    w_1 \\\\\n    \\vdots \\\\\n    w_M \\\\\n  \\end{array}\n\\right]}_{\\mathbf{w}}\n\\]\nComo no caso da reta, o melhor hiperplano que se ajusta aos dados segundo o critério dos mínimos quadrados é o que minimiza o quadrado da norma Euclidiana do vetor de erros, dada por\n\\[\n\\|\\mathbf{e}\\|^2=\\|\\mathbf{d}-\\mathbf{X}\\mathbf{w}\\|^2.\n\\]\nGeneralizando os passos para obtenção da reta que se ajusta aos dados, chega-se a\n\\[\\begin{equation*}\n\\fbox{$\\displaystyle\n\\mathbf{w}^{\\rm o}=(\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1}\\mathbf{X}^{\\rm T}\\mathbf{d}\n$}\n\\end{equation*}\\]\nem que \\(\\mathbf{w}^{\\rm o}=[\\,b^{\\rm o}\\;\\;w_1^{\\rm o}\\;\\;w_2^{\\rm o}\\;\\;\\cdots\\;\\;w_M^{\\rm o}\\,]^{\\rm T}\\) é o vetor que contém o bias e pesos ótimos que minimizam \\(\\|\\mathbf{e}\\|^2\\).\nObservações importantes:\n\nCalcular a inversa da matriz \\(\\mathbf{X}^{\\rm T}\\mathbf{X}\\) diretamente pode levar a problemas numéricos, dependendo do valor de \\(M\\). Isso ocorre, por exemplo, quando se utiliza a função inv.m no Matlab. Algo semelhante também ocorre em Python e é pior ao se considerar precisão de 32 bits em ponto flutuante. Procure evitar isso, resolvendo o sistema linear\n\\[\n\\mathbf{X}^{\\rm T}\\mathbf{X}\\mathbf{w}^{\\rm o}=\\mathbf{X}^{\\rm T}\\mathbf{d}\n\\]\npara encontrar \\(\\mathbf{w}^{\\rm o}\\). No Matlab, basta fazer \\((\\mathbf{X}^{\\rm T}\\mathbf{X})\\backslash(\\mathbf{X}^{\\rm T}\\mathbf{d})\\). Em Python, pode-se, por exemplo, usar a função np.linalg.solve do NumPy.\nA matriz \\(\\mathbf{X}^{\\rm T}\\mathbf{X}\\) é uma estimativa da matriz de autocorrelação dos dados de entrada \\(x\\).\nO vetor \\(\\mathbf{X}^{\\rm T}\\mathbf{d}\\) é uma estimativa da correlação cruzada entre os dados de entrada \\(x\\) e o sinal desejado \\(d\\).\nQuando se deseja ajustar um polinômio de grau \\(M\\) aos dados\n\\[\n\\{(x_1,d_1),(x_2,d_2),\\cdots, (x_N,d_N)\\},\n\\]\nbasta usar o resultado do caso multivariado, considerando\n\\[\n\\{(x_{1}, x_{1}^2, \\cdots, x_{1}^M ,d_1), (x_{2}, x_{2}^2, \\cdots, x_{2}^M ,d_2),\\cdots, (x_{N}, x_{N}^2, \\cdots, x_{N}^M ,d_N)\\}.\n\\]\nIsso leva à seguinte aproximação\n\\[\ny=b+w_1x+w_2x^2+\\cdots+w_Mx^M\\approx d.\n\\]\nNeste caso a matrix \\(\\mathbf{X}\\) se torna\n\\[\\mathbf{X}=\\left[\n   \\begin{array}{ccccc}\n     1      & x_{1} & x_{1}^2 & \\cdots & x_{1}^M \\\\\n     1      & x_{2} & x_{2}^2 & \\cdots & x_{2}^M \\\\\n     \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n     1      & x_{N} & x_{N}^2 & \\cdots & x_{N}^M \\\\\n   \\end{array}\n\\right].\n\\]\nO resultado do item anterior pode ser usado para aproximar os dados não só por polinômios, mas também por outras funções. Por exemplo, poderíamos calcular as seguintes aproximações para os dados\n\\[\ny=b+w_1\\ln(x+5)+w_2\\exp(x-2)\\approx d\n\\]\nou\n\\[\ny=b+w_1\\cos(2\\pi f_0 x)+w_2{\\rm sen}(2\\pi f_0 x)\\approx d\n\\]\nem que \\(f_0\\) é uma frequência pré-determinada. Um outro exemplo útil em Engenharia Elétrica é aproximar uma função \\(f(t)\\) periódica com período \\(T_0=1/f_0\\) por uma soma de senos e cossenos, ou seja,\n\\[\n\\begin{align}\n{f}(t)\\approx \\;b&+w_{11}\\cos(2\\pi f_0 t)+ w_{12}{\\rm sen}\\,(2\\pi f_0 t)\\nonumber\\\\\n&+w_{21}\\cos(2\\pi 2 f_0 t)+ w_{22}{\\rm sen}\\,(2\\pi 2 f_0 t)\\nonumber\\\\\n&+\\cdots\\nonumber\\\\\n&+w_{M1}\\cos(2\\pi f_0 M t)+ w_{M2}{\\rm sen}\\,(2\\pi f_0 M t).\\nonumber\n\\end{align}\n\\] Os coeficientes \\(b, w_{11}, w_{12}, w_{21}, w_{22}, \\cdots, w_{M1}, w_{M2}\\) são conhecidos como coeficientes da série de Fourier e os dados usados para obter essa aproximação são obtidos a partir da amostragem da função \\(f(t)\\)."
  },
  {
    "objectID": "t_regressao_linear.html#overfitting",
    "href": "t_regressao_linear.html#overfitting",
    "title": "Regressão linear",
    "section": "",
    "text": "Um conceito que aparece de forma recorrente em redes neurais é o chamado overfitting. Apesar de nem termos falado em redes neurais ainda, é possível já introduzir esse conceito considerando regressão linear. Antes de falar de overfitting, precisamos fazer algumas considerações importantes.\nSuponha que se deseja criar um modelo de regressão para prever o valor de venda de um automóvel usado. Dispomos de um banco de dados que contém várias informações sobre diferentes automóveis usados como ano de fabricação, modelo, estado de conservação, valor da tabela Fipe, valor médio de venda no mercado, etc. Utilizando esse banco de dados, podemos obter um modelo de regressão linear, que neste caso será multivariada, pois dispomos de muitas variáveis. Poderíamos usar todos os dados disponíveis para gerar o modelo. Se fizéssemos isso, como conseguiríamos avaliar se o modelo obtido é bom? Como saber se o modelo é capaz de prever adequadamente o valor de venda de um determinado carro que não aparece no banco de dados? Por isso, é importante reservar uma parte dos dados para avaliar a qualidade do modelo. Assim, é uma prática comum separar os dados de forma aleatória em dois conjuntos disjuntos: (1) conjunto de treinamento (ou aprendizado) e (2) conjunto de teste1. Os dados do conjunto de treinamento são efetivamente usados para gerar o modelo. Os dados do conjunto de teste são então usados para avaliar a qualidade do modelo gerado. Os dados usados para avaliação não devem aparecer no treinamento e vice-versa. Se o modelo se sair bem no teste, costuma-se dizer que ele tem uma boa capacidade de generalização.\nEm qualquer problema de regressão, deseja-se que o modelo tenha uma boa capacidade de generalização. No exemplo do automóvel usado, é importante que o modelo consiga prever com o menor erro possível o valor de venda de um carro que não constava no banco de dados. No entanto, um modelo com muitos parâmetros pode ter um ótimo desempenho no treinamento, mas uma baixa capacidade de generalização, o que leva a um erro elevado na fase de teste. Isso é chamado de overfitting. Modelos com baixa capacidade de generalização não são desejáveis, uma vez que na prática serão apresentados a dados que não foram usados no treinamento e deveriam ser capazes de realizar uma predição ou classificação de maneira adequada. Diante isso, existem várias técnicas em aprendizado de máquina que foram propostas para evitar o overfitting. Por ora, vamos apenas entender melhor esse conceito com um exemplo.\nConsidere que dispomos de apenas dez valores igualmente espaçados de \\(x\\) no intervalo \\([0,\\!1;\\;1,\\!5]\\). Os valores de \\(d\\) são gerados utilizando a função\n\\[\nd=0,\\!5+0,\\!25\\cos(2\\pi x)+v,\n\\]\nem que \\(v\\) é um ruído branco gaussiano com média zero e desvio padrão \\(0,\\!06\\). Assim, por exemplo, poderíamos ter o seguinte conjunto de treinamento\n\\[\n\\{(0,\\!1000,\\;0,\\!7055),\\;\\;(0,\\!2556,\\;0,\\!4357),\\;\\;(0,\\!4111,\\;0,\\!3264),\\;\\;\\cdots,\\;\\;(1,\\!5000,\\;0,\\!2514)\\}.\n\\]\nComo o valor de \\(d\\) depende do ruído, se não fixarmos uma semente, cada vez que gerarmos os dados teremos valores distintos. O objetivo é encontrar uma função (um modelo) polinomial de grau \\(M\\) que melhor se aproxima dos pontos do conjunto de treinamento, levando em conta a forma da cossenóide sem ruído. Na Figura 2, são mostrados os pontos disponíveis no treinamento (em vermelho), as curvas pretas representam o sinal senoidal sem ruído e as azuis os polinômios obtidos com a regressão. Foram considerados polinômios com graus \\(M=1\\) (reta), \\(M=2\\) (parábola) até \\(M=9\\). É possível ver que para \\(M=1\\) e \\(M=2\\) ocorre o underfitting, ou seja, as distâncias dos pontos de treinamento aos pontos gerados pelos polinômios dos modelos são elevadas, o que indica que eles não são adequados. À medida em que o valor do grau do polinômio aumenta, observa-se um melhor ajuste entre os pontos vermelhos e as curvas azuis, até o caso extremo de \\(M=9\\). Neste caso, o polinômio obtido passa exatamente em todos os pontos do treinamento, mas claramente a curva azul fica distante da cossenóide sem ruído em alguns trechos como pode ser visto pelas flutuações indesejadas. Isso indica que pode ter ocorrido overfitting devido ao número excessivo de parâmetros do modelo.\n\n\n\n\n\n\n\n\n\nFigura 2: Regressão linear polinomial; \\(M\\) representa o grau do polinômio, os pontos do conjunto de treinamento estão representados em vermelho; as curvas pretas representam o sinal senoidal sem ruído e as azuis os polinômios obtidos com a regressão.\n\n\n\n\n\n\nPara analisar o overfitting, geramos um conjunto de teste com 1401 valores de \\(x\\) igualmente espaçados no intervalo \\([0,\\!1;\\;1,\\!5]\\) e calculamos o valor de \\(d\\). Como há ruído na geração de \\(d\\), os pontos gerados no teste são diferentes dos de treinamento. Para cada valor de \\(M\\), medimos o valor absoluto médio do erro de predição, levando em conta o conjunto de treinamento e de teste. Na Figura 3, são mostrados os valores desses erros em função do grau do polinômio. Como esperado, o erro de aprendizagem (com os dados do treinamento) diminuem monotonicamente, chegando a zero para \\(M=9\\). Esse comportamento é típico sempre que o modelo ajustado varia do mais simples para o mais complexo. Em contrapartida, o erro do teste diminui até \\(M=5\\) e depois aumenta, indicando que modelos com muitos parâmetros têm baixas capacidades de generalização.\n\n\n\n\n\n\n\n\n\nFigura 3: Regressão linear polinomial; valor médio do módulo do erro de predição levando em conta o conjunto de treinamento e o conjunto de teste.\n\n\n\n\n\n\n\n[Miranda et al. (2015)](Goodfellow, Bengio, e Courville 2016)[Bishop (2006)](Izenman 2008)(Haykin 2009)"
  },
  {
    "objectID": "t_regressao_linear.html#footnotes",
    "href": "t_regressao_linear.html#footnotes",
    "title": "Regressão linear",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nNa realidade, é comum reservar também uma parte dos dados em um conjunto chamado de validação, mas isso será abordado posteriormente. No momento, considere apenas os conjuntos de treinamento e teste.↩︎"
  }
]